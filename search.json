[{"title":"Java基础面试问题","url":"/2025/07/10/Interview/Java%20Base/","content":"Java基础a &#x3D; a + b 与 a +&#x3D; b 的区别+&#x3D; 隐式的将加操作的结果类型强制转换为持有结果的类型。如果两个整型相加，如 byte、short 或者 int，首先会将它们提升到 int 类型，然后在执行加法操作。\nbyte a = 127;byte b = 127;b = a + b; // error : cannot convert from int to byteb += a; // ok\n\n(因为 a+b 操作会将 a、b 提升为 int 类型，所以将 int 类型赋值给 byte 就会编译出错)\n为什么需要泛型？适用于多种数据类型执行相同的代码\n引入泛型，它将提供类型的约束，提供编译前的检查\n泛型方法泛型方法创建\n\n泛型方法使用\n\n泛型方法创建的时候需要使用&lt;T&gt;来声明这是一个泛型方法, 在传入的参数中需要有Class&lt;T&gt; c参数来指明传入的参数的类型, 然后在方法中通过反射newInstance方法来创建一个新的对象\n使用泛型方法的时候, 可以通过Class.forName(“全限定类名”)来获取Class类\n泛型的上限和下限？在使用泛型的时候，我们可以为传入的泛型类型实参进行上下边界的限制，如：类型实参只准传入某种类型的父类或某种类型的子类。\n上限\nclass Info&lt;T extends Number&gt;&#123;    // 此处泛型只能是数字类型\n\n下限\npublicstaticvoidfun(Info&lt;? super String&gt; temp)&#123;// 只能接收String或Object类型的泛型，String类的父类只有Object类System.out.print(temp +&quot;, &quot;);&#125;\n\n如何理解Java中的泛型是伪泛型？泛型中类型擦除 Java泛型这个特性是从JDK 1.5才开始加入的，因此为了兼容之前的版本，Java泛型的实现采取了“伪泛型”的策略，即Java在语法上支持泛型，但是在编译阶段会进行所谓的“类型擦除”（Type Erasure），将所有的泛型表示（尖括号中的内容）都替换为具体的类型（其对应的原生态类型, 同时这里是会擦除成下限类型），就像完全没有泛型一样。\n注解元注解，元注解是用于定义注解的注解，包括@Retention、@Target、@Inherited、@Documented\n\n@Retention用于标明注解被保留的阶段\n@Target用于标明注解使用的范围\n@Inherited用于标明注解可继承\n\nJava异常类层次结构?\nThrowable\n 是 Java 语言中所有错误与异常的超类。 \n\nError 类及其子类：程序中无法处理的错误，表示运行应用程序中出现了严重的错误。\nException 程序本身可以捕获并且可以处理的异常。Exception 这种异常又分为两类：运行时异常和编译时异常。\n\n\n\n\n\n运行时异常\n\n都是RuntimeException类及其子类异常，如NullPointerException(空指针异常)、IndexOutOfBoundsException(下标越界异常)等，这些异常是不检查异常，程序中可以选择捕获处理，也可以不处理。这些异常一般是由程序逻辑错误引起的，程序应该从逻辑角度尽可能避免这类异常的发生。\n运行时异常的特点是Java编译器不会检查它，也就是说，当程序中可能出现这类异常，即使没有用try-catch语句捕获它，也没有用throws子句声明抛出它，也会编译通过。\n\n非运行时异常 （编译异常）\n\n是RuntimeException以外的异常，类型上都属于Exception类及其子类。从程序语法角度讲是必须进行处理的异常，如果不处理，程序就不能编译通过。如IOException、SQLException等以及用户自定义的Exception异常，一般情况下不自定义检查异常。\n可查的异常（checked exceptions）和不可查的异常（unchecked exceptions）区别？\n可查异常（编译器要求必须处置的异常）：\n\n正确的程序在运行中，很容易出现的、情理可容的异常状况。可查异常虽然是异常状况，但在一定程度上它的发生是可以预计的，而且一旦发生这种异常状况，就必须采取某种方式进行处理。\n除了RuntimeException及其子类以外，其他的Exception类及其子类都属于可查异常。这种异常的特点是Java编译器会检查它，也就是说，当程序中可能出现这类异常，要么用try-catch语句捕获它，要么用throws子句声明抛出它，否则编译不会通过。\n\n不可查异常(编译器不要求强制处置的异常)\n\n包括运行时异常（RuntimeException与其子类）和错误（Error）\n什么是SPI机制？SPI（Service Provider Interface），是JDK内置的一种 服务提供发现机制，可以用来启用框架扩展和替换组件，主要是被框架的开发人员使用，比如java.sql.Driver接口，其他不同厂商可以针对同一接口做出不同的实现，MySQL和PostgreSQL都有不同的实现提供给用户，而Java的SPI机制可以为某个接口寻找服务实现。Java中SPI机制主要思想是将装配的控制权移到程序之外，在模块化设计中这个机制尤其重要，其核心思想就是 解耦。\nSPI整体机制图如下：\n\n当服务的提供者提供了一种接口的实现之后，需要在classpath下的META-INF/services/目录里创建一个以服务接口命名的文件，这个文件里的内容就是这个接口的具体的实现类。当其他的程序需要这个服务的时候，就可以通过查找这个jar包（一般都是以jar包做依赖）的META-INF/services/中的配置文件，配置文件中有接口的具体实现类名，可以根据这个类名进行加载实例化，就可以使用该服务了。JDK中查找服务的实现的工具类是：java.util.ServiceLoader。\nSPI机制的应用？\nSPI机制 - JDBC DriverManager\n\n在JDBC4.0之前，我们开发有连接数据库的时候，通常会用Class.forName(“com.mysql.jdbc.Driver”)这句先加载数据库相关的驱动，然后再进行获取连接等的操作。而JDBC4.0之后不需要用Class.forName(“com.mysql.jdbc.Driver”)来加载驱动，直接获取连接就可以了，现在这种方式就是使用了Java的SPI扩展机制来实现。\n\nJDBC接口定义\n\n首先在java中定义了接口java.sql.Driver，并没有具体的实现，具体的实现都是由不同厂商来提供的。\n\nmysql实现\n\n在mysql的jar包mysql-connector-java-6.0.6.jar中，可以找到META-INF/services目录，该目录下会有一个名字为java.sql.Driver的文件，文件内容是com.mysql.cj.jdbc.Driver，这里面的内容就是针对Java中定义的接口的实现。\n\npostgresql实现\n\n同样在postgresql的jar包postgresql-42.0.0.jar中，也可以找到同样的配置文件，文件内容是org.postgresql.Driver，这是postgresql对Java的java.sql.Driver的实现。\n\n使用方法\n\n上面说了，现在使用SPI扩展来加载具体的驱动，我们在Java中写连接数据库的代码的时候，不需要再使用Class.forName(&quot;com.mysql.jdbc.Driver&quot;)来加载驱动了，而是直接使用如下代码：\nString url = &quot;jdbc:xxxx://xxxx:xxxx/xxxx&quot;;Connection conn = DriverManager.getConnection(url,username,password);.....","categories":["Interview","Java","JavaBase"],"tags":["Java","Interview","JavaBase"]},{"title":"Java Colletion-集合框架面试问题","url":"/2025/07/10/Interview/Java%20Collection/","content":"&#x2F;&#x2F; TODO\n","categories":["Interview","Java","Collection"],"tags":["Java","Interview","Collection"]},{"title":"Java JUC-并发编程面试问题","url":"/2025/07/11/Interview/Java%20JUC/","content":"并发编程问题\n","categories":["Interview","Java","JUC"]},{"title":"算法-Binary Sort-二分查找","url":"/2025/07/10/computer-fundamentals/Algorithm/Binary%20Search/","content":"二分查找典型特征\n原数组是有序的, 或者改变原数组顺序不影响答案\n我们要找出来一个数字在数组中的位置(标准二分查找题目)\n\n一般化特征\n我们能一次性排除解空间中的一半解\n我们要找出来解空间中的某一个解的位置\n\n解题通法-红蓝染色法我们能将数组依照单调性分成两部分, 以我们要找出target为例\nnum &gt;&#x3D; target的部分染成蓝色, num &lt; target为红色, 我们需要染色的区间是[left, right]或者(left-1, right+1)…\n\n[left, right]要染色的区间的含义就是我们现在没有染色也就是不知道其中的元素和target之间的关系\n\n循环不变量(以两端闭区间举例)\n\nleft - 1始终是红色\nright + 1始终是蓝色\n\n思考顺序\n首先确定我们怎么确定答案, 这种方式一定是要利用原数组在找到答案方面的单调性, 我们一定能一次性排除一半的解空间\n确定下来红蓝染色情况\n确定下来没有染色区间的开闭选择(一般是双开区间)\n\n典型例题及实现找到第一个大于等于target的值\n闭区间private int lower_bound(int[] nums, int target) &#123;    int left = 0;    int right = nums.length - 1;    // [left, right], left == right + 1 区间为空    // left - 1始终 &lt; target    // right + 1始终 &gt;= target    while (left &lt; right + 1) &#123;        int mid = left + (right - left) / 2;        if (nums[mid] &lt; target)             left = mid + 1;        else             right = mid - 1;    &#125;    return right + 1;&#125;\n\n开区间private int lower_bound(int[] nums, int target) &#123;    int left = -1;    int right = nums.length;    // (left, right), left + 1 == right 区间为空    // left 始终 &lt; target    // right 始终 &gt;= target    while (left + 1 &lt; right) &#123;        int mid = left + (right - left) / 2;        if (nums[mid] &lt; target)             left = mid;        else             right = mid;    &#125;    return right;&#125;\n\n左开右闭private int lower_bound(int[] nums, int target) &#123;    int left = -1;    int right = nums.length - 1;    // (left, right], left == right 区间为空    // left 始终 &lt; target    // right + 1始终 &gt;= target    while (left &lt; right) &#123;        int mid = left + (right - left + 1) / 2;        if (nums[mid] &lt; target)             left = mid;        else             right = mid - 1;    &#125;    return right + 1;&#125;\n\n这里会有个额外的问题是mid向下取整, 会在left &#x3D; -1, right &#x3D; 0的时候取到-1, 所以我们需要调整成向上取整, 在left &#x3D; -1, right &#x3D; 0的时候取整到0, 就会不落到区间外面了\n左闭右开private int lower_bound(int[] nums, int target) &#123;    int left = 0;    int right = nums.length;    // [left, right), left == right 区间为空    // left - 1始终 &lt; target    // right 始终 &gt;= target    while (left &lt; right) &#123;        int mid = left + (right - left) / 2;        if (nums[mid] &lt; target)             left = mid + 1;        else             right = mid;    &#125;    return right;&#125;\n\n题单典型题目35. 搜索插入位置\n74. 搜索二维矩阵\n34. 在排序数组中查找元素的第一个和最后一个位置\n非典型题目33. 搜索旋转排序数组\n153. 寻找旋转排序数组中的最小值\n4. 寻找两个正序数组的中位数\n参考链接灵茶山艾府-红蓝染色法\n","categories":["Computer Fundamentals","Algorithm","Binary sort"],"tags":["Computer Fundamentals","Algorithm","Binary Sort"]},{"title":"算法-栈 (单调栈)","url":"/2025/07/30/computer-fundamentals/Algorithm/Stack/","content":"栈栈核心特征 (什么时候使用栈)本质上是一种用空间换时间的优化方案, 如果我们缓存数据的方向是从左往右(从右往左), 而我们求解答案的时候是从右往左的, 计算完成以后就不需要了, 符合后进先出的特征, 使用栈结构\n比如括号匹配问题, 我们从左向右遍历和缓存数据, 然后在我们匹配到右括号的时候, 从右往左将数据弹出\n一般解法for (int i = 0; i &lt; nums.length; i++) &#123;   \t// 特殊的情况下, 也就是匹配到了需要解递归的数据的时候    // 元素出栈    while (!stack.isEmpty() &amp;&amp; condition) &#123;        int j = stack.pop();        ...    &#125;        // 一般情况下直接将元素入栈    stack.push(i);&#125;\n\n典型题目20. 有效的括号\n394. 字符串解码\n非典型题目155. 最小栈\n单调栈核心特征单调栈是栈的一种特殊的用法, 我们维护的栈是一个大小单调变化的, 它的需要弹出节点的时刻是将要入栈的元素会比栈顶元素大(小), 我们需要不断弹出元素直至到这个新加入的元素比栈顶元素小(大). 往往弹出元素的时候也就是计算答案的时候.\n常用于解决我们需要知道某个节点的左右最小值(大)的位置的题目, 比如每日温度和柱状图中的最大矩形都是很典型的题目. 在每日温度里面我们需要知道下一个更高温度出现在几天后, 柱状图的最大矩形中, 我们在计算以height[i]为高的矩形大小的时候, 需要知道i左右两边第一个小于height[i]的元素的位置\n一般解法for (int i = 0; i &lt; nums.length; i++) &#123;    while (!stack.isEmpty()            &amp;&amp; nums[i] &lt; nums[stack.peek()]) &#123;\t\tint top = stack.pop();        ans = .... // 计算ans    &#125;    stack.push(i);&#125;\n\n\n\n典型题目739. 每日温度\n84. 柱状图中最大的矩形\n","categories":["Computer Fundamentals","Algorithm","Stack"],"tags":["Computer Fundamentals","Algorithm","Stack"]},{"title":"算法-堆","url":"/2025/08/01/computer-fundamentals/Algorithm/Heap/","content":"堆核心概念\n这里并不讨论堆的实现和基本的原理, 只是讨论这个数据结构的使用场景, 解决的问题\n\n使用堆我们能获得O(logn)的时间复杂度的插入和删除元素, O(1)的时间复杂度的获取某个集合的最大(小)值\n如果一个场景需要持续获取某个变化集合的最大(小)值, 我们就能考虑使用堆, 并且一定会有出堆操作!!! 不然我们简单维护一个min或max变量就行了\n一般解法// 创建小堆Queue&lt;Integer&gt; minHeap = new PriorityQueue&lt;&gt;();// 创建大堆Queue&lt;Integer&gt; maxHeap = new PriorityQueue&lt;&gt;((a,b) -&gt; b-a);\n\n典型例题295. 数据流的中位数\n347. 前 K 个高频元素\n215. 数组中的第K个最大元素\n","categories":["Computer Fundamentals","Algorithm","Heap"],"tags":["Computer Fundamentals","Algorithm","Heap"]},{"title":"计网-深入理解三次握手的实现原理","url":"/2025/08/02/computer-fundamentals/Network/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E7%9A%84%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","content":"三次握手的内部实现原理\n不同于八股中简单的对三次握手的流程的介绍, 本文会从在Linux中使用socket建立TCP连接完成的工作的角度深度剖析三次握手\n参考:\n\n深入理解Linux网络： 修炼底层内功，掌握高性能原理 (张彦飞)\n\n\n使用Socket通信的过程\n// 客户端的核心代码int main()&#123;\tint fd = socket(AF_INET, SOCK_STREAM, 0);    connect(fd, ....);    ....&#125;\n\n// 服务端的核心代码int main() &#123;    int fd = socket(AF_INET, SOCK_STREAM, 0);    bind(fd, ...);    listen(fd, 128);    accept(fd, ...);    ...&#125;\n\nsocket函数的作用从开发者的角度我们调用socket函数, 创建一个socket, 然后返回一个句柄用于访问和操作我们这个创建的socket. 从内核的角度来看, 调用这个函数会在内核内部创建一系列的socket相关的内核对象\n\n从socket的系统调用出发, 创建socket的主要位置是sock_create, 而sock_create又到了__scok_create\n在__scok_create中, 首先调用sock_alloc来分配一个struct sock内核对象, 接着获取协议族的操作函数表, 并调用其create方法, 对于AF_INET协议族来说, 执行到的是inet_create方法\n在inet_create方法中, 根据SOCK_STREAM超找到对于TCP定义的操作方法的实现集合inet_stream_ops和tcp_prot, 并将它们分别设置到socket-&gt;ops和sock-&gt;sk_prot上.\n再往下在sock_init_data中, 将sock的sk_data_ready函数指针进行了初始化, 设置为默认的sock_def_readable\n在软中断上收到数据包时会通过sk_data_ready函数指针 (实际上被设置成了sock_def_readable())来唤醒sock上等待的进程\n现在一个tcp对象, 确切地说是AF_INET协议族下的SOCK_STREAM对象就算创建完毕了. 花费了一次socket系统调用的开销\nsocket小结\nsocket系统调用完成的工作有\n\n创建struct sock等一系列内核对象\n找到协议族的操作函数表, 初始化操作函数\n对sock_init_data中的sk_data_ready函数指针进行初始化, 这里是sock_def_readable()\n\n开销是一次系统调用\nbind函数的作用简单来说就是让操作系统将一个特定的socket和一个IP:port绑定起来\n做的工作有\n\n将要绑定的IP地址设置到socket的inet-&gt;inet_rcv_saddr成员上\n将要绑定的端口设置到socket的inet-&gt;inet_sport成员上\n\nlisten函数的作用listen函数的主要作用就是申请和初始化连接队列, 包括全连接队列和半连接队列. 其中全连接队列是一个链表, 而半连接队列为了快速查找使用的是哈希表.\nlisten系统调用首先到listen系统调用, 在这一步主要是\n\n通过句柄拿到socket内核对象\n获取内核参数somaxconn(if backlog &gt; somaxconn than backlog &#x3D; somaxconn)\n接下来通过sock-&gt;ops-&gt;listen进入到协议栈的listen函数\n\n协议栈listen因为是TCP, AF_INET类型的socket对象, sock-&gt;ops-&gt;listen指向的是inet_listen函数\n\n如果状态不是LISTEN, 执行inet_csk_listen_start()函数\n然后设置全连接队列的长度是backlog, 也就是服务端的全连接队列的长度是min(backlog, net.core.somaxconn)\n\n再看到inet_csk_listen_start()函数\n\n将sock强转成功inet_connection_sock(叫icsk)\n调用reqsk_queue_alloc(&amp;icsk-&gt;icsk_accept_queue, nr_table_entries), 接收队列内核对象的申请和初始化\n\n这里能强转成功的原因是这些sock是逐层嵌套的关系\n\n对于TCP的socket来说, sock对象实际上是一个tcp_sock. 因此可以随便强转成其中的某个数据结构\nreqsk_queue_alloc包含了两件很重要的事情, 接受队列数据结构的定义和接收队列的申请和初始化\n接收队列的定义这里的接收队列并不是socket接收数据的rcv队列, 是指一个包含了全连接队列和半连接队列的数据结构\nicsk-&gt;icsk_accept_queue定义在inet_connection_sock下面, 是一个request_sock_queue类型的对象, 是内核用来实现客户端请求的主要数据结构. 我们平时说的全连接队列和半连接队列都是在这个数据结构中实现的\nstruct inet_connetcion_sock &#123;\tstruct inet_sock\t\ticsk_inet;    struct request_sock_queue icsk_accept_queue;&#125;\n\nstruct request_sock_queue &#123;    // 全连接队列    struct request_sock *rskq_accept_head;    struct request_sock *rskq_accept_tail;        // 半连接队列    struct listen_sock  *listen_opt;&#125;\n\n\n对于全连接队列, 因为不需要复杂的查找工作, accept处理的时候, 只需要先进先出就好, 所有使用链式队列就好\n而半连接队列相关的数据结构是listen_opt, 是listen_sock类型的\nstruct listen_sock &#123;    u8\t\t\t\tmax_qlen_log;    u32\t\t\t\tnr_table_entries;    ...    struct request_sock *syn_table[0];&#125;\n\n因为服务端需要在第三次握手的时候快速地查找出来第一次握手时留存的reques_sock, 所以使用了哈希表来管理\n接收队列申请和初始化回到inet_csk_listen_start函数中. 调用了reqsk_queue_alloc来申请和初始化icsk_accept_queue这个对象\n\n首先计算出来半连接队列的长度\n为listen_sock对象申请内存, 这里包含了半连接队列\n全连接队列头初始化, 设置成NULL\n将半连接队列挂到了接收队列queue上\n\n半连接队列长度计算在nr_table_entries在最初调用reqsk_queue_alloc计算, 值是net.core.somaxconn和用户调用listen的时候传入的backlog二者之间的最小值\n\nmin_t(u32, nr_table_entries, sysctl_max_syn_backlog)和sysctl_max_syn_backlog内核对象再取了一次最小值\nmax_t(u32, nr_table_entries, 8), 保证nr_table_entries不会小于8. 防止新手传入的一个太小的值无法建立连接\nroundup_pow_of_two(nr_table_entris + 1)用来向上对齐到2的整数次幂\n\n所以最后, 半连接队列的长度是min(backlog, net.core.somaxconn, tcp_max_syn_backlog) + 1再向上取整到2的N次幂, 但是最小不能小于16(也就是前面的min计算出来的值不能小于8)\n同时为了提升比较性能, 内核并没有直接记录半连接队列的长度, 而是记录的N次幂\nlisten小结\n对于全连接队列, min(backlog, net.core.somaxconn)\n半连接队列的长度是min(backlog, net.core.somaxconn, net.ipv4.tcp_max_syn_backlog) + 1再向上取整到2的N次幂, 但是最小不能小于16(也就是前面的min计算出来的值不能小于8)\n也就是如果我们要调整半连接队列的长度, 要同时考虑这三个参数\nconnect函数的作用\nconnect调用链展开首先就是和listen一样的步骤, 调用connect(fd, ...)系统调用, 在系统调用内部首先使用sockfd_lookup_light(fd, ...)来获取内核中对应的socket对象\n对于AF_INET类型的socket内核对象来说, sock-&gt;ops-&gt;connect指针指向的是inet_stream_connect()函数. 然后会进入到__inet_stream_connect()\n刚创建的socket状态时SS_UNCONNECTED, 会在__inet_stream_connect()进入到case SS_UNCONNECTED的处理逻辑中. 取出socket中的sock对象, 然后执行sock中的sk-&gt;sk_prot-&gt;connect指向的tcp_v4_connect()\n在tcp_v4_connect()中, 设置socket的状态为TCP_SYN_SENT, 调用inet_hash_connnect(…, sk)动态地选择一个端口, 然后调用tcp_connect(sk)来构建发送一个syn报文\n选择可用的端口接下来就是看到inet_hash_connect()是怎么动态地选择出来一个可用的端口的,  inet_hash_connect()会直接调用__inet_hash_connect(death_row, sk, inet_sk_port_offset(sk), __inet_check_established, __inet_hash_nolisten)\n其中有两个重要的参数\n\ninet_sk_port_offset(sk): 这个函数是根据连接目的IP和端口等信息生成的一个随机数\n__inet_check_established: 是检查是否和现有ESTABLISH状态的连接冲突的时候使用的函数\n\n接下来, 我们进入到__inet_hash_connect()\n\n判断这个socket是不是bind过, 如果调用过, 相当于已经手动选定了客户端的端口了, 就不需要动态地获取端口了. 如果没有调用过, 则snum为0, 我们进入到遍历查找出来可用的端口\n\n接着从内核中获取本地端口配置, remaining &#x3D;  high - low - 1\n\n从遍历所有的端口查找可用的端口\n\nif (!snum) &#123;\n    // 遍历查找\n    for (i = 1; i &lt;= remaining; i++) &#123;\n        port = low + (i + offset) % remaining;\n\n        // 查看是否是保留端口, 是则continue跳过\n        \n        //查找和遍历已经使用的端口的哈希链表\n        \n        // 如果端口已经使用过了, 进一步调用check_established()检查端口是否可用\n    &#125;\n&#125;\n\n\n\n在循环内部\n\n判断inet_is_reserved_local_port, 判断要选择的端口是否在net_ipv4.ip_local_reserved_ports中, 在的话就不能用\n整个系统中会维护所有已经被使用过的端口的哈希表, hinfo-&gt;bhash. 代码会在这个哈希表中查找要选择的端口有没有被使用过, 如果没有找到, 说明这个端口是可用的. 这个时候通过net_bind_bucklet_create申请一个inet_bind_bucket来记录端口已经使用了\n遍历完所有的端口都没有找到可用的端口, 则会返回-EADDRNOTAVAIL(Error ADDRess NOT AVAILable), 在用户程序的视角上看就是Cannot assign requested address这个错误\n\n如果端口已经被使用过了我们如果在哈希表bhash中发现了这个端口已经使用过了,  会进一步进入到check_established 继续检查是否可用, 如果这个函数返回了0, 说明这个端口还能接着用\n\n为什么使用过了还能接着使用?\n我们只需要保证四元组是不一样的就行, 所以即使saddr和sport都是一样的, 只要daddr或者dport有一个不一样就行\n\ncheck_established由调用方传入, 实际使用的是__inet_check_establied\n在这个函数中会找到inet_ehash_bucket中这个端口对应的hash bucket, 然后遍历看看有没有四元组都一样的, 一样的话就报错. 其中inet_ehash_bucket是所有的ESTABLISH状态的socket组成的哈希表. 遍历这个哈希表, 然后使用INET_MATCH宏来判断是否可用\n发起syn请求回到tcp_v4_connect, 这个时候已经完成了获取一个可用端口了, 接下来就进入到tcp_connect(sk)\n\n申请一个skb, 并将其设置成SYN包\n添加到发送队列上\n调用tcp_transmit_skb将该包发出去\n启动一个重传定时器, 超时重传\n\nconnect小结\n客户端在执行connect函数的时候, 把本地socket状态设置成TCP_SYN_SENT, 然后选一个可用的端口, 接着发出SYN握手请求并启动重传定时器.\n搞清楚了TCP连接中客户端的端口会在两个位置确定\n\n如果在connect之前调用了bind, 如果bind的不是0, 则会使用bind中指定的端口号\n如果没有调用过bind(bind的端口号是0也会自动选择), 则会在connect的时候, 随机地从ip_local_port_range选择一个位置开始循环判断, 如果端口号查找失败, 则会报错 “Cannot assign requested address”\n如果你不想某个端口号被使用到, 则把他们写入到ip_local_reserved_ports这个内核参数中就行了\n\n\n\n完整的TCP连接的建立过程客户端connect客户端在执行connect函数的时候, 把本地socket状态设置成TCP_SYN_SENT, 然后选一个可用的端口, 接着发出SYN握手请求并启动重传定时器.\n第一次重传超时时间一般是1s, 老版本的Linux可能是3s\n服务端响应SYN所有的TCP包, 都经过了网卡, 软中断, 进入到tcp_v4_rcv函数. 该函数根据网络包(skb) TCP头信息中的目的IP查到当前处于listen状态的socket, 然后继续进入到tcp_v4_do_rcv处理握手过程\ntcp_v4_do_rcv()\n\n如果socket的状态是TCP_LISTEN, 会进入到tcp_v4_hnd_req查看半连接队列. 因为是第一次握手, 所以半连接队列中是空的, 相当于什么都处理.\n在tcp_rcv_state_process里根据不同的socket状态进行不同的处理(第一步握手的SYN和第三步握手的ACK就是在这里区分开来)\n\ntcp_rcv_state_process在sk状态是TCP_LISTEN状态并且包是syn握手包的时候, 进入到icsk_af_ops_conn_request &#x3D; tcp_v4_conn_request函数, 服务端响应SYN主要处理逻辑都在里面\ntcp_v4_conn_request()\n\n判断半连接队列是否已经满了, 如果满了, 进入到tcp_syn_flood_action判断有没有开启tcp_syncookies内核参数. 如果队列满, 并且没有开启tcp_syncookies, 握手包直接丢弃\n判断全连接队列是不是满了, 如果满了, 且young_ack的数量 &gt; 1, 同样直接丢弃\n\n\nyoung_ack是半连接队列中保持的一个计数器, 记录的是刚有SYN到达, 没有被SYN_ACK重传定时器重传过SYN_ACK. 同时也没有完成过三次握手的sock数量\n\n\n创建request_sock, 构造synack响应包, 通过ip_build_and_send_pkt发送响应包,  添加到半连接队列, 并开启定时器\n\n这一步的主要工作就是, 判断接收队列是不是满了, 满了的话, 可能会丢弃该请求, 否则发送出去synack, 申请request_sock添加到半连接队列中, 同时启动定时器\n客户端响应SYNACK客户端收到SYNACK包的时候, 同样会进入到tcp_rcv_state_process函数. 因为自身的状态是TCP_SYN_SENT, 所以会进入到另一个不同的分支\ntcp_rcv_synsent_state_process()是客户端响应synack的主要逻辑\n\n调用tcp_finish_connect标记该socket连接建立完成, 状态变成ESTABLISH, 初始化拥塞控制, 打开TCP保活计时器\n调用tcp_send_ack(), 申请和构造ack包, 发送出去\n调用tcp_clean_rtx_queue(), 删除发送队列, 删除connect时设置的重传定时器\n\n客户端响应来自服务端的synack时清除了connect时创建的重传定时器, 把当前socket状态设置成ESTABLESHED, 开启保活定时器后发出第三次握手的请求\n服务端响应ACK反直觉的是, 这里服务端监听socket的状态仍然是TCP_LISTEN, 所以仍然会进入到tcp_v4_hnd_req中, 不过因为半连接队列不是空了, 所以执行的逻辑会发生变化\ninet_csk_search_req负责在半连接队列中找到现在的TCP请求对应的半连接request_sock对象, 然后进入到tcp_check_req\ntcp_check_req()\n\n调用icsk_af_ops-&gt;syn_recv_sock &#x3D; tcp_v4_syn_recv_sock函数创建子socket\n调用inet_csk_reqsk_queue_unlink清理半连接队列\n将子inet_csk_reqsk_queue_add将子socket对应的request sock添加到全连接队列\n\n创建子sockettcp_v4_syn_recv_sock()\n\n判断接收队列是不是满了, 如果满了, goto exit_overflow修改一下计数器就将请求丢弃\n创建sock并初始化\n\n删除半连接队列inet_csk_reqsk_queue_unlink()\n\nreqsk_queue_unlink()函数将连接请求从半连接队列中删除\n\n添加全连接队列inet_csk_reqsk_queue_add()\n\n在reqsk_queue_add中将握手成功的request_sock对象插入到全连接队列链表的尾部\n\n设置状态为RESTABLISHED第三次握手的时候进入到tcp_rcv_state_process的路径不一样, 是通过子socket进来的. 这个时候子socket的状态是TCP_SYN_RECV\n在tcp_set_state(sk, TCP_ESTABLISHED), 将连接设置成TCP_ESTABLISHED. 服务端响应第三次握手ACK所作的工作是把当前半连接对象删除, 创建了新的socket后加入到全连接队列, 然后将新连接的状态设置为ESTABLISHED\n服务端acceptinet_csk_accept中调用reqsk_queue_remove从全连接队列中获取一个头元素并返回.\naccept重点工作就是从已经建立好的全连接队列中取出来一个返回给用户进程\n连接建立过程总结\n","categories":["Computer Fundamentals","Network","三次握手的实现原理"],"tags":["Computer Fundamentals","Network","三次握手的实现原理"]},{"title":"MySQL Buffer","url":"/2025/07/20/middleware/MySQL/MySQL%20Buffer%20Pool/","content":"Buffer Pool就像CPU Cache一样的缓存中间层, 来提高MySQL的性能, 写方面采用写回策略(修改的时候如果在缓存中, 就会直接修改缓存中的数据, 然后标记为脏页)\nBuffer Pool缓存了什么在MySQL启动的时候, 会为Buffer Pool分配一片连续的内存空间, 然后按照默认的16KB的大小分页, Buffer Pool中的页就是缓存页\nBuffer Pool中缓存了这六种信息\n\n\nundo页记录的是什么\n\n生成undo log的时候, undo log会写入到Buffer Pool中的Undo 页面\n\n查询一条记录, 就只会缓冲一条记录吗\n\n会将整个页都缓存进去\n","categories":["Middleware","MySQL"],"tags":["Middleware","MySQL"]},{"title":"MySQL 日志","url":"/2025/07/20/middleware/MySQL/MySQL%20%E6%97%A5%E5%BF%97/","content":"MySQL日志\n参考文章:  小林coding-MySQL 日志\n\n执行一条update语句, 期间会发生什么\nUPDATE t_user SET name = &#x27;xiaolin&#x27; WHERE id = 1;\n\n首先是前面和查询语句相似的流程\n\n客户端先通过连接器建立连接, 连接器判断用户的身份\n查询缓存, 但是因为这是一条update语句, 所以不会走查询缓存的步骤, 相反会将对应的表的缓存给清空\n通过解析器分析update语句, 拿到update关键字, 表名等信息, 构建出来语法树, 做语法检查\n通过预处理器判断表和字段是否存在\n优化器确定执行计划, 这里因为是通过id作为where的条件, 会通过id这个主键执行查询\n将执行计划交给存储引擎执行, 找到这一行, 然后执行更新\n\n\n而日志就在最后的更新步骤出现了\n\nundo log: Innodb引擎层生成的日志, 实现了事务的原子性, 主要实现了事务回滚和MVCC\nredo log: Innodb引擎层生成的日志, 实现了事务的持久性, 主要实现了crush-safe\nbinlog: Server层生成的日志, 主要用于数据备份和主从复制\n\nundo log是怎么工作的在执行DML语句的时候, 会将回滚时需要的数据都记录到undo log中\n\ninsert语句, 将这个记录的主键值记录下来, 之后要回滚的时候, 就只需要将这个主键值的记录给删除就行了\ndelete语句, 将这个被删除的记录的内容都记录下来, 之后回滚的时候, 就再重新插入\nupdate语句, 将更新列的旧值记录下来, 之后回滚的时候, 将记录重新用旧值覆盖\n如果不是主键列, 会记录旧值, 在回滚的时候用旧值覆盖\n如果是主键列, 会记录成删除旧行再插入新行, 回滚的时候也会将新记录先删除再插入原来的记录\n\n\n\n每条记录有trx_id事务id和roll_pointer指针\n\ntrx_id: 该记录是哪个事务修改的\nroll_pointer: 该指针将一条记录的undo log串联起来, 形成一条版本链\n\n\n在事务执行失败的时候, 就回滚到这个事务执行之前的版本\n上面都是通过undo log实现的事务回滚, undo log同样用于MVCC\n对于 [读提交] 和 [可重复读] 两个执行级别来说, 它们的快照读是通过 Read View + undo log来实现的, 区别在于创建undo log的时机不一样\n\n对于读提交: 在每个select后都会生成一个Read View, 在这个隔离级别中, 不会有脏读现象, 但是如果其他事务修改了数据并且提交了, 该事务中就能读到, 也就会出现同一个select语句前后执行的结果不一样的问题\n对于可重复度: 在事务启动的时候生成一个Read View, 保证了在事务中, 不会读到其他事务修改的数据\n\n为什么需要redo logredo log是用于解决崩溃恢复问题, 也就是数据的持久化(这里包括undo log和表数据)\nredo log是物理日志, 会记录对某个数据页进行了什么修改, 比如对XXX表空间中的YYY数据页ZZZ偏移量的地方做了AAA更新\n\n为什么要写到redo log, 而不直接将数据写到磁盘中, 多此一举\n\nredo log是使用了WAL(Write-Ahaed Logging)技术, MySQL的写操作并不是立刻写到磁盘上, 而是先写日志, 然后写到磁盘上. \n如果直接将数据写入到磁盘中, 就是随机写, 而将数据写入到log文件中因为是追加写, 所以是顺序写, 磁盘的I&#x2F;O性能有10~100倍的提升\n\n产生的redo log是直接写入到磁盘中吗\n\n问出来很明显就不是了, 我们需要注意到内核的缓冲区这个存在, 我们执行write函数, 实际上是将数据写入到内核缓冲区中, 直到我们主动调用fsync()或者内核自己同步, 才会刷盘. 这是其中一方面, 另一方面是redo log本身就有一个redo log buffer, 用于缓冲log, 提高IO性能\nredo log什么时候刷盘redo log刷盘的时机主要有下面几个(注意是刷盘, 而不是写入到文件中, 前者是直接写入到磁盘中的, 后者只是将内容写入到内核缓冲区中)\n\nMySQL正常关闭的时候\nredo log buffer中的记录写入量大于redo log buffer内存空间的一半的时候\n每次事务提交的时候都将缓存在redo log buffer中的redo log直接持久化到磁盘中(这个策略通过innodb_flush_log_at_trx_commit参数控制)\n\n\ninnodb_flush_log_at_trx_commit 参数控制的是什么\n\n\n参数为0的时候: 每次事务提交会将redo log留在redo log buffer\n参数为1的时候: 每次事务提交会将redo log buffer中的redo log直接持久化到磁盘中\n参数为2的时候: 每次事务提交会将redo log buffer中的redo log写入到redo log文件中(并不是持久化到了磁盘中, 而是到了内核中的Page Cache)\n\n\n\n那么innodb_flush_log_at_trx_commit参数的值是0或者2的时候, 什么时候刷盘?\n\nInnoDB后台线程每隔一秒:\n\n参数为0的时候: 后台线程执行write()将redo log buffer中的redo log写到Page Cache中, 再执行fsync()写入到磁盘中, 所以MySQL进程的崩溃, 会导致前1s的所有事务丢失\n参数为2的时候: 区别就是只需要调用fsync(), 数据已经都在Page Cache中了, 所以MySQL进程的崩溃, 不会导致事务丢失, 只有宕机崩溃的时候, 会导致前1s的所有事务丢失\n\n\nredo文件写满了怎么办InnoDB中redo log是以重做文件日志组的形式存在的, redo log group中有两个redo log文件, 两个文件大小一致, 分别叫做 ib_logfile0和ib_logfile1\n重做文件日志组是采用循环写的模式, 从头开始写, 写到了末尾又回到了开头, InnoDB引擎会先写 ib_logfile0, 然后写ib_logfile1, 当 ib_logfile1也写满的时候, 就会重新回到logfile0的开头\nInnoDB使用write pos 表示redo log当前记录写到的位置, checkpoint 表示当前要擦除的位置, 其中要擦除的位置就是已经持久化到磁盘中脏页的位置\n\n如果write pos 追上了 checkpoint, 说明redo log已经满了, 这个时候MySQL会被阻塞, 会停下来将Buffer Pool中的脏页刷新到磁盘中, 将redo log中对应的记录标记并清除, 然后移动checkpoint, MySQL恢复正常\nredo log的格式redo log是物理日志, 记录的是对数据页的修改, redo log的格式是:\nLSN | Log Record Type | Log Record Length | Log Record Body\n\nLSN: Log Sequence Number, redo log的序列号, 用于标识redo log的顺序\nLog Record Type: redo log的类型, 比如更新数据页, 删除数据页\nLog Record Length: redo log的长度, 用于标识redo log的大小\nLog Record Body: redo log的具体内容, 包含了对数据页的修改信息Log Record Body的内容是根据Log Record Type的不同而不同的, 比如更新数据页的Log Record Body会包含更新前的数据和更新后的数据, 删除数据页的Log Record Body会包含被删除的数据\n\n为什么需要binlogbinlog是逻辑日志, 记录的是对数据的修改, binlog的主要作用是用于数据备份和主从复制\nMySQL在执行完一条更新操作以后, Server层还会生成一条binlog, 等事务提交的时候, 会将该事务执行过程中产生的binlog写入到binlog文件中\nbinlog只能用于归档, 是不具备crush-safe能力的\nredo log和binlog的区别\n适用对象不同\n\n\nbinlog是Server层实现的日志, 所有的存储引擎都能使用\nredo log是InnoDB引擎实现的日志, 只能用于InnoDB存储引擎\n\n\n文件格式不同\n\n\nbinlog有 3 种格式: STATEMENT, ROW, MIXED\nSTATEMENT: 记录每一条更新的操作的SQL语句, 这也是为什么binlog会被称作逻辑日志, 主从复制种slave端就是通过binlog中记录的SQL语句来执行更新操作的. 但是会存在动态函数的问题, 比如NOW()函数, 在主从复制中, 主库执行的NOW()函数和从库执行的NOW()函数的时间不一致, 会导致数据不一致\nROW: 记录行数据最后被修改成什么样子(这种格式的日志就不是逻辑日志了), 缺点就是如果是批量更新, 会记录每一行的修改, 可能会导致binlog文件过大\nMIXED: 结合了STATEMENT和ROW两种格式, 会根据不同的情况自动选是使用STATMENT模式还是ROW模式\n\n\n\n\n写入方式不同\n\n\nbinlog是追加写, 写满一个文件, 就创建一个新的文件继续写, 不会覆盖之前的文件, 保存的是全量的日志\nredo log是循环写, 写满一个文件, 就回到开头继续写, 只保存最近的日志\n\n\n用途不同\n\n\nbinlog主要用于数据备份和主从复制, 也可以用于数据恢复\nredo log主要用于崩溃恢复, 也可以用于数据恢复\n\n\n如果整个数据库的数据都被删除了, 能通过redo log来恢复吗\n\n不能, 只能通过binlog来恢复, redo log记录的不是全量的数据, 如果脏页被写入到了磁盘中, 就会从redo log中删除\n主从复制是怎么实现的MySQL集群的主从复制可以分成三个阶段\n\n写入binlog: 主库写binlog日志, 提交事务并更新本地存储数据\n同步binlog: 从库通过IO线程连接到主库, 从主库获取binlog日志, 写入到本地的中继日志中\n回访binlog: 从库通过SQL线程读取中继日志, 执行SQL语句, 更新本地存储数据\n\n\n详细过程是\n\n主库在收到的客户端提交事务的请求之后, 会先写入到binlog中, 然后提交事务, 更新存储引擎中的数据, 事务提交完成后, 返回给客户端”操作成功”的响应\n从库创建一个专门的IO线程, 连接主库的log dump线程, 来接收主库的binlog日志, 再把binlog信息写入到relay log中, 再返回给主库复制的复制成功的响应\n从库会创建一个用于回放binlog的线程, 去读relay log中的中继日志, 然后回放binlog更新存储引擎中数据\n\n\n从库是不是越多越好\n\n并不是, 从库越多, 主库需要创建同样多的log dump线程来处理从库连接上来的IO线程, 对主库的资源消耗较高\n\n主从复制还有哪些模型\n\n\n同步模型: MySQL主库提交事务的线程需要等待所有的从库都响应复制成功以后, 才会返回给客户端结果, 这样的形式性能和可用性都很差\n异步模型: 就是上面的模型, 缺点就是如果主库宕机, 就会出现数据的丢失\n半同步模型: 介于上面两者之间, 事务线程不用等待所有的从库返回复制成功响应, 只需要等待一部分返回复制成功响应就会返回给客户端. 通过这种形式, 保证了即使主库宕机, 也还有从库保存有最新的数据, 不会导致数据丢失\n\nbinlog什么时候刷盘在Server层, binlog有一个binlog cache, 事务提交的时候, 是先写到binlog cache中, 然后再把binlog cache写入到binlog文件中.\n一个线程中只能有一个事务正在执行, 每个线程也都携带了一个binlog cache\n\n什么时候binlog cache会被写入到binlog中\n\n在事务提交的时候, 会将binlog cache中的完整事务写入到binlog文件中, 并清空binlog cache (注意这里不是持久化, 也就是只调用了write(), 而没有调用fsync())\n\nMySQL中提供了sync_binlog参数来控制数据库binlog刷盘的频率\n\nsync_binlog &#x3D; 0的时候, 表示每次提交事务都只write, 不fsync(), 什么时候fsync交给操作系统决定\nsync_binlog &#x3D; N的时候, 表示每次提交事务的时候都只write(), 在积累了N个事务以后fsync\n\nMySQL中默认的是sync_binlog &#x3D; 0\n\n小结update语句的执行过程\n\n这里我们从优化器分析出来了成本最小的执行计划以后, 执行器按照其计划执行\n\n执行器调用存储引擎的接口, 通过主键索引树搜索id &#x3D; 1这一行的记录\n如果id &#x3D; 1这一行所在的数据也本身就在buffer pool中, 就直接返回给执行器更新\n如果不在, 就需要先将数据页从磁盘中加载到buffer pool中, 返回记录给执行器更新\n\n\n执行器拿到聚簇索引记录以后, 比较更新的记录和原记录是不是一样的\n如果是一样的, 不执行后续流程\n不一样, 将更新前的记录和更新后的记录都当作参数传给InnoDB层, 让InnoDB执行真正的更新操作\n\n\n开启事务, InnoDB更新前需要先记录相应的undo log, 因为是更新操作, 所以将旧值存入undo log, 将undo log写入buffer pool中的Undo页面, 然后将这个Undo页面记录到redo log中\nInnoDB层开始更新记录, 先更新内存中记录, 并标记成脏页, 然后将记录写入到redo log中, 这时就是更新完成了, 后台线程将内存中脏页刷盘\n开始记录该语句对应的binlog, binlog先被保存到binlog cache中, 在事务提交的时候统一将该事务运行过程中的所有binlog更新到磁盘上\n最后就是事务提交, 也就是[两阶段提交], 接下来就是讲两阶段提交\n\n为什么需要两阶段提交在主从复制的场景中, 主库MySQL宕机了, redo log负责的是主库的崩溃恢复, binlog负责的是事务能否传播到从库\n而redo log和binlog持久化到磁盘是两个独立逻辑, 那么就会出现下面的两种情况\n\n如果在redo log刷盘以后, MySQL宕机了, binlog还没有来得及刷盘. 这个时候, 崩溃恢复以后, 主库中能将Buffer Pool中的事务修改的数据恢复, 但是从库读binlog并没有执行之前的事务, 出现了主库是新值, 从库是旧值\n如果在binlog刷盘以后, MySQl宕机了, redo log没有刷盘. 就会变成主库是旧值, 从库是新值\n\n这两种情况都是出现了主从库数据不一致的问题\nMySQL给出的解决方案就是两阶段提交, 将事务提交的过程分成了两个阶段, 准备阶段和提交阶段\n两阶段提交是怎么样的为了保证binlog和redo log的一致性, MySQL使用了内部XA事务\n在客户端执行commit的时候, MySQL内部开启一个XA事务, 分两阶段完成XA事务的提交\n\n将redo log的写入拆分成了两个阶段: prepare和commit, 中间穿插binlog\n\nprepare阶段: 将XID (内部XA事务的ID) 写入到redo log, 将redo log对应的事务状态设置成prepare, 将redo log持久化到磁盘(innodb_flush_log_at_trx_commit &#x3D; 1的时候)\ncommit阶段: 将XID写入到binlog中, 然后将binlog持久化到磁盘 (sync_binlog &#x3D; 1的时候), 接着调用引擎的提交事务接口, 将redo log状态设置成 commit\n\n异常重启后的现象\n在MySQL重启以后, 会按照顺序扫描redo log文件, 碰到处于 prepare 状态的 redo log, 就会拿着redo log的XID去binlog中查看是否有该XID\n\n如果没有XID, 说明redo log完成了刷盘, 但是binlog还没有完成刷盘, 回滚事务, 因为从库会无法同步这个数据, 对于到途中的A时刻\n如果有XID, 说明redo log和binlog都完成了刷盘, 则提交事务, 对应B时刻崩溃恢复\n\n对于处于prepare阶段的redo log, 既可以提交事务, 也可以回滚事务, 这取决于是否在binlog中查找到与redo log相同的XID\n两阶的提交是以binlog写成功为事务提交成功的标志\n两阶段提交的问题\n磁盘的I&#x2F;O次数高, 每次事务提交都会有两次刷盘 (如果在两个刷盘配置都是1的情况下)\n锁竞争激烈, 多事务的场景下, 需要通过锁来保证提交的原子性(即保证binlog的写入顺序和事务提交顺序一致)\n\n组提交为了解决锁竞争激烈的问题, 引入了binlog组提交机制, 当有多个事务提交的时候, 将多个binlog刷盘操作合并成一个, 从而减少磁盘I&#x2F;O的次数\n将commit拆分成了三个阶段\n\nflush阶段: 多个事务按照进入的顺序将binlog从binlog cache写入到文件(不刷盘)\nsync阶段: 将多个事务的binlog合并一次刷盘\ncommit阶段: 各个事务按顺序做commit操作\n\n\n在每个阶段引入了锁机制以后, 锁就只针对每个队列进行保护, 不再锁住提交事务的整个过程, 可以看出来, 锁粒度减小了, 这样就使得多个阶段可以并发执行\n\n\nflush 阶段\n\nflush阶段的队列用于支撑redo log的组提交\n\nsync 阶段\n\n事务会在sync队列上等待一定时间, 以组合更多事务的binlog然后一起刷盘\nsync队列的作用是用于支持binlog的提交\n\ncommit 阶段\n\n承接sync阶段的事务, 完成最后的引擎提交\n","categories":["Middleware","MySQL"],"tags":["Middleware","MySQL"]},{"title":"MySQL 锁机制","url":"/2025/07/16/middleware/MySQL/MySQl%20%E9%94%81/","content":"\n参考:\n\nhttps://relph1119.github.io/mysql-learning-notes/#/mysql/25-%E5%B7%A5%E4%BD%9C%E9%9D%A2%E8%AF%95%E8%80%81%E5%A4%A7%E9%9A%BE-%E9%94%81 \nhttps://xiaolincoding.com/mysql/lock/how_to_lock.html\n\n\nMySQL锁机制详解快照读与锁定读MySQL中读可以分作两类\n\n快照读: 也就是普通的select, 读-读场景不需要额外的机制保证并发安全, 而读-写场景通过MVCC来实现隔离级别\n锁定读: 会加锁的读, 是select ... for update或者select ... lock in share mode , 在MySQL中update, delete操作会分成两部分, 读取阶段和写入(删除)阶段, 前一阶段就属于锁定读\n\n共享锁(S)和互斥锁(X)共享锁: 如果一个事务给一个表(记录)加了S锁, 其他事务能再获取这个S锁, 但是该事务和其他事务都不能再获取这个表(记录)的X锁\n互斥锁: 独占锁, 如果有一个事务给一个表(记录)加了X锁, 其他事务不能再从这个表(记录)上获取锁了(无论XS), 同时也不能给已经上锁了的表(记录)加上X锁\n\n\n\n兼容性\n共享锁S\n互斥锁X\n\n\n\n共享锁S\n兼容\n不兼容\n\n\n互斥锁X\n不兼容\n不兼容\n\n\n类似于读-读并发安全, 所以不需要额外处理, 也就是S锁能兼容, 其他的情况都存在并发安全问题, 所以不能兼容\n表锁表级锁锁上整张表的锁, 分有X和S两种\n什么时候会加上表级锁: MySQL InnoDB引擎中因为有更细粒度的行级锁, 所以其实表级锁应用场景极其有限(没啥用)\n但是现在有个问题就是, 如果我们要对某个表加X锁或者S锁, 有个问题就是, 我们需要确保现在这个表是没有不兼容的行锁的\n\n我们要加上表级S锁的时候, 就需要保证表内没有X锁\n我们要加上表级X锁的时候, 就需要保证表内没有锁\n\n这种时候很明显我们不能遍历每行来看是不是有加锁, Innodb引入了意向锁的机制\n意向锁IS锁: 共享意向锁, IX锁: 互斥意向锁\n什么时候会加上意向锁: 现在在加锁前, 我们会现在表级上加上一个意向锁, 比如我们要加上一个互斥X锁, 我们就会先在表上加一把IX, IS同理\n在有了意向锁以后, 我们就能通过判断表上有没有持有IS和IX锁来快速判断现在我们能不能加表锁\nIS, IX是表级锁, 它们的出现仅仅是为了在之后加表级锁的时候快速判断表中是不是存在加锁的记录, IS锁和IS锁之间是兼容的,  IX和IX之间是兼容的\n\n\n\n兼容性\nX\nIX\nS\nIS\n\n\n\nX\n不兼容\n不兼容\n不兼容\n不兼容\n\n\nIX\n不兼容\n兼容 兼容的\n不兼容\n兼容 兼容的\n\n\nS\n不兼容\n不兼容\n兼容 兼容的\n兼容 兼容的\n\n\nIS\n不兼容\n兼容 兼容的\n兼容 兼容的\n兼容 兼容的\n\n\nMDL锁Meta Data Lock: 元数据锁, 是针对DDL操作的锁, 防止在存在事务还在执行的时候变更表结构\n什么时候会加上MDL: \n\n对一张表CRUD的时候, 加上MDL读锁\n修改表结构的时候, 加上MDL写锁\n\nMDL写锁的获取会阻塞MDL读锁的获取, 也就是如果有一个事务在修改表结构获取MDL写锁的时候阻塞了, 后续的CRUD操作都会被阻塞住\nMDL在事务提交后才会释放\nAUTO-INC锁是用于处理AUTO_INCREAMENT自增字段的自增的锁, 如果并发地向一张表中插入记录, 就可能会导致自增字段值重复\n有两种锁来解决并发自增问题\n\nAUTO-INC锁: 在执行插入语句的时候, 会加上一个表级的AUTO_INC锁, 然后分配自增属性的值, 在插入语句执行结束后将锁释放掉, 以此将插入时生成自增值串行化\n轻量级锁: 在执行插入语句的时候, 获取这个轻量级锁, 在生成自增属性的值以后就将锁释放掉\n\n如果我们的插入语句在执行前就知道要插入多少条数据, 就会采用轻量级锁\n\nTIP：设计InnoDB的大佬提供了一个称之为innodb_autoinc_lock_mode的系统变量来控制到底使用上述两种方式中的哪种来为AUTO_INCREMENT修饰的列进行赋值，当innodb_autoinc_lock_mode值为0时，一律采用AUTO-INC锁；当innodb_autoinc_lock_mode值为2时，一律采用轻量级锁；当innodb_autoinc_lock_mode值为1时，两种方式混着来（也就是在插入记录数量确定时采用轻量级锁，不确定时使用AUTO-INC锁）。不过当innodb_autoinc_lock_mode值为2时，可能会造成不同事务中的插入语句为AUTO_INCREMENT修饰的列生成的值是交叉的，在有主从复制的场景中是不安全的。\n\n行锁行锁在MySQL中是InnoDB独有的更加细粒度的锁\n记录锁Record Lock: 记录锁, 有S锁和X锁之分, 是针对一条记录上加的锁, 也就是对某一行加上的锁\n什么时候加锁: 在执行锁定读的时候会对遍历到的行加上, 往往是以next-key lock的组成部分的形式被加上\n间隙锁Gap Loc: 间隙锁, 有S锁和X锁, 但是没有分别, 实际上是都是S锁的行为, 多个事务可同时获取一个间隙的Gap Lock, 是对一个开区间的锁, 用于解决可重复读隔离级别下的幻读现象的\n\n在id范围(3, 5)的间隙锁以后, 其他事务就无法再插入id &#x3D;4的记录了\n什么时候加锁: 和记录锁是一样的, 执行锁定读的时候会对遍历到的行加上, 往往是以next-key lock的组成部分的形式被加上\nNext-Key LockNext-Key Lock &#x3D; Record Lock + Gap Lock, 锁定一个范围, 并且锁定记录本身\n因为Next-Key Lock是包含Record Lock的, 所以是分X锁和S锁的\n插入意向锁是在insert前会对某个记录加上锁, 用于提高插入的并发效率, 只要插入位置不同, 想插入的事务间不会相互阻塞, 只有当多个事务尝试插入相同位置时才会产生冲突\n什么时候加锁: 在执行插入操作的时候需要判断这个位置上有没有间隙锁, 如果有, 插入操作就会被阻塞, 然后生成一个插入意向锁(is_wait &#x3D; true), 在这个间隙锁被释放掉以后, 插入意向锁就会被真正获取到, 执行插入操作\n有插入意向锁（当前机制）：\ncss复制索引：[1] [4] [7] [10]事务A：INSERT 5  → 在(4,7)设置插入意向锁事务B：INSERT 6  → 在(4,7)设置插入意向锁结果：两个事务可以并发执行 ✓\n\n没有插入意向锁（假设情况）：\ncss复制索引：[1] [4] [7] [10]事务A：INSERT 5  → 在(4,7)设置间隙锁事务B：INSERT 6  → 等待事务A释放间隙锁结果：事务B被阻塞，必须等待 ✗\n","categories":["Middleware","MySQL","Lock"]},{"title":"操作系统I/O - 多路复用","url":"/2025/07/25/computer-fundamentals/Operating%20System/IO-%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/","content":"I&#x2F;O-多路复用多路复用是解决的什么问题解决的最根本的问题是: 我们怎么让我们的服务器能并发处理更多的数量的请求\n最经典的问题就是C10K问题: 服务器怎么并发处理1w个请求\n解决这个问题我们就需要考虑到, 连接占用的资源有哪些\n\n文件描述符: Socket实际上是一个虚拟的文件, 也就对应着有相应的文件描述符, 在Linux中一个进程能打开的文件描述符的数量是有限的, 一般来说是1024(默认值)\n系统内存: 每个TCP连接在内核中都有对应的数据结构, 也就是每个连接都占用了一定的内存\n\n在这些基础上, 我们该怎么实现并发处理1w个请求呢?\n\n多进程模型?\n\n我们每成功建立一个连接就创建一个进程, 这个时候因为fork()创建的子进程中的文件描述符也是被继承过去, 让子进程来通过已连接Socket来提供服务\n但是这种方式很明显是不能解决C10K问题的, 没有哪个系统扛得住创建1W个进程, 并且进程间切换的成本很高, 性能很差\n\n多线程模型\n\n为了解决多进程模型中, 进程的体量很大并且切换成本高的问题, 我们换成多线程模型\n当服务器与客户端 TCP 完成连接后，通过 pthread_create() 函数创建线程，然后将「已连接 Socket」的文件描述符传递给线程函数，接着在线程里和客户端进行通信，从而达到并发处理的目的。\n使用线程池避免频繁地创建和销毁线程, 维护一个已连接Socket队列, 每建立一个连接, 就将已连接Socket添加到队列中, 然后子线程负责从队列中取出来已连接Socket进行处理\n但是同样是没有哪个操作系统能同时维护1w个线程, 也是不可行的\nI&#x2F;O 多路复用为每个请求分配一个进程&#x2F;线程不合适, 我们就只能使用一个进程来维护多个Socket, 这个就是I&#x2F;O多路复用技术\n这就像一个线程调度算法一样, 我们只有一个CPU, 但是我们通过线程调度, 提供了一种我们并发执行多个程序的视图\n我们将处理每个请求的事件耗时控制在0.1ms内, 我们1s就能同时处理1w个请求, 拉长时间来看, 就是多个请求复用了一个进程, 也被称为时分多路复用\nselect&#x2F;poll&#x2F;epoll 内核提供给用户态的多路复用系统调用, 一个进程可以通过一个系统调用函数从内核中获取多个事件, 当其中任何一个文件描述符准备好IO操作的时候, 程序会被通知\n它们是怎么获取网络事件的呢? 处理事件的时候, 先将所有的连接 (文件描述符) 传给内核, 再由内核返回产生了事件的连接, 然后在用户态处理这些连接对应的请求\nselect&#x2F;poll\n参考 小林coding-I&#x2F;O 多路复用\n\n","categories":["Computer Fundamentals","OS","IO"],"tags":["Computer Fundamentals","OS","IO"]},{"title":"操作系统内存管理 - Linux物理内存篇","url":"/2025/08/09/computer-fundamentals/Operating%20System/Linux%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","content":"Linux物理内存物理内存检测在物理内存这个硬件和操作系统之间, 还存在着一个固件层(firmware)也叫BIOS. 它负责硬件自检, 初始化所有硬件设备, 加载操作系统引导程序, 将控制权移交给操作系统并提供结构供操作系统读取硬件信息. 操作系统所需的内存等硬件信息都是通过固件来获取的\n在固件ACPI接口规范中定义了探测内存的物理分布规范. 内核请求中断15H , 并设置操作码为E820H,  因为操作码是E820, 所以这个机制也被称为E820\n会在detect_memory_e820函数发出15号中断并处理所有结果, 把内存地址范围保存到boot_params.e820_table对象中. boot_params只是一个中间数据, 专门还有一个e820_table全局数据结构来保存内存地址范围, 在e820__memory_setup中会将boot_params.e820_table保存到e820_table中, 并打印出来. 服务器能通过mseg命令来查看到实际的物理内存地址.\nmemblock内存分配器的创建在完成了E820机制检测到可用的内存地址范围以后, 调用e820__memory_setup把检测结果保存到e820_table全局数据结构中以后. 紧接着就是调用e820__memblock_setup创建memblock内存分配器, 这个分配器会进行初期物理内存的一个粗粒度的分配\n\nmemblock数据结构\n\nmemblock的实现非常简单, 就是按照检测的物理内存的地址范围是useable还是reserved分成两个对象, 分别使用memblock_region数组存起来\n\n\ne820__memblock_setup\n\n\n遍历e820_table中的每一段内存区域, 如果是预留内存就调用memblock_reserve添加到reserved成员中\n如果是可用内存就调用memblock_add添加到memory成员中\n创建完成以后还会调用依次memblock_dump_all()进行依次打印输出.不过要启用这个输出的信息需要修改Linux启动参数, 添加memblock &#x3D; debug并重启才可以, 然后就能通过dmesg来查看了\n\n向memblock分配器申请内存在启动时伙伴系统创建以前, 是通过memblock来分配内存的, 两个比较重要的使用场景就是crash kernel和页管理初始化\n\n crash kernel\n\n内核为了能在崩溃的时候记录崩溃的现场, 方便以后排查分析, 设计了一套kdump机制. 实际上是在服务器上启动了两个内核, 第一个是正常使用的内核, 第二个是崩溃发生的时候应急内核. 发生崩溃的时候kdump使用kexec启动到第二个内核中运行. 这样第一个内核中的内存就得以保留下来了. 然后就可以把崩溃时候的所有运行状态都收集到dump core中\n对此机制不展开, 只是说明这个机制是需要额外的内存才能工作的\n\n页管理初始化\n\nLinux的伙伴系统是按页的方式来管理物理内存的, 一个页的大小是4KB. 每一个页使用一个struct page对象来表示, 这个对象也是需要消耗内存的, 这个数据结构的大小一般是64B\n在内核初始化的阶段会为所有的页面都申请一个struct page对象, 将来通过这个对象对页面来进行管理.\n内存页管理模型现在默认采用的是SPARSEMEM模型. 在内存中就是一个二维数组\n\nNUMA信息感知为什么会有NUMANUMA : Non-Uniform Memory Access 非一致性内存访问\n\n服务器主板不同于个人主机主板, 提供了CPU的拓展功能, 也就是一块主板上能有多个CPU, 每个CPU都有自己直连的内存, 如果想访问另一个CPU的直连内存, 就需要经过UPI总线.\n这样的访问机制导致对于同一个CPU来说, 访问自己的直连内存与访问需要经过UPI总线的其他的CPU的直连内存, 经过的物理链路的长度是不一样的, 也就导致了访问的时延是不同的\nLinux读取NUMA的信息Linux根据内存访问特性的相似性, 将cpu核心和物理内存划分成一个个node, 比如在上面的图像中, 左边的cpu和它的直连内存就是一个node, 另一个cpu和它的直连的内存就是另一个node\n在操作系统和硬件之间的fireware固件层向Linux提供了NUMA的信息, 这里主要就是向Linux提供能划分出来node的信息\n固件层向Linux操作系统提供了SRAT和SLIT两个表\n\nSRAT(System Resource Affinity Table): 提供了cpu核与内存的关系图, 有几个node, 每个node有几个逻辑核, 有哪些内存\nSLIT(System Locality Information Table): 提供了node之间的距离\n\nLinux通过下图的调用链, 最终执行到x86_acpi_numa_init函数来从ACPI中读取出来SRAT表, 并将读取的结果保存在numa_meminfo这个数据结构中, 这是一个全局的列表, 每一项都是(起始地址, 结束地址, 节点编号)的三元组\nstart_kernel-&gt; setup_arch\t---&gt; initmem_init\t\t// 内存初始化, 包括NUMA的初始化-----&gt;  x86_numa_init-------&gt; numa_init---------&gt; x86_acpi_numa_init-&gt;acpi_numa_init // 读取SRAT表, 保存到全局数据结构numa_meminfo中\n\nmemblock分配器关联NUMA信息获取到内存块的node节点信息, 接下来还需要把NUMA信息写入到memblock分配器中, 来保证系统启动时的创建的数据结构能正确地分配到对应的内存节点上, 不会出现跨节点内存访问等由NUMA导致的问题, 同时在这一步让分配器感知到的NUMA信息也会被后续继承给buddy allocator\nstart_kernel-&gt; setup_arch\t---&gt; initmem_init\t\t// 内存初始化, 包括NUMA的初始化-----&gt;  x86_numa_init-------&gt; numa_init---------&gt; init_func // 把NUMA相关的信息保存到numa_memeinfo中---------&gt; numa_register_memblks // memblock添加numa信息, 并为每个node申请对象---------&gt; numa_init_array // 将各个cpu core与NUMA node关联\n\n\nnuma_register_memblks函数\n\n\n将每一个memblock region和NUMA节点号关联\n为所有可能存在的node申请pglist_data结构体空间\n打印memblock内存分配器的详细调试信息\n\n\n物理页管理之伙伴系统memblock分配器管理的粒度太大了, 操作系统需要能申请更小的物理内存的物理内存管理系统, 这个系统就是伙伴系统(buddy allocator)\n伙伴系统相关的数据结构操作系统将物理内存划分到了一个个node中, 可以使用numactl来查看到每个node的情况\n# numactl --hardwareavailable: 1 nodes (0)node 0 cpus: 0 1node 0 size: 3950 MBnode 0 free: 1429 MBnode distances:node   0   0:  10 \n\n\n\nnode的信息保存在struct pglist_data node_data[]这个全局列表中\nstruct pglist_data &#123;    struct zone node_zones[MAX_NR_ZONES];    ...;    int node_id;&#125;\n\n在node下面, linux进一步划分成了三个zone, zone表示内存中的一块范围, 有三种类型\n\nZONE_DMA: 地址最低的一块内存区域, 供ISA设备DMA访问(ISA设备是24位的, 这块区域对应的是16MB, 能保证向ISA设备提供的地址是24位的)\nZONE_DMA32: 用于支持32-bits地址总线的DMA设备, 同时也可以作为普通的内存分配, 是一种向下兼容的设计\nZONE_NORMAL: x86-64的架构下, DMA和DMA32之外的内存地址都在NORMAL的ZONE中管理\n\n# cat /proc/zoneinfoNode 0, zone      DMA  pages free     3977        managed  3977Node 0, zone    DMA32  pages free     297080        managed  761850Node 0, zone   Normal  pages free     65848        managed  245375\n\n对于zong数据结构\nstruct zone &#123;    .... ;    // zone的名称    const char *name;        // 管理zone下面所有页面的伙伴系统    struct free_area free_area[MAX_ORDER];&#125;\n\n其中的free_area就是伙伴系统实现的重要的数据结构, 由此也能看出来内核中不是只有一个伙伴系统, 每一个zone都有一个伙伴系统\n伙伴系统是怎么管理空闲页面的伙伴系统中的free_area是一个有11个元素的数组, 每个数组分别代表的是空闲可分配连续4KB, 8KB, 16KB, …, 4M的内存链表\n\n# cat /proc/pagetypeinfo \n\n\n内核提供分配器函数alloc_pages到上面的链表中寻找可用连续页面\n以我们要寻找8KB的物理内存为例, 忽略UNMOVABLE之类的区别, 统一按照free_list来看\n\n尝试从8KB的free_list获取一块连续的8KB的内存, 获取失败, free_list中元素是null\n向下移动一位, 尝试从16KB的free_list中获取, 获取成功, 这个时候我们将原来的16KB拆分成了两部分了, 一部分8KB是我们返回的内存\n另一部分没使用的8KB被整理到8KB的free_list中\n\nmemblock向伙伴系统交接交接的调度链路是\nstart_kernel-&gt; setup_arch\t---&gt; e820__memory_setup  //\t保存物理内存检测结果...-&gt; mm_init---&gt; mem_init-----&gt; memblock_free_all // 向伙伴系统移交控制权\n\n具体的交接过程是在memblock_free_all-&gt;free_low_memory_core_early中执行的\n\nfree_low_memory_core_early函数\n\n\nreverse内存交接\n可用内存交接\n通过for_each_free_mem_range遍历\n调用__free_memory_core释放并将页面放到zone的free_area数组中对应的位置上去\n\n\n\nSLAB内存分配器虽然我们已经实现了更细粒度的4KB的物理内存分配管理器, 但是很显然, 我们不可能为每个内核中的数据结构都申请一个4KB的内存页, 我们需要一种更加灵活的内存管理器. 这个需求在用户态同样是需要的, 不过用户态的程序是通过mmap&#x2F;brk来申请内存, 通过ptmalloc等用户态的分配器来管理内存. 内核同样需要这样的内存分配器, 这就是SLAB内存分配器.\n\n分配器原理slab分配器抽象出来slab集装箱这个概念, 一个slab内只分配特定大小, 甚至是特定的对象. 这样当一个对象释放了内存以后, 另一个同类的对象可以直接使用这块内存\n\n分配器实现最基础的数据结构是kmem_cache, 这个数据结构就对应着某个特定大小或特定对象的slab池, 每个kmem_cache下又有kmem_cache_node来对应具体的内存池\nstruct kmem_cache &#123;    struct kmem_cache_node **node;    ....;&#125;struct kmem_cache_node &#123;    struct list_head slabs_partial;    struct list_head slabs_full;    struct list_head slabs_free;&#125;\n\n\n在这张图片中, 左边的cache_chain中的每一个具名实例就是一个kmem_cache的实例, 不同大小的对象都有不同的cache\n分配器的接口\n\nkmem_cache_create: 创建一个基于slab的内核对象管理器\nkmem_cache_alloc: 快速为某个对象申请内存\nkmem_cache_free: 归还对象占用的内存给slab管理器\n\n常用命令查看内核对象slabinfo, 可以看到vm_struct_area这个老熟人, active_objs有10393个, num_objs有10674个, objsize是216, objperslab是18, pagesperslab是1, 对应的含义就是, 对于这个内核对象\n\n活跃的对象数量是10393\n对象总数是10674\n对象的大小是216 bit\n每个slab中能存18个这个对象\n每个slab占用1个page\n\n# cat /proc/slabinfo\n\n\n\n查看内核对象的内存情况\n# slabtop\n\n\n","categories":["Computer Fundamentals","OS","Memory"],"tags":["Computer Fundamentals","OS","Memory"]},{"title":"操作系统I/O - 零拷贝","url":"/2025/07/25/computer-fundamentals/Operating%20System/IO-%E9%9B%B6%E6%8B%B7%E8%B4%9D/","content":"I&#x2F;O的演进DMA技术在最开始的时候, 读一个文件的过程是\n\n\n用户进程调用read(), 切换到内核态, CPU向磁盘发起IO请求\n磁盘将数据放入到磁盘控制器缓冲区里面, 发送IO中断信号给CPU\nCPU将数据从磁盘数据控制器缓冲区中拷贝到 PageCache\nCPU将数据从PageCache中拷贝到用户缓冲区\nread()调用返回, 切换到用户态\n\n一共发生了两次用户态和内核态的上下文切换, 发生了两次拷贝, 并且数据的搬运也是交给CPU来操作的, 占用了大量的CPU的时间, 导致了CPU吞吐量下降\n解决方式就是引入了DMA(Direct Memory Access)直接内存访问技术, \n我们将把数据从磁盘控制器缓冲区搬运到PageCache的工作交给了DMA控制器来进行\n\n现在的读一个文件的流程变成了\n\n用户进程调用read(), 切换到内核态, 向操作系统发起IO请求, 将数据读取到自己的内存缓冲区中, 进程进入到了阻塞状态\nCPU收到操作系统发送的指令以后, 将请求发送给DMA, DMA再进一步发送给磁盘\n磁盘将数据放入到磁盘控制器缓冲区里面, 发送IO中断信号给DMA控制器, 告知缓冲区已满\nDMA收到磁盘的信号, 将磁盘缓冲区中的数据拷贝到内核缓冲区中, 此时不占用CPU, CPU可以执行其他的任务\nCPU将数据从PageCache中拷贝到用户缓冲区\nread()调用返回, 切换到用户态\n\n零拷贝我们如果要使用网络传输一个文件, 最初是怎么实现的\n我们一般要用到两个系统调用\nread(file, tmp_buf, len);write(socket, tmp_buf, len);\n\nread读取文件到buf中, write再将这个buf中的内容通过socket发送给客户端, 在这个过程中, 我们会经历四次用户态和内核态的上下文切换, 四次拷贝\n\n\n用户调用read()系统调用, 切换内核态\n将文件从磁盘控制器缓冲区DMA拷贝到内核缓冲区\n将文件从内核缓冲区CPU拷贝到用户缓冲区\n切换到用户态\n用户调用write(), 切换到内核态\n将用户缓冲区中的数据CPU拷贝到socket缓冲区中\n将socket缓冲区的数据DMA拷贝到网卡中, 发送数据\n切换回用户态\n\n我们要优化这种IO形式, 关键就在于减少内核态与用户态的上下文切换, 并减少拷贝的次数\n\n怎么减少用户态和内核态的切换, 只要执行了一次系统调用, 不可避免地会有两次切换, 所以核心思路就是减少系统调用的数量\n\n\n怎么减少拷贝的数量, 在这个文件出书的场景中, 其实用户缓冲区是完全没有必要的存在\n\nmmap + write使用mmap代替read函数\nbuf = mmap(file, len);write(sockfd, buf, len);\n\nmmap系统调用直接把内核缓冲区中的数据映射到用户空间, 这样我们就相当于不使用用户缓冲区中转数据, 减少了一次数据的拷贝\n\n现在PageCache中的数据直接通过CPU拷贝到scoket缓冲区中\n但是这种形式系统调用的次数仍然是两次, 也就是内核态和用户态的上下文切换的次数仍然是四次\nsendfile#include &lt;sys/socket.h&gt;ssize_t sendfile(int out_fd. int in_fd, off_t *offset, size_t count);\n\n这个系统调用能直接将mmap + write合并, 能减少一次系统调用的次数\n并且如果网卡支持SG-DMA技术(The Scatter-Gather Direct Memory Access), 还能再减少一次数据拷贝\n\n\n通过DMA将磁盘的数据拷贝到内核缓冲区\n缓冲区描述符和数据长度传到socket缓冲区, 网卡的SG-DMA控制器直接将内核缓存中的数据拷贝到网卡的缓冲区中\n\n省去了将数据从内核缓冲区中拷贝到socket缓冲区中的过程\n这样就是零拷贝的最终形态, 只需要两次数据的拷贝, 两次内核态和用户态的上下文切换, 并且没有在内存层面的数据拷贝, 也就是这个过程全程是通过DMA来进行的, 不需要CPU参与\n大文件传输实现的特殊性PageCache我们常说的IO文件的时候的内核缓冲区, 就是PageCache(磁盘高速缓存). 在零拷贝中我们就使用了PageCache, 读文件时候, 会先将数据通过DMA控制器读到内核缓冲区中. 实际上PageCache就可以看作是磁盘和内存之间的中间层, 和CPU Cache是性质类似的东西\nPageCache有预读功能, 同时PageCache内部是一个LRU队列, 会维护热点数据. 如果我们读大文件, 就会导致LRU队列我们批量读取文件的时候的预读失效和缓存污染问题(当然Linux通过inactive_list和active_list以及升级策略很大程度上解决了这个问题). \n但是我们还是能很明显看出, 在读大文件的时候PageCache是没有什么作用的, \n\n读大文件预读功能没有什么用\n缓存最近被访问的数据, 大文件导致缓存污染\n\n相当于DMA多做了一次没有的数据拷贝\n直接I&#x2F;O我们直接绕开DMA, 并且不再拷贝到内核缓冲区中. 阻塞问题我们使用异步I&#x2F;O来解决\n\n\n用户进程调用系统调用, 发起异步I&#x2F;O读, CPU发送指令到磁盘, 这里会直接返回, 也就是不阻塞读\n然后磁盘准备好数据后发送IO中断信号, DMA将数据从磁盘控制器缓冲区拷贝到用户缓冲区\n通知用户进程读取成功了\n\n直接I&#x2F;O绕过了PageCache就无法享受到内核提供的两点优化\n\n内核的I&#x2F;O调度算法会缓存尽可能多的I&#x2F;O请求在PageCache中, 最后合并成一个更大的I&#x2F;O请求, 减少I&#x2F;O次数\n无法享受到预读机制\n\n\n参考文档:\n小林coding 9.1什么是零拷贝\n\n","categories":["Computer Fundamentals","OS","IO"],"tags":["Computer Fundamentals","OS","IO"]},{"title":"操作系统进程管理-Linux进程与线程","url":"/2025/08/12/computer-fundamentals/Operating%20System/Linux%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B/","content":"进程与线程进程的创建fork函数是linux中创建进程的核心函数, fork的原意是叉子, 也就是分叉. fork调用是程序执行的一个分叉点, 从这里开始, 原本的一个执行流变成了两个独立的执行流\n创建的子进程继承了父进程的资源\n\n打开的文件描述符\n文件系统信息\n…\n\n创建的子进程在创建的时候是和父进程一样的内存空间, 会将父进程的地址空间也就是页表复制, 并复制所有的VMA, 但是标记为只读, 在修改的时候会触发page fault, 分配新的物理页, 复制数据, 更新页表项为可写\n值得一提的是这里的继承和复制都是深拷贝, 也就是会将fs_struct, mm_struct, file_struct等资源都是深拷贝, 这里虽然是继承过去了, 但是实际上已经和父进程的资源是隔离的了, 只是在最开始的时候数据是完全相同的\ndoforkfork函数是以一个系统调用的形式存在的, 这个系统调用执行的内容就是执行dofork\nSYSCALL_DEFINE0(fork)&#123;\treturn do_fork(SIGCHLD, 0, 0, NULL, NULL);&#125;\n\n\n6.1版本及以后调用的是kernel_clone\n\ndo_fork函数传入的参数中flag是核心参数, 也是在同样是调用do_fork函数, 为什么创建进程和创建线程的时候do_fork的行为不一样, 原因就是传入的flag不一样\n可传入的flag有很多\n// file:include/uapi/linux/sched.h#define CLONE_VM 0x00000100 /* set if VM shared between processes */#define CLONE_FS 0x00000200 /* set if fs info shared between processes */#define CLONE_FILES 0x00000400 /* set if open files shared between processes */...#define CLONE_NEWNS 0x00020000 /* New mount namespace group */...#define CLONE_NEWCGROUP 0x02000000 /* New cgroup namespace */#define CLONE_NEWUTS 0x04000000 /* New utsname namespace */#define CLONE_NEWIPC 0x08000000 /* New ipc namespace */#define CLONE_NEWUSER 0x10000000 /* New user namespace */#define CLONE_NEWPID 0x20000000 /* New pid namespace */#define CLONE_NEWNET 0x40000000 /* New network namespace */\n\n\nCLONE_VM: task之间共享虚拟地址空间\nCLONE_FS: task之间共享文件系统信息\nCLONE_FILES: task之间共享打开的文件描述符\n\n还有几个是和命名空间, cgroup相关的\n\nCLONE_NEWS: 新任务会创建一个新的挂载点命名空间 (隔离文件系统挂载点)\nCLONE_NEWGROUP: 新任务会创建新的CGroup\nCLONE_NEWIPC: 新任务会创建新的IPC命名空间 (隔离主机名和域名)\nCLONE_NEWUTS: 创建新的UTS命名空间(隔离主机名和域名)\nCLONE_NEWUSER: 新任务创建新的User命名空间 (隔离用户ID和组ID)\nCLONE_NEWPID: 新任务创建新的PID命名空间 (隔离进程的PID)\nCLONE_NEWNET: 新任务创建新的网络命名空间 (隔离网卡设备路由表等)\n\n\n这里创建了新的命名空间, 虽然说是”隔离”, 实际上只是在可见性上做了屏蔽处理, 并不是实际的像进程的地址空间一样的完全的隔离\n\n这里传入的SIGCHLD的含义是子进程终止后发送SIGCHLD信号通知父进程, 没有设置其他的flag\n无论是do_fork函数还是6.1版本的kernel_clone, 其核心都是一个copy_process函数, 这个函数拷贝父进程的方式创建一个新的进程. 然后调用wake_up_new_task将新的进程添加到调度队列中等待调度\n\ncopy_process\n\n这个函数比较长, 我们分阶段说明\n1. 复制父进程的task_struct结构体在这一步会将父进程的task_struct完全地一模一样的, 只是复制值的, 类似浅拷贝地复制过去, 核心函数是调用dup_task_struct\n申请task_struct对象的时候, 是调用的alloc_task_struct_node(node), 这个函数就是调用的slab分配器从slab内核内存管理区中申请一块内存出来\n这一步值得注意的是在这个时间节点, 两个task_struct是完全一样的,  mm, fs等指针都是一样的, 只是拷贝了task_struct本身, 仍然和current(父进程)指向相同的对象\n\n2. 拷贝files_struct调用copy_files函数, 这一步会传入flag以clone_flag的形式\nstatic int copy_files(unsigned long clone_flags, struct task_struct *tsk) &#123;    struct files_struct *oldf, *newf;    oldf = current-&gt;files;    if (clone_flags &amp; CLONE_FILES) &#123;        atomic_inc(&amp;oldf-&gt;count);        goto out;    &#125;    newf = dup_fd(oldf, &amp;error);    tsk-&gt;files = newf;    ...&#125;\n\nclone_flags &amp; CLONE_FILES操作, 用于检测flag中有没有CLONE_FILES的flag, \n如果有, 说明进程之间共享打开的文件描述符, 让新的进程的files_struct指向父进程的files_struct, 增加一下引用计数以后, 就通过out返回了\n如果flag中没有CLONE_FILES, 说明要重新创建一个新的struct files_struct, 这个时候就会执行到dup_fd创建一个新的原本的fd的副本\n\n为新的files_struct申请内存, 调用的是kmem_cache_alloc\n然后对新的files_struct进行初始化, 这个新的创建的files_struct和原本的fd的值是一样\n\n执行完毕以后, 新的进程就有自己的fd了\n3. 拷贝fs_struct调用copy_fs函数, 这里的逻辑和上面的copy_files是一样的\n\n检测传入的flag里面有没有CLONE_FS, 有则fs_&gt;user++; return, 没有则执行copy_fs_struct创建一个父进程的副本\n\n4. 拷贝mm_struct调用copy_mm函数\n\n检测传入的flag里面有没有CLONE_VM, 如果没有会通过dup_mm申请一个新的地址空间出来, 通过allocate_mm申请了新的mm_struct, 并且将当前进程的地址空间拷贝到了新的mm_struct中用于初始化\n\n虽然这里申请了新的地址空间, 但初始化的时候, 新的地址空间和当前进程的地址空间是完全一样, 所以子进程也能直接使用父进程中加载的可执行程序, 全局数据等(但是对于子进程来说, 这些公用的地址空间是只读的, 如果想要修改共享的地址空间, 会触发page_fault, 修改页表, 映射到新的物理内存地址上)\n5. 拷贝进程的命名空间nsproxy创建进程或线程的时候, 可以让内核帮我们创建独立的命名空间, 在fork系统调用中, 创建进程没有指定命名空间相关的flag, 所以新旧进程仍然是共用的一套命名空间\n\n6. 申请pid通过alloc_pid为当前任务申请pid\n在申请pid内核对象的时候, 需要传入pid_namespace, 然后创建pid_namespace-&gt;level个pid, 因为这个进程需要在每个命名空间都创建一个pid, 比如我们容器中的一个进程, 这个进程会在容器中有一个pid, 在宿主机也有一个pid, 也就是有两个pid\n通过idr_alloc调用分配一个空闲的pid编号, 在3.1版本中申请进程号的函数不是idr_alloc而是alloc_bitmap.\n在那个版本中, 所有的pid分配情况都是通过bitmap来管理的, bitmap最大的优点就是节省内存, 局部性很好, 但是也带来了每要获取一个没有空闲的pid都需要遍历bitmap的缺陷, 也就是获取pid是一个O(n)的操作\n\n这里的第三个bit是1, 也就是3这个pid被使用过了.\n随着容器技术和硬件的发展, 核数和进程数快速增长, 并且内存越来越大, bitmap节省内存方面的收益已经弥补不了它获取PID O(n)的弊端, 在之后通过基数树来组织pid\n\n基数树 (内核中有4bit和6bit两种, 默认使用6bit)\n\n基数树是前缀树的一个变种, 如果使用前缀树, 我们要记录一个pid(32bit的整数)是不是使用过, 需要32层的树(实际上取决于最深的叶子节点的层高, 最大为32层). 为了降低层高, 每层树不只记录1bit的信息, 而是6bit\n\n前缀树: 记录1和3使用过\n\n\n\n基数树\n\n基数树每6bit作为一层, 也就是每层有64个槽位\n\nstruct xa_node &#123;\t...\tunsigned char shift;    void __rcu *slots[XA_CHUNK_SIZE];    union &#123;        unsigned long tags[XA_MAX_MARKS][XA_MARK_LONGS];    \t...    &#125;&#125;\n\n\nshift: 表示自己在数字中的表示第几段数字, 每6bit是一段. 最低一层的shift&#x3D;0, 倒数第二层 shift&#x3D;6, 以此类推\nslots: 一个指针数组, 存储的是其指向的子节点的指针, 没有下一级的节点的时候指向null\ntags: 记录每个slot数组中每一个下表的存储状态, 用来表示每一个slot是否已经分配出去的状态. 一个long类型的数组, 一个long类型刚好是64bit\n\n在基数树的基础上判断一个整数值是否存在, 或者是从这个树上分配一个新的未使用过的整数ID出来的时候, 只需要对树节点进行遍历, 分别查看每一层中的tag状态位, 看slots对应的下标是否已经占用.\n7. 进入就绪队列\n在copy_process执行完毕的时候, 表示新进程的一个task_struct就创建出来了, 接下来内核会调用wake_up_new_task将这个新创建出来的子进程添加到就绪队列中等待调度\n线程的创建这里不讨论线程持有的资源有内核态和用户态共同创建之类的问题(这个问题可以看到这篇文章), 主要从内核态的角度来看操作系统是怎么创建代表线程的task_struct的, 和创建进程的task_struct有什么不同\n","categories":["Computer Fundamentals","OS","Process"],"tags":["Computer Fundamentals","OS","Process"]},{"title":"操作系统内存管理 - Linux虚拟内存管理","url":"/2025/08/04/computer-fundamentals/Operating%20System/Linux%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","content":"虚拟内存管理以问题来引入\n\n申请内存申请到的真的是物理内存吗\n对虚拟内存的申请如何转化成对物理内存的访问?\ntop命令输出进程的内存指标中VIRT和RES分别是什么含义\n堆栈的大小限制是多大, 当堆栈发生溢出以后应用程序会发生什么\n进程栈和线程栈是一个东西吗\nmalloc大概是怎么工作的\n\n虚拟内存和物理页为什么要有虚拟内存\n用户进程访问内核数据要加以限制\n用户进程之间需要隔离\n内存不足的时候swap到硬盘上, 保证系统可正常运行\n\n虚拟地址空间到底是什么在内核中的定义, 每个进程的task_struct都有一个核心对象 - mm_struct类型的mm. 代表的就是进程的虚拟地址空间\nstruct task_struct &#123;    ...        struct mm_struct \t*mm;&#125;\n\n在这个虚拟内存空间里面, 每一段已经分配出去的地址范围都是通过一个个虚拟内存区域VMA来表示, 也就是对应到内核中的数据结构vm_area_struct\nstruct vm_area_struct &#123;\tunsigned long vm_start;    unsigned long vm_end;    ...&#125;\n\n其中vm_start和vm_end就是使用了的虚拟地址范围的开始和结尾\n通过一个个的vm_area_struct就组成了这个进程已经分配出去的地址范围, 加起来就是对整个虚拟地址空间的占用情况. 内核会保证vm_area_struct的范围之间不会有交叉的情况出现\n内存访问的过程中, 需要经常查找虚拟地址和某个vm_area_strcut的对应关系. 所以我们需要合适的数据结构来组织多个vm_area_strcut\n在Linux6.1之前, 使用的是红黑树来提供logn的查询效率, 双链表来提高高效遍历效率\nstruct mm_struct&#123;    ...    // 双向链表    struct vm_area_struuct *mmap;    // 红黑树    struct rb_root_mm_rb;    // 锁    struct rw_semaphore mmap_sem;&#125;\n\n这个方案最明显的缺陷就是随着现在的服务器上的核数越来越多, 多线程情况下锁争抢问题. 需要加锁的原因是红黑树的平衡操作牵扯到多个节点, 以及修改需要同步到双向链表. 所以这两个数据结构都需要加锁修改.\n在Linux6.1以后, 对VMA管理换成了maple tree. 是按照RCU无锁编程方式来实现的\nstruct mm_struct &#123;    struct &#123;         struct maple_tree mm_mt;         ...     &#125;&#125;// file:include/linux/maple_tree.hstruct maple_tree &#123;     ...     void __rcu *ma_root;     unsigned int ma_flags; &#125;\n\n\n什么时候分配的物理页这个其实是个老生常谈的话题, 也是面试常考的问题, 答案就是是在触发缺页中断的时候, 但是这里我们将深入到Linux在触发中断以后的调用的具体的函数和涉及到的具体的数据结构来看\n当进程在运行的过程中, 在栈上开始分配和访问变量的时候, 如果物理页还没有分配,这个时候就会触发缺页中断. 在中断处理函数中来真正分配物理内存\n内存缺页中断的核心处理入口do_user_addr_fault(unsigned long address)函数\n\nvma = find_vma(mm, address): 根据新的addr来找到对应的vma\nfault = handle_mm_fault(mm, vma, address, flags)调用handle_mm_fault函数来真正地分配物理内存申请\n\n\nfind_vma(mm, address)\n这个函数会优先尝试从vmacache中获取, 这是软件层面的cache, 由进程持有,  每个进程有自己VMA缓存, 缓存的内容是最近访问VMA. 下面的两个遍历操作, 都是在缓存没有命中的时候进行的\n\n在Linux6.1以前, 因为mm_struct中的vm_area_struct是通过红黑树和双向链表组织起来的, find_vma的实现是通过遍历VMA双向链表, 找到满足vm_start &lt;&#x3D; vma &lt;&#x3D; vm_end的VMA并返回()\n在6.1及以后, 组织VMA的数据结构也变成了maple_tree, find_vma也就变成了调用maple tree的查找函数mas_walk来查询了\n在find_vma找到了正确的vma以后, 就进入到了真正的物理内存的分配, 会依次调用handle_mm_fault-&gt;__handle_mm_fault来完成物理内存分配\n\n__handle_mm_fault(vma, addr…)\n\n首先要介绍在现在的64位的cpu下, 四级的虚拟内存页表\n\n一级页表: Page Global Dir, 简称 pgd\n二级页表: Page Upper Dir, 简称pud\n三级页表: Page Mid Dir, 简称pmd\n四级页表: Page Table, 简称pte\n\n在__handle_mm_fault(vma, addr...)中\n\n依次查看或申请每一级的页表项\n完成对页表的处理以后, 在handle_pte_fault(vm_fault)中进入到do_anoymous_page中进行处理\n在do_anonymous_page中调用alloc_zeroed_user_highpage_movable分配一个可移动的匿名物理页出来, 在底层会调用伙伴系统的alloc_pages进行实际物理页面的分配\n\n\nhandle_pte_fault(vm_fault)会处理很多种内存缺页处理, 比如文件映射缺页处理, swap缺页处理, 写时复制缺页处理, 匿名映射页缺页处理等情况. 开发者申请的变量内存对应的是匿名映射页处理, 会进入到do_anonymous_page\n\n\n内核是用伙伴系统来管理所有的物理页的 其他的模块需要物理页的时候都会调用伙伴系统对外提供的函数来申请物理内存\n\n小结\n 申请内存申请到的真的是物理内存吗\n\n申请的时候实际上只会申请VMA, 真正的物理内存是等到访问的时候触发缺页中断, 再调用alloc_pages从伙伴系统中申请\n进程的堆栈的物理内存分配时间也是同样的, 创建进程的时候新进程的栈内存分配的也只是一段地址空间范围\n\n\ntop命令输出进程的内存指标中VIRT和RES分别是什么含义\n\n\nVIRT就是进程使用的虚拟内存的大小, RES就是进程实际申请的物理内存的大小\n虚拟内存使用方式整个进程的运行过程中, 几乎都是在围绕着对虚拟内存的分配和使用而进行的, 具体的使用方式主要有三种\n第一类是操作系统加载程序时在加载逻辑里对新进程的虚拟内存的设置和使用\n\n程序启动时, 加载程序会将程序代码段, 数据段通过mmap映射到虚拟地址空间中\n对新进程初始化栈区和堆区\n\n第二类是程序运行期间动态地对所存储各种数据进行申请和释放. 涉及到栈, 进程线程运行时函数调用, 存储局部变量使用到的都是栈\n第三类是堆, 各种开发语言运行时通过new, malloc等函数就是从堆中分配内存. 这类内存申请和释放需要依赖操作系统提供的关系虚拟地址空间相关的mmap, brk等系统调用来实现.\n进程启动时对虚拟内存的使用在进程加载完毕以后, 在解析完ELF文件信息以后\n\n为进程创建新的地址空间, 同时为其准备一个默认大小4KB的栈\n将可执行文件以及它所依赖的各种动态链接so库通过elf_map函数映射到虚拟地址空间中\n对堆区进行初始化\n\n在程序加载启动成功以后, 在进程的地址空间中的代码段, 数据段设置完毕, 堆, 栈也都准备好了\n底层实现上, 上面四者在底层都是对应一个个的vm_area_struct对象, 每一个vma都表示这段虚拟内存空间已经被分配和使用了\n\n对于栈的申请\n\n是在execve依次调用do_execve_common, bprm_mm_init, 最后在__bprm_mm_init中申请的vma对象\n\n对于可执行文件以及进程所依赖的各种so动态链接库\n\nexecve时依次调用do_execve_common, search_binary_handler, load_elf_binary, elf_map, 调用mmap_region申请vm_area_struct对象, 最终将可执行文件中的代码段, 数据段等映射到内存中\n\n对于堆内存\n\n在load_elf_binary的最后set_brk初始化堆的时候, 依次调用vm_brk_flags, 最后成功申请vma对象\n可以使用cat/proc/&lt;pid&gt;/maps来查看进程的虚拟地址空间概要\nmmap这个系统调用是虚拟内存管理中提供的最接近底层调用, 也是最常用的mmap. 可以用于文件映射和匿名映射, 这里我们忽略文件映射. \n\n匿名映射这个名字有一些的迷惑性, 实际上是文件映射有文件, 而匿名映射没有对应的物理文件, 直接叫普通内存地址空间会更容易理解\n\n匿名映射的过程实际上就是向内核申请一段可用的内存地址范围而已, 非常简单. 也就是调用了mmap以后, 内核就会多申请一个vm_area_struct出来. 表示这段内存可用, 然后返回给用户.\nmmap的调用逻辑比较深, mmap &#x3D;&gt; ksts_mmap_pgoff &#x3D;&gt; vm_mmap_pgoff &#x3D;&gt; do_mmap_pgoff &#x3D;&gt; do_mmap &#x3D;&gt; mmap_region. 最关键的就是最后的mmap_region\n在这个位置会申请新的vm_area_struct, 并对其初始化(初始化vm_start和vm_end), 最后返回addr, 也就是虚拟地址的起始位置\n这个时候用户申请的虚拟空间就申请好了, 用户就可以使用了.\nsbrk&#x2F;brk在set_brk中会先为数据段申请虚拟内存, 然后初始化堆区的指针, mm_struct-&gt;brk &#x3D; end(这里的end是数据段的末尾)\n从set_brk开始, 依次调用到do_brk_flags的时候申请堆区内存(也就是申请了vma), 然后初始化mm中和堆区相关的值, brk指向的就是堆区的终止地址, start_brk指向堆区的起始地址(也就是数据段的末尾)\n\nsbrk系统调用: 返回mm_struct-&gt;brk的指针值\nbrk系统调用: 修改mm_struct-&gt;brk的指针值\n往大了改就是加大堆区\n往小了改就是缩小堆区\n\n\n\n进程栈内存的使用\n从问题开始\n\n进程栈的大小限制是多少\n栈限制的大小可以调整吗, 可以的话怎么调整\n栈溢出以后会发生什么\n\n\n进程栈的初始化加载系统调用execve依次调用do_execve_common,  \n\n在bprm_mm_init的时候申请一个全新的地址空间mm_struct对象, 准备给新的进程使用.\n申请完地址空间以后, 调用__bprm_mm_init为新进程的栈申请一页大小的虚拟内存空间, 作为给新进程准备的栈内存. 申请完以后, 把栈的指针(vma-&gt;vm_end - sizeof(void *))保存到bprm-&gt;p中记录起来\n\nbprm-&gt;vma = vma = vm_area_alloc(mm);vma-&gt;vm_end = STACK_TOP_MAX;vma-&gt;vm_start = vma-&gt;vm_end - PAGE_SIZE;...bprm-&gt;p = vma-&gt;vm_end - sizeof(void *);\n\n接下来进程加载过程会使用load_elf_binary真正开始加载可执行二进制程序. 在加载时把前面的进程栈的地址空间指针设置到新进程mm对象上\ncurrent-&gt;mm-&gt;start_stack = bprm-&gt;p;\n\n这个时候, 新的进程就能使用进程栈内存了\n栈的自动增长随着进程的运行, 进程的栈空间超过4KB是难免的问题, 这个时候进程的栈空间就需要增长\n首先会进入到缺页处理函数里面__do_page_fault, 然后会进入到处理用户进程的缺页异常处理函数do_user_addr_fault\n\ndo_user_addr_fault\n\n// arch/x86/mm/fault.cvoid do_user_addr_fault(struct pt_regs *regs,\t\t\tunsigned long error_code,\t\t\tunsigned long address)&#123;   \t...     // 如果vma的start比addr小, 说明现在使用的栈地址是在栈的范围内的, 不需要额外申请虚拟地址 (vma的start指向低地址)\tif (likely(vma-&gt;vm_start &lt;= address))\t\tgoto good_area;        // vma的开始地址比addr大, 说明需要额外申请虚拟内存空间    // 通过判断VM_GROWDOWN来判断可不可以动态扩张\tif (unlikely(!(vma-&gt;vm_flags &amp; VM_GROWSDOWN))) &#123;\t\tbad_area(regs, error_code, address);\t\treturn;\t&#125;        // 执行栈的动态扩张\tif (unlikely(expand_stack(vma, address))) &#123;\t\tbad_area(regs, error_code, address);\t\treturn;\t&#125;\t/*\t * Ok, we have a good vm_area for this memory access, so\t * we can handle it..\t */good_area:\t...    fault = handle_mm_fault(vma, address, flags, regs);&#125;\n\n\n__do_page_fault实际上是之前版本的缺页处理函数, 在6.1已经更名为handle_page_fault, 这里为了和前面以及市面上广泛的教材保持一致, 使用前者\n\n栈一般是向下的增长的, 如果vma-&gt;start &gt; addr表示栈不够用了, 这个时候就需要调用expand_stack进行扩张\n\n其实在Linux栈地址空间增长是分成两种的, 一种是从高地址往低地址增长, 一种是反过来. 大部分情况都是由高往低增长. 这里只以向下增长为例\n\n// mm/mmap.cint expand_stack(struct vm_area_struct *vma, unsigned long address)&#123;\treturn expand_downwards(vma, address);&#125;int expand_downwards(struct vm_area_struct *vma, unsigned long address)\t\t    // 计算扩张以后的栈的大小    size = vma-&gt;vm_end - address;\t// 计算需要扩张的页面数量\tgrow = (vma-&gt;vm_start - address) &gt;&gt; PAGE_SHIFT;\t// 校验能否扩张\terror = acct_stack_growth(vma, size, grow);\t// 修改vm_start, 开始扩张\tvma-&gt;vm_start = address;&#125;\n\n这个函数进行了两个运算\n\n计算出来扩张以后栈的大小. size = vma-&gt;vm_end - address;\n计算需要扩张以后的页面数量. grow = (vma-&gt;vm_start - address) &gt;&gt; PAGE_SHIFT;\n\n扩充的操作就是简单粗暴的修改vm_start就行\n接下来看到acct_stack_growth进行了哪些限制判断\n// mm/mmap.cstatic int acct_stack_growth(struct vm_area_struct *vma,\t\t\t     unsigned long size, unsigned long grow)&#123;\t/* 检查地址空间是否超过限制 */\tif (!may_expand_vm(mm, vma-&gt;vm_flags, grow))\t\treturn -ENOMEM;\t/* 检测是否超出栈的大小限制 */\tif (size &gt; rlimit(RLIMIT_STACK))\t\treturn -ENOMEM;\t    ...\treturn 0;&#125;\n\nacct_stack_growth主要进行了两个判断\n\nmay_expand_vm判断的是增长完这几个页以后, 是否超出整体虚拟地址空间大小的限制\nrlimit判断是否超出栈的大小限制\n\n后者栈的大小限制, 能通过命令查看\n# ulimit -a...max memory size         (kbytes, -m) unlimitedstack size              (kbytes, -s) 8192virtual memory          (kbytes, -v) unlimited\n\n也能通过命令来修改\n\n重启后会丢失\n\nulimit -s &lt;new_size&gt;\n\n\n重启后不会丢失\n\nvim /etc/security/limits.conf...# 添加下面这行* soft stack &lt;new_size&gt;\n\n线程栈是怎么使用内存的在Linux内核里面其实并没有线程这个概念, 内核原生的clone系统调用只是支持生成一个和父进程共享地址空间等资源的轻量级进程而已\n\nLinux的线程包含了两部分的实现\n\n第一部分是用户态的glibc库, 创建线程的时候调用的pthread_create就是在glibc实现的, glibc完全是用户态运行的, 不是内核源码\n内核态的clone系统调用, 内核通过clone系统调用可以创建出来和父进程共享地址空间的轻量级用户进程\n\nglibc的线程对象线程包括了内核态和用户态的两部分的实现, 所对应的资源也是同样的分成了两部分, 一部分是内核资源, 例如代表轻量级进程的内核对象task_struct, 另一部分就是用户态的资源, 包含线程栈. 在C语言glibc中的实现中, 用户资源这一部分的核心的数据结构就是pthread. 存储了线程的相关信息, 包含了线程栈\n// file:nptl/descr.hstruct pthread&#123;    pid_t tid;    ......;    // 线程栈内存    void *stackblock;    size_t stackblock_size; &#125;\n\n\ntid对象存储了线程的ID值\nstackblock指向了线程栈内存\nstackblock_size表明栈的内存区域大小\n\n线程栈的创建pthread_create的调用路径是__pthread_create_2_1 -&gt; create_thread\n\ncreate_thread\n\n\n定义线程对象 pthread\n执行ALLOCATE_STACK\n确定栈空间的大小\n申请用户栈内存\n\n\n创建用户进程\n\nALLOCATE_STACK是一个宏, 最后会调用到allocate_stack函数\n\nallocate_stack\n\n\n确定栈空间大小: size = attr-&gt;stacksize ?: __default_stacksize;如果用户创建线程的时候指定了栈的大小, 就会使用用户指定的大小, 如果没有指定, 就会使用默认的大小\n\n\n在__pthread_initialize_minimal_internal中会为__default_stacksize这个变量赋值, 逻辑是\n\n如果ulimit没有配置或者配置的是无限大, 那么大小就是ARCH_STACK_DEFAULT_SIZE （32MB）\n如果用户配置的太小了, 可能会导致程序无法正常运行,  glibc也给了一个PTHREAD_STACK_MIN (16384B)\n在ulimit配置合理的情况下, 会将配置数值对齐一下就会使用了\n\n\n\n确定完栈空间大小以后, 就要执行栈空间的申请了\n首先尝试通过get_cached_stack获取一块缓存直接用\n假设没有取到缓存, 就使用mmap系统调用直接申请一块匿名页内存空间\n将pthread对象放到栈上\n将栈添加到全局在用栈的链表中管理起来\n\n\n\n\n通过这里的我们能发现, 线程栈是不能伸缩的, 并没有想进程栈一样, 提供了伸缩的函数, 和判断进程栈是不是用超了的逻辑, 所以线程栈要是用超了, 就要开始报错了\n\n在栈申请好以后, 在create_thread中调用do_clone系统调用开始创建\nglibc中的每个线程在结束阶段都会执行一个公共操作, 释放掉那些已结束线程的栈空间, 从stack_used移除, 放入到stack_cache中, 相当于析构函数\n小结\n进程栈和线程栈式一个东西吗\n\n进程栈\n\n持有的资源是完完全全的内核态的资源, \n在创建进程的时候完成初始化, \n默认是4KB, \n在访问栈内存的时候会执行expand_stack自动扩容, 有向下和向上两种扩容逻辑\n\n而Linux中glibc中的线程库是nptl线程, 包含了两部分的资源\n\n第一部分是内核态中的task_struct, 地址空间等内核对象. \n另一部分就是用户态的管理用户的线程对象的pthread, \n申请的栈内存最大32MB, 最小16384B, 可以在创建线程的时候指定大小, \n不能动态伸缩\n同时维护了全局链表stack_used和stack_cache, 结束的线程会做将栈内存从stack_used中移除, 添加到stack_cache中, 创建线程的时候会优先尝试从stack_cache中获取\n\n进程的栈默认是进程使用的, 线程是不能使用进程的栈的, 所以才需要额外的线程的的栈内存\nglibc中的每个线程在结束阶段都会执行一个公共操作, 释放掉那些已结束线程的栈空间, 从stack_used移除, 放入到stack_cache中, 相当于析构函数\n\n堆栈的大小限制是多大, 当堆栈发生溢出以后应用程序会发生什么\n\n进程的栈, 默认是4KB, 线程栈最小是16438B, 最大不超过32MB, 默认是ulimit -a中的栈的配置的大小, 栈溢出以后因为访问了没有申请的内存, 会发生Segmentation fault\n\n进程栈的大小限制是多少\n\n进程栈最大大小通过ulimit -a中的stack size来限制\n\n栈限制的大小可以调整吗, 可以的话怎么调整\n\n通过 ulimit -s &lt;size&gt;和修改 &#x2F;etc&#x2F;security&#x2F;limits.conf文件来调整\nmalloc堆内存分配原理如果只是使用裸的系统调用来使用和分配内存, 很容易出现的问题就是内部和外部的内存碎片, 同时每次申请内存都需要进入到内核态执行系统调用, 开销很大\n所以我们需要一个任意大小内存块管理的机制. 内存管理器的核心机制都是类似的. 都是预先向操作系统申请一些内存\n\n当我们申请内存的时候, 直接由分配器从预先申请好的内存池里申请\n释放内存的时候, 分配器会将这些内存管理起来, 通过一些策略来判断是否要将其回收给操作系统\n\nmalloc_chunk: 最小内存分配单位// file:malloc/malloc.cstruct malloc_chunk &#123;\tINTERNAL_SIZE_T prev_size; /* Size of previous chunk (if free). */\tINTERNAL_SIZE_T size; /* Size in bytes, including overhead. */\tstruct malloc_chunk* fd; /* double links -- used only if free. */\tstruct malloc_chunk* bk;&#125;;\n\n上面的内容字段实际上是malloc_chunk的header部分, 实际申请的内存是紧接着header的后面的. 这种机制使malloc_chunk非常的灵活.\n\nptmalloc: 内存分配器每次调用malloc申请内存的时候, 分配器都会给我们分配一个chunk出来, 并把body部分的user data地址返回给用户程序\nchunk如果还没有分配出去, 或者通过free释放的话. 内存实际上并不会归还给内核. 而是glibc又组织起来\nArenaptmalloc的第一个单元其实并不是malloc_chunk而是malloc_state, 可以简单的将一个malloc_state看作是一个内存池\n// file:malloc/malloc.cstruct malloc_state &#123;    // 锁, 用来解决在多线程分配时的竞争问题    mutex_t mutex;        // 分配区下管理内存的各种数据结构    ...            // Linked List     struct malloc_state *next;&#125;\n\n最开始的版本的malloc是使用全局锁来保证多线程竞争情况下的安全性, 但是一个全局锁导致竞争严重, 无法有效利用多核CPU, 于是分配器抽象出来了一个Arena的概念, 一个Arena就相当一个内存池, 只有使用同一个Arena的线程之间要竞争锁. \n\n什么时候创建Arena?\n\n在程序启动的时候会创建一个main_arena.\n//file: malloc/malloc.cstatic struct malloc_state main_arena =&#123;  .mutex = _LIBC_LOCK_INITIALIZER,  .next = &amp;main_arena,  .attached_threads = 1&#125;;\n\n\n\n在后续多线程调用malloc的时候\n\n尝试get空闲的arena, 如果没有尝试创建\n同时没有空闲的arena, 创建失败, 或者因为arena数量已经达到上限了, 无法创建arena, 这个时候才会出现多个线程之间复用一个arena然后出现malloc的锁竞争问题\n\n分配区下管理内存的数据结构struct malloc_state&#123;  ...;  /* Fastbins */  mfastbinptr fastbinsY[NFASTBINS];  /* Base of the topmost chunk -- not otherwise kept in a bin */  mchunkptr top;  /* Normal bins packed as described above */  mchunkptr bins[NBINS * 2 - 2];&#125;;\n\n这里剩下的就是和管理内存的相关的最核心的数据结构\n glibc会将相似大小的空闲chunk都串起来, 这样下次用户再来分配的时候, 找到链表就可以快速分配. 这样的一个链表被称为一个bin\n\nfastbins: 用于管理小块的内存, 用来支持小内存的快速申请\n\n\n\nunsorted bins, small bins, large bins\n\n\n\nunsorted bins\n\n在将内存free的时候, 并不会直接将chunk放到对应的bin上, 而是会先统一地放到unsorted bins上, unsorted bins相当于bins的缓冲区\n\nsmall bins, large bins\n\n分别管理32~1008B的chunk和大于1024B的chunk\n从地址空间的视角来看, 堆中的内存基本上是由一个又一个chunk组成的, 其中有个特殊的chunk是Top chunk, 它是所有没有申请了的但是没有被分配的内存的集合\n\nmalloc的工作过程申请内存步骤\n\n对用户请求的内存规范化, 对齐到32, 48, 64等字节数\nfast bin 申请: 如果申请的字节数小于fastbin管理的内存块的最大字节数, 则尝试从fastbin中获取, 申请成功则返回. 用于快速处理频繁的小内存管理\nsmall bin 申请: 尝试从small bins中申请内存, 申请成功返回, 处理32 - 1008之间大小的内存块的申请\nunsorted bin 申请: 尝试从unsorted bins中申请内存, 成功则返回. 这里是为了首先将free时回收的内存利用起来, 顺带地会把chunk往small large里面整理\nunsorted bin 申请: 尝试从unsorted bins中申请内存, 成功则返回. 这里是为了处理1024以上的大内存块的申请\ntop chunk 申请: 前面的申请都没有成功,尝试从top chunk中申请\n从操作系统中申请: 最后的兜底解决方法就是向操作系统中申请\n\n","categories":["Computer Fundamentals","OS","Memory"],"tags":["Computer Fundamentals","OS","Memory"]},{"title":"操作系统进程管理-进程实现原理","url":"/2025/08/11/computer-fundamentals/Operating%20System/Linux%E8%BF%9B%E7%A8%8B%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","content":"进程实现原理进程是一个程序运行时的实例, 一个程序要运行起来, 需要硬盘, 内存, CPU, 网络等资源, 如果这些部分都有用户手动来管理, 开发一个程序会变成一个极其繁琐和困难的事情, 操作系统针对这些程序运行时需要的资源抽象出来了进程这个概念. 进程持有并统一管理所有一个程序要运行时需要的资源\n对于资源的集合, 在概念中被称为PCB(Process Control Block), 而在Linux中对应的内核对象就是task_struct这个数据结构\nstruct task_struct &#123;    // 1. 进程的状态    volatile long state;        // 2. 进程的pid    pid_t pid;    pid_t tgid;        //3. 和进程树的关系 (父进程, 子进程, 兄弟进程)    struct task_struct __rcu *parent;    struct listhead children;    struct listhead sibling;    struct task_struct *group_header;        // 4. 进程优先级    int prio, static_prio, normal_prio;    unsigned int rt_priority;        // 5. 进程地址空间    struct mm_struct *mm, *active_mm;        // 6. 进程文件系统信息 (当前目录...)    struct fs_struct *fs;        // 7. 进程打开的文件描述符    struct files_struct *files;        // 8. namespace    struct nsproxy *nsproxy;&#125;\n\n\n进程的特性接下来更进一步地讲解进程中的中的各个特性, 也基本是围绕进程持有的资源展开\n进程状态// 1. 进程的状态volatile long state;\n\n通过top命令查看, 其中的S一列就是进程的State\n\n进程的简单的状态转化图\n\n其他的状态\n\nPID和进程树// 2. 进程的pidpid_t pid;pid_t tgid;\n\n这里的tgid是实际的进程的PID, 调用getgid()返回的也是tgid, pid在单线程的时候等于tgid, 在多线程的时候, 属于同一个进程的每个线程的tgid都是一样的, pid不同.\n通过ps -ef命令能看到所有正在运行的进程\n\n其中的PID一栏就是该进程的PID, PPID是其父进程的PID (PID是0的进程是systemd进程)\n//3. 和进程树的关系 (父进程, 子进程, 兄弟进程)struct task_struct __rcu *parent;struct listhead children;struct listhead sibling;struct task_struct *group_header;\n\n#pstree\n\n\n进程通过task_struct中的parent和children两个task_struct对象, 将进程按照父子关系组合成了一棵树, 其中所有的进程的最终父进程都是systemd (PID&#x3D;0)\n进程调度// 4. 进程优先级int prio, static_prio, normal_prio;unsigned int rt_priority;\n\n调度器分成实时调度器和完全公平调度器, 前者常用于内核task, 后者常用于用户态的task, 运行的时间会收到prio的影响, 这一部分内容会在后面详细说明\n进程内存地址空间// 5. 进程地址空间struct mm_struct *mm, *active_mm;\n\nstruct mm_struct &#123;    struct vm_area_struct * mmap; // 链表    struct rb_root mm_rb;\t // 红黑树        // 进程中的各个逻辑段的地址    unsigned long mmap_base;    unsigned long task_size;    unsigned long start_code, end_code, start_data, end_data;    unsigned long start_brk, brk, start_stack;    unsigned long arg_start, arg_end, env_start, env_end;&#125;\n\n\n所有的进程共用相同的内核内存区域, 并且内核内存是通过物理内存直接映射分配的\n同时对于内核线程, 它的task_struct中的mm是null, 因为它没有用户态的虚拟地址空间\n进程文件系统// 6. 进程文件系统信息 (当前目录...)struct fs_struct *fs;\n\nstruct fs_struct &#123;    ...;    struct path root, pwd;&#125;struct path &#123;    struct vfdsmount *mnt;    struct dentry *dentry;&#125;\n\n不同于接下来要介绍的打开的文件, 这个属性记录的是pwd(当前工作目录), root(根目录)等进程在文件系统的位置\n进程文件打开列表// 7. 进程打开的文件描述符struct files_struct *files;\n\n在内核中实际上是以一个数组的形式存在的, 我们大名鼎鼎的socket就是在这个位置存储的\n\n其中前三个打开的文件就是我们的标准输入, 标准输入, 标准错误, 这也是为什么这些std对应的数字是0,1,2\n进程的命名空间这块的内容其实是服务于容器技术的, 通过命名空间来提供可见性, 但是并不保证隔离性. 和CPP中的命名空间的概念差不多, 本质上还是提供可见性上的区别\n// 8. namespacestruct nsproxy *nsproxy;struct nsproxy &#123;    atomic_t count;    struct uts_namespace *uts_ns;    struct ipc_namespace *ipc_ns;    struct mnt_namespace *mnt_ns;    struct pid_namespace *pid_ns;    struct net\t\t\t *net_ns;&#125;\n\n对于本机非容器环境来说, 就一个固定的命名空间\n\n","categories":["Computer Fundamentals","OS","Process"],"tags":["Computer Fundamentals","OS","Process"]},{"title":"AI-Agent 工作流-Chain Workflow","url":"/2025/07/18/project/AI%20Agentic%20System/1.%20Workflow-ChainWorkflow/","content":"工作流-lian’shi\n参考\n\nhttps://www.anthropic.com/engineering/building-effective-agents\n\n\n流程说明对于链式工作流, 整体的呈现效果就是用户输入userInput以后, 这个userInput在输入以后会链式的向下执行, 在每个节点携带上一个节点的回答和使用当前节点的prompt来和LLM对话\n\n填充systemPrompts数组, 在这种情况下, Chain Workflow也被称作Prompts Chain工作流\n遍历每个prompt\n将上一轮的回答和现在的prompt组成一个新的input\n对话获取response\n\n\n输出最后的response\n\n\n代码\n代码\n\npublic String chain(String userInput) &#123;    String response = usertInput;        for (String promt : systemPrompts) &#123;        String input = String.format(&quot;&#123;%s&#125;\\n &#123;%s&#125;&quot;, prompt, response);        String response = chatClient.prompt(input).call().conmtent();                if(!check(response)) &#123;            return this.errorResponse;        &#125;    &#125;    return response;&#125;\n\n\n系统prompt示例\n\n// Step 1    &quot;&quot;&quot;    Extract only the numerical values and their associated metrics from the text.    Format each as&#x27;value: metric&#x27; on a new line.    Example format:    92: customer satisfaction    45%: revenue growth&quot;&quot;&quot;,// Step 2    &quot;&quot;&quot;    Convert all numerical values to percentages where possible.    If not a percentage or points, convert to decimal (e.g., 92 points -&gt; 92%).    Keep one number per line.    Example format:    92%: customer satisfaction    45%: revenue growth&quot;&quot;&quot;,// Step 3    &quot;&quot;&quot;    Sort all lines in descending order by numerical value.    Keep the format &#x27;value: metric&#x27; on each line.    Example:    92%: customer satisfaction    87%: employee satisfaction&quot;&quot;&quot;,// Step 4    &quot;&quot;&quot;    Format the sorted data as a markdown table with columns:    | Metric | Value |    |:--|--:|    | Customer Satisfaction | 92% | &quot;&quot;&quot;\n\n\n\n使用场景此工作流非常适合于任务可以轻松明晰地分解成多个固定子步骤的场景. 用延迟换取更高的准确性\n\n原文: \nWhen to use this workflow: This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.\n\n举例说明: \n\n撰写文章大纲, 检查大纲是否符合某些标准, 然后根据大纲完成文档\n从MCP中获取应用监控数据, 提取出来关键信息, 捕捉其中的异常, 推送给管理者\n\n","categories":["Project","AI Agentic System","Workflow"],"tags":["Project","AI Agentic System","Workflow"]},{"title":"AI-Agent 工作流-Routing Workflow","url":"/2025/07/18/project/AI%20Agentic%20System/2.%20Workflow-RoutingWorkflow/","content":"工作流-路由工作流流程说明用户输入userInput和可行的路由Map&lt;String, Object&gt; String是对这个路由的简介, Object就是实际的路由内容\n这里以角色分配, 将不同的任务分配给不同的promt角色的使用方式举例\n\n用户输入初始的userInput和Map&lt;String, String&gt;\n根据userInput和Map的keySet让LLM决策当前任务使用的prompt角色, 给出原因和选择, 使用结构化的输出到RoutingResponse中\n使用对应的prompt角色, 输出最后的结果\n\n\n代码\nroute方法\n\npublic String route(String userInput, Map&lt;String, Object&gt; routes, String mode) &#123;    RoutingResponse routingResponse = determineRoute(input, routes.keySet);        Object select = routes.get(routingResponse.getSelect());    \tswitch (mode) &#123;        case &quot;prompts&quot;:            // 选择不同的prompt解决这个问题            return chatClient.prompt((String) select + &quot;\\nInput: &quot; + userInput).call().content();        case &quot;modelChoose&quot;:            // 选择不同的client解决这个问题            return (chatClient) select.prompt(userInput).call().content();    &#125;&#125;\n\n\ndetermineRoute方法\n\nprivate String determineRoute(String input, Iterable&lt;String&gt; availableRoutes) &#123;\tString selectorPrompt = String.format(&quot;&quot;&quot;                Analyze the input and select the most appropriate support team from these options: %s                First explain your reasoning, then provide your selection in this JSON format:                \\\\&#123;                    &quot;reasoning&quot;: &quot;Brief explanation of why this ticket should be routed to a specific team.                                Consider key terms, user intent, and urgency level.&quot;,                    &quot;selection&quot;: &quot;The chosen team name&quot;                \\\\&#125;                Input: %s&quot;&quot;&quot;, availableRoutes, input);                                              RoutingResponse routingResponse = chatClient.prompt(selectorPrompt).call().entity(RoutingResponse.class);    return routingResponse.selection();&#125;\n\n适用场景适合处理存在不同的分类, 分别处理的复杂任务\n\n原文:\nWhen to use this workflow: Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model&#x2F;algorithm.\n\n\n将简单&#x2F;常见问题路由到较小的模型（如 Claude 3.5 Haiku），将困难&#x2F;不寻常的问题路由到功能更强大的模型（如 Claude 3.5 Sonnet），以优化成本和速度。\n将不同类型的客户服务查询（一般问题、退款请求、技术支持）引导到不同的下游流程、提示和工具中。\n\n","categories":["Project","AI Agentic System","Workflow"],"tags":["Project","AI Agentic System","Workflow"]},{"title":"AI-Agent 工作流-Parallelization Workflow","url":"/2025/07/18/project/AI%20Agentic%20System/3.%20Workflow-ParallelizationWorkflow/","content":"工作流-并行工作流流程说明\n用户输入input数组对应需要并行执行一系列任务, 适用于所有的任务的prompt, 以及线程池的数量\n将每个input并行化运行LLM\n等待所有的运行完毕后返回\n\n代码public List&lt;String&gt; parallel(String prompt, List&lt;String&gt; inputs, int nWorks) &#123;    ExecutorService executor = Executors.newFixedThreadPool(nWorkers);        try &#123;        List&lt;CompletableFuture&lt;String&gt;&gt; futures = input.stream()            .map(input -&gt; CompletableFuture.supplyAsync(() 0&gt; &#123;                try&#123;                    return chatClient.prompt(prmopt + &quot;\\nInput: &quot; + input).call().content();                &#125; catch (Exception e) &#123;                    throw new RuntimeException(&quot;Failed to process input: &quot; + input, e);                &#125;            &#125;, executor))            .collect(Collectors.toList());                // 等待所有的任务结束        CompletableFuture&lt;Void&gt; allFutures = CompletableFuture.allof(        \t\t\tfutures.toArray(CompletableFuture[]::new));        allFutures.join();                return futures.stream()            .map(CompletableFuture::join)            .collec(Collectors.toList());    &#125;&#125;\n\n适用场景当拆分后的子任务可以并行化以提高速度的时候, 或者需要多个视角或尝试来获取更高置信度的结果的时候. 涉及到多方面考量的复杂任务的时候, 每个考量都由单独的LLM调用处理, LLM的表现会更好\n\n原文:\nWhen to use this workflow: Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.\n\n\n投票场景, 多次运行相同的任务来获得不同的输出\n切分: 将并行化的任务交给不同的LLM来执行, 比如自动评估的时候, 或者防护机制, 一个LLM处理用户的查询, 另一个模型沙宣不适当的请求和内容\n\n","categories":["Project","AI Agentic System","Workflow"],"tags":["Project","AI Agentic System","Workflow"]},{"title":"AI-Agent 工作流-Orchestrator-workers","url":"/2025/07/18/project/AI%20Agentic%20System/4.%20Workflow-Orchestrator-workers/","content":"工作流-编排器-工人工作流流程说明这个工作流可以看作是ParallelizationWorkflow的自动分配任务版本\n中央LLM动态分解任务, 并将其委派给工作者LLM, 并综合结果\n\n用户输入任务描述input\n将用户的任务和编排器prompt输入给LLM用于划分任务\n将划分后的任务集合和工人prompt输入给LLM用于执行划分后的任务\n返回所有的执行结果\n\n代码\n执行代码, process\n\npublic FinalResponse process(String taskDescription) &#123;    // 获取编排器的编排结果    OrchestratorResponse orchestratorResponse = this.chatClient.prompt()        .user(u -&gt; u.text(this.orchestratorPrompt))        \t\t\t.param(&quot;task&quot;, taskDescription)        .call()        .entity(OrchestratorResponse.class);        // 执行每个任务    List&lt;String&gt; workerResponse = orchestratorResponse.tasks().stram().map(task -&gt; this.chatClient.prompt()     .user(u -&gt; u.text(this.workerPrompt)          .param(&quot;original_task&quot;, taskDescription)          .param(&quot;task_type&quot;, task.type())          .param(&quot;task_description&quot;, task.description())      ))      .call()      .content().toList();        return new FinalResponse(orchestratorResponse.analysis(), workerResponse);&#125;\n\n\n\n\norchestrator prompt\n\n&quot;&quot;&quot;Analyze this task and break it down into 2-3 distinct approaches:Task: &#123;task&#125;Return your response in this JSON format:\\\\&#123;&quot;analysis&quot;: &quot;Explain your understanding of the task and which variations would be valuable.Focus on how each approach serves different aspects of the task.&quot;,&quot;tasks&quot;: [\\\\&#123;&quot;type&quot;: &quot;formal&quot;,&quot;description&quot;: &quot;Write a precise, technical version that emphasizes specifications&quot;\\\\&#125;,\\\\&#123;&quot;type&quot;: &quot;conversational&quot;,&quot;description&quot;: &quot;Write an engaging, friendly version that connects with readers&quot;\\\\&#125;]\\\\&#125;&quot;&quot;\n\n\nworker\n\n&quot;&quot;&quot;Generate content based on:Task: &#123;original_task&#125;Style: &#123;task_type&#125;Guidelines: &#123;task_description&#125;&quot;&quot;&quot;\n\n适用场景非常适合无法预测所需子任务的复杂任务, 在拓扑结构上和并行化类似, 和并行化的关键区别是在于灵活性-子任务不需要预先定义, 而是由编排器根据具体的输入确定\n\n原文:\nWhen to use this workflow: This workflow is well-suited for complex tasks where you can’t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it’s topographically similar, the key difference from parallelization is its flexibility—subtasks aren’t pre-defined, but determined by the orchestrator based on the specific input.\n\n\n每次对多个文件进行复杂更改的编码产品\n搜索任务涉及从多个来源搜集和分析信息以获取可能相关的信息\n\n","categories":["Project","AI Agentic System","Workflow"],"tags":["Project","AI Agentic System","Workflow"]},{"title":"AI-Agent 工作流-Evaluator-optimizer","url":"/2025/07/19/project/AI%20Agentic%20System/5.%20%20Workflow-Evaluator-optimizer.md/","content":"工作流-评估优化器流程说明在评估-优化器中, 一个LLM调用生成响应, 另一个在循环中提供评估和反馈\n\n用户输入task\nLLM生成第一轮的回复\n对LLM的回复生成评估\n检查评估的结果是不是PASS, 如果是, 直接返回最终结果\n\n\n将所有的历史memory都添加到上下文中, 以及将回复评估添加到上下文中\n进行下一次循环\n\n代码\nloop\n\nprivate RefinedResponse loop(String task, String context, List&lt;String&gt; memory,                             List&lt;Generation&gt; chainOfThought) &#123;    Generation generation = generate(task, context);    memory.add(generation.response());    chainOfThought.add(generation);    EvaluationResponse evaluationResponse = evalute(generation.response(), task);    if (evaluationResponse.evaluation().equals(EvaluationResponse.Evaluation.PASS)) &#123;        // Solution is accepted!        return new RefinedResponse(generation.response(), chainOfThought);    &#125;    // Accumulated new context including the last and the previous attempts and    // feedbacks.    StringBuilder newContext = new StringBuilder();    newContext.append(&quot;Previous attempts:&quot;);    for (String m : memory) &#123;        newContext.append(&quot;\\n- &quot;).append(m);    &#125;    newContext.append(&quot;\\nFeedback: &quot;).append(evaluationResponse.feedback());    return loop(task, newContext.toString(), memory, chainOfThought);&#125;\n\n\ngenerate\n\nprivate Generation generate(String task, String context) &#123;    Generation generationResponse = chatClient.prompt()        .user(u -&gt; u.text(&quot;&#123;prompt&#125;\\n&#123;context&#125;\\nTask: &#123;task&#125;&quot;)              .param(&quot;prompt&quot;, this.generatorPrompt)              .param(&quot;context&quot;, context)              .param(&quot;task&quot;, task))        .call()        .entity(Generation.class);    System.out.println(String.format(&quot;\\n=== GENERATOR OUTPUT ===\\nTHOUGHTS: %s\\n\\nRESPONSE:\\n %s\\n&quot;,                                     generationResponse.thoughts(), generationResponse.response()));    return generationResponse;&#125;\n\n\nevalute\n\nprivate EvaluationResponse evalute(String content, String task) &#123;    EvaluationResponse evaluationResponse = chatClient.prompt()        .user(u -&gt; u.text(&quot;&#123;prompt&#125;\\nOriginal task: &#123;task&#125;\\nContent to evaluate: &#123;content&#125;&quot;)              .param(&quot;prompt&quot;, this.evaluatorPrompt)              .param(&quot;task&quot;, task)              .param(&quot;content&quot;, content))        .call()        .entity(EvaluationResponse.class);    System.out.println(String.format(&quot;\\n=== EVALUATOR OUTPUT ===\\nEVALUATION: %s\\n\\nFEEDBACK: %s\\n&quot;,                                     evaluationResponse.evaluation(), evaluationResponse.feedback()));    return evaluationResponse;&#125;\n\n\n\n\nGenerator prompt\n\n&quot;&quot;&quot;Your goal is to complete the task based on the input. If there are feedbackfrom your previous generations, you should reflect on them to improve your solution.CRITICAL: Your response must be a SINGLE LINE of valid JSON with NO LINE BREAKS except those explicitly escaped with \\\\n.Here is the exact format to follow, including all quotes and braces:&#123;&quot;thoughts&quot;:&quot;Brief description here&quot;,&quot;response&quot;:&quot;public class Example &#123;\\\\n    // Code here\\\\n&#125;&quot;&#125;Rules for the response field:1. ALL line breaks must use \\\\n2. ALL quotes must use \\\\&quot;3. ALL backslashes must be doubled: \\\\4. NO actual line breaks or formatting - everything on one line5. NO tabs or special characters6. Java code must be complete and properly escapedExample of properly formatted response:&#123;&quot;thoughts&quot;:&quot;Implementing counter&quot;,&quot;response&quot;:&quot;public class Counter &#123;\\\\n    private int count;\\\\n    public Counter() &#123;\\\\n        count = 0;\\\\n    &#125;\\\\n    public void increment() &#123;\\\\n        count++;\\\\n    &#125;\\\\n&#125;&quot;&#125;Follow this format EXACTLY - your response must be valid JSON on a single line.&quot;&quot;&quot;\n\n\nEvaluator prompt\n\n&quot;&quot;&quot;Evaluate this code implementation for correctness, time complexity, and best practices.Ensure the code have proper javadoc documentation.Respond with EXACTLY this JSON format on a single line:&#123;&quot;evaluation&quot;:&quot;PASS, NEEDS_IMPROVEMENT, or FAIL&quot;, &quot;feedback&quot;:&quot;Your feedback here&quot;&#125;The evaluation field must be one of: &quot;PASS&quot;, &quot;NEEDS_IMPROVEMENT&quot;, &quot;FAIL&quot;Use &quot;PASS&quot; only if all criteria are met with no improvements needed.&quot;&quot;&quot;\n\n适用场景在有清晰的评估标准, 并且迭代改进提供可衡量的价值. 良好契合的两个标志是\n\n当人类清晰地表达反馈时, LLM的答案可以得到显著的改进\nLLM能表达这种反馈\n\n\n原文:\nWhen to use this workflow: This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.\n\n\n复杂的搜索任务需要多轮搜索和分析才能收集全面的信息, 评估人员是否需要进一步搜索\n生成代码的自优化\n\n","categories":["Project","AI Agentic System","Workflow"],"tags":["Project","AI Agentic System","Workflow"]},{"title":"分布式架构-架构演进概述","url":"/2025/07/25/project/Architecture/%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B%E6%A6%82%E8%BF%B0/","content":"\n参考凤凰架构: 演进的架构一大章节\n\n演进中的架构\n计算机总是务实, 某一架构的出现和兴盛往往都是在承载着某个历史使命, 衰败则是因为使命的消失或者有了更好的方案. 所以想要知道某个架构的历史使命, 我们不得不向过去看, 了解到它到底是解决了之前无法解决的什么问题, 也要向现在看它为什么能兴盛, 最后向未来看它为什么衰败, 如此才能理解这个架构的历史和使命 @fung\n\n原始分布式时代令人惊讶的是, 使用多个独立的分布式服务共同构建一个大型系统的设想和尝试, 反而是比今天的大型单体系统出现的时间更早\n在20世纪70年代末期到80年代初, 计算机硬件的运算能力的局促, 直接妨碍到了在单台计算机上信息系统软件能够达到的最大规模. 于是计算机科学家们开始探索一种多台计算机共同协作来支撑一套软件系统, 这就是原始分布式时代\n负责制定 UNIX 系统技术标准的“开放软件基金会”（Open Software Foundation，OSF，也即后来的“国际开放标准组织”）邀请了当时业界主流的计算机厂商一起参与，共同制订了名为“分布式运算环境”（Distributed Computing Environment，DCE）的分布式技术体系. \n\nUNIX 的分布式设计哲学\nSimplicity of both the interface and the implementation are more important than any other attributes of the system — including correctness, consistency, and completeness\n保持接口与实现的简单性，比系统的任何其他属性，包括准确性、一致性和完整性，都来得更加重要。\n—— Richard P. Gabriel，The Rise of ‘Worse is Better’，1991\n\nOSF尝试设计出来一种简单的, 透明的远程方法调用模式, 让程序员无需关注自己使用的方法是本地方法还是远程方法\n调用远程方法的网络环境带来了一系列的新问题\n\n远程服务在哪里(服务发现)\n有多少个(负载均衡)\n网络出现了分区, 超时或者服务出错怎么办(熔断, 隔离, 降级)\n方法的参数和返回结果如何表示 (序列化协议)\n信息如何传输 (传输协议)\n服务权限如何管理 (认证, 授权)\n如何保证通信安全 (网络安全层)\n如何令调用不同机器的服务返回相同的结果 (分布式数据一致性)\n\n尝试的结果自然是失败的, 不然我们现在看到的应用也不会是现在的以单体应用为主的模式了. 远程和本地方法在性能上的鸿沟是无法跨越的(抛弃了内联等优化), 在当时得硬件条件下如果想为用户提供可以接受的速度的远程调用服务, 不得不使用将多个方法打包到同一个方法体中等奇技淫巧, 并且开发者必须无时无刻不意识到本地和远程之间的边界. 设计为性能让步, DCE的努力”付之东流”\n不过在这个时期产出了很多后续分布式的关键技术和概念\n\n源自 NCA 的远程服务调用规范（Remote Procedure Call，RPC），当时被称为DCE&#x2F;RPC，它与后来 Sun 公司向互联网工程任务组（Internet Engineering Task Force，IETF）提交的基于通用 TCP&#x2F;IP 协议的远程服务标准ONC RPC被认为是现代 RPC 的共同鼻祖；\n源自 AFS 的分布式文件系统（Distributed File System，DFS）规范，当时被称为DCE&#x2F;DFS；源自 Kerberos 的服务认证规范；\n还有时间服务、命名与目录服务，就连现在程序中很常用的通用唯一识别符 UUID 也是在 DCE 中发明出来的。\n\n\n原始分布式时代的教训\nJust because something can be distributed doesn’t mean it should be distributed. Trying to make a distributed call act like a local call always ends in tears\n某个功能能够进行分布式，并不意味着它就应该进行分布式，强行追求透明的分布式操作，只会自寻苦果\n—— Kyle Brown，IBM Fellow，Beyond Buzzwords: A Brief History of Microservices Patterns，2016\n\n单体系统时代\n单体架构（Monolithic）\n“单体”只是表明系统中主要的过程调用都是进程内调用，不会发生进程间通信，仅此而已。\n\n单体架构是最早的架构, 也是使用得最自然的架构, 以至于相当长的一段时间里人们都没有去为单体架构专门下一个定义出来\n在剖析单体架构之前, 非常有必要理清的一个概念就是, 单体架构并不是”不如”微服务架构, 在书籍中常出现的作为微服务架构的对比对象, 来说明单体系统的不足, 实际上是大型单体系统, 对于小型系统上单体架构有自己的优势\n小型系统上, 单体架构易开发, 易部署, 易测试. 调用过程都是进程间调用, 运行效率最高\n同时在纵向上, 无论是单体还是微服务抑或是其他架构风格, 都会对代码进行纵向的分层, 这点无关架构.\n\n在横向角度上看, 单体架构同样能将系统从技术, 功能, 职责上进行模块拆分. 横向拓展上来看, 部署多个单体副本也是个常见的需求.\n单体系统的真正缺陷不在于拆分, 而是拆分后的隔离性与自治能力上的欠缺. 在隔离性上, 单体系统所有的代码都共享同一片进程空间, 错误的传播范围是全局性并且式难以隔离的. 自治能力的欠缺会导致在错误传播以后往往会对程序造成极大的破坏. 同时因为隔离性的不支持, 也带来了单体系统难以动态更新的问题. 并且很难优雅地支持异构(Java的native方法还是支持异构的, 所以还是支持的, 只是支持的形式不优雅).\n但是上面列举的问题, 都不是现今微服务取代单体系统成为潮流趋势的核心原因. 最核心的原因是, 单体系统难以支持Phoenix的特性(程序动态迭代更新, 并通过不可靠的部件构建出来一个可靠的整体服务). 单体架构风格潜在的观念是希望系统的每一个部件, 每一处代码都尽量可靠, 靠不出或少出缺陷来构建可靠系统. 单体系统靠高质量来保证的思路, 在小规模软件上还能运作良好, 但是规模越大, 交付一个可靠的单体系统就变得越来越具有挑战性. \n为了允许程序出错, 为了获得隔离, 自治的能力, 可以技术异构等目标, 是继为了性能和算力之后, 让程序再次选择分布式的理由.\nSOA(面向服务架构)时代\nSOA 架构（Service-Oriented Architecture）\n面向服务的架构是一次具体地、系统性地成功解决分布式服务主要问题的架构模式。\n\n对一个大型的单体项目进行拆分, 使每个子系统都能独立部署, 运行, 更新, 开发者尝试了很多种架构\n\n烟囱式架构, 也叫信息孤岛架构, 各个模块之间没有任何的信息交互, 严格来说这个甚至不能叫做架构, 只是各个独立的子系统部署在了一起而已\n微内核架构, 也叫插件式式架构, 在烟囱架构中, 即使是没有任何的业务往来的子系统之间也需要共享一些如人员权限, 组织等公共的主数据, 那就不妨将这些被共享的资源集中在一块, 成为一个被各个组件共同依赖的核心, 这个核心就是微内核, 具体的业务以插件的形式存在. 常用于桌面端的程序, 比如Web程序. 不过微内核架构也有它的缺陷和局限性, 微内核架构的假设是各个组件之间是相互独立的, 不可预知系统安装了哪些模块, 只是向他们提供一些公共的数据, 所有组件之间是不会有直接交互的. 我们必须找到方法既能拆分系统, 又能让子系统之间进行交互\n\n\n\n事件驱动架构, 为了能让子系统之间交互, 我们在子系统之间提供一个事件队列管道, 子系统可以从事件管道中获取到自己的想要的也就是订阅了的事件, 同时子系统也能发布事件. 和发布订阅机制比较像. 同时每个子系统之间是独立, 高度解耦合的\n\n\n在事件驱动架构之后, 软件架构发展就迎来了SOAP协议的诞生, 这个时候SOA (Service Oriented Architecture, SOA), 已经有了登上历史舞台的全部的前置条件. \n软件架构来到了SOA时代, 许多的概念, 思想都已经能在今天的微服务中找到对应的身影了, 譬如服务之间的松散耦合, 注册, 发现, 治理, 隔离, 编排等等, 这些在今天微服务中耳熟能详的名词概念. SOA针对这些问题, 甚至是软件开发这件事情本身, 都提供了更加系统性, 更加具体的探索\n\n更具体: 体现在尽管SOA本身还属于抽象的软件架构风格, 但是实际上提供一套软件设计的基础平台\n更系统: 是SOA的宏大理想, 终极目标是希望能总结出来一套自上而下的软件开发研究的方法论\n\nSOA在21世纪初10年间曾风靡一时, 最终还是偃旗息鼓, 沉寂下去. SOAP会被边缘化的本质原因: 过于严格的规范定义带来过度的复杂性. 而构建在SOAP基础之上的ESB, BPM, SCA, SDO等诸多上层建筑, 进一步加剧了这种复杂性. 过于精密的流程和理论也需要懂得复杂概念的专业人员才能驾驭, 它能实现多个异构代行系统之间的复杂集成交互, 却很难作为一种具有广泛普适性的软件架构风格来推广\n经过了30年的技术进步, 软件受架构复杂性牵制越来越大, 已经离透明二字越来越远了, 这是不是忘记了我们最开始透明和简单的初心? 微服务时代, 似乎正是带着这样的自省开始的\n微服务时代\n微服务架构（Microservices）\n微服务是一种通过多个小型服务组合来构建单个应用的架构风格，这些服务围绕业务能力而非特定的技术标准来构建。各个服务可以采用不同的编程语言，不同的数据存储技术，运行在不同的进程之中。服务采取轻量级的通信机制和自动化的部署机制实现通信与运维。\n\n微服务并不是SOA的变体\n\nMicroservices and SOA\nThis common manifestation of SOA has led some microservice advocates to reject the SOA label entirely, although others consider microservices to be one form of SOA , perhaps service orientation done right. Either way, the fact that SOA means such different things means it’s valuable to have a term that more crisply defines this architectural style\n由于与 SOA 具有一致的表现形式，这让微服务的支持者更加迫切地拒绝再被打上 SOA 的标签，尽管有一些人坚持认为微服务就是 SOA 的一种变体形式，也许从面向服务方面这个方面来说是对的，但无论如何，SOA 与微服务都是两种不同的东西，正因如此，使用一个别的名称来简明地定义这种架构风格就显得更有必要。\n—— Martin Fowler &#x2F; James Lewis，Microservices\n\n微服务有九个核心的业务与技术特征\n\n围绕业务能力构建 （Organized around Business Capability）:  也就是康威定律, 有怎么样结构, 规模, 能力的团队, 就会产出对应的结构, 规模, 能力的产品. 比如本应该归属于一个产品内的功能被划分到了不同的团队, 必然会产生大量的跨团队的沟通协作, 跨越团队边界进行协作在管理, 工作安排, 沟通上都会有更高昂的成本, 高效的团队必然会进行改进, 当团队和产品磨合稳定以后, 团队和产品就会有一致性的结构\n分散治理（Decentralized Governance）: 谁家的孩子谁来管, 服务对应的开发团队有直接对服务运行质量负责的责任, 也应该有着不受外界干预地掌控服务各方面的权力. 实际开发中并不会有高程度的技术异构, 甚至一般来说是有主语言, 微服务更加强调的是确实有必要的技术异构时, 应该有选择不统一的权力\n通过服务来实现独立自治的组件（Componentization via Services）: 通过服务这种进程外组件, 以远程调用的形式提供服务, 虽然有着更高昂的车成本, 但是这是为组件带来隔离和自治能力的必要代价\n产品化思维 (Products not Projects): 避免把软件研发视作去完成某个功能, 而是要视作一种持续改进, 提升的过程. 微服务下, 要求开发团队中的每个人都有产品化思维, 关心整个产品的全部的可行性. DDD也是在这个时期提出的理念.\n数据去中心化 （Decentralized Data Management）:  微服务明确体长数据应该按照领域分散管理, 更新, 维护, 存储. 如果使用中心化的存储, 所有领域必须修改和映射到同一个实体身上, 这使得不同服务之间相互影响, 丧失了独立性. 尽管在分布式中要想处理好一致性问题相当困难, 但是两害相权取其轻, 有一些必要的代价是值得付出的\n强终端弱管道 (Smart Endpoint and Dumb Pipe) : 弱管道可以说是指名道姓地反对SOAP和ESB地复杂地通信机制. 认证, 事务一致性, 授权等一系列的工作, 构建在通信管道上对于一些应用程序来说是必要的, 但是对于更多的应用程序来说都是一个负担. 如果服务需要额外的通信能力, 应该在自己的Endpoint上解决, 而不是在管道上一揽子地解决\n容错性设计 (Design for Failure) : 不再虚幻地追求服务永远稳定, 而是接受服务总是会出错的现实, 要求在微服务的设计中, 有自动的机制对其依赖的服务能够进行快速的故障检测, 并在持续出错的时候进行隔离, 服务恢复的时候进行重建. 如果没有容错性的设计, 系统很容易就会因为一两个服务的崩溃所带来的雪崩效应给淹没. 可靠系统完全可能由会出错的服务组成, 这也是微服务最大的价值\n演进式设计 (Evolutionary Design) : 容灾性设计是允许服务出错, 演进式设计则是承认服务会报废淘汰, 一个设计良好的服务应该是能够报废的,  而不是期望能够得到永生. 系统中出现不可替代, 不可更改的服务, 不能说明这个服务有多重要, 恰恰相反, 是说明了系统设计上的脆弱性\n基础设施自动化 (Infrastructure Automation) : 微服务下运维的对象比起单体架构要有数量级上的增长, 使用微服务的团队更加依赖于基础设施上的自动化\n\n微服务所带来的自由是一把双刃开锋的宝剑，当软件架构者拿起这把宝剑，一刃指向 SOA 定下的复杂技术标准，将选择的权力夺回的同一时刻，另外一刃也正朝向着自己映出冷冷的寒光。微服务时代中，软件研发本身的复杂度应该说是有所降低。一个简单服务，并不见得就会同时面临分布式中所有的问题，也就没有必要背上 SOA 那百宝袋般沉重的技术包袱。需要解决什么问题，就引入什么工具；团队熟悉什么技术，就使用什么框架。此外，像 Spring Cloud 这样的胶水式的全家桶工具集，通过一致的接口、声明和配置，进一步屏蔽了源自于具体工具、框架的复杂性，降低了在不同工具、框架之间切换的成本，所以，作为一个普通的服务开发者，作为一个“螺丝钉”式的程序员，微服务架构是友善的。可是，微服务对架构者是满满的恶意，对架构能力要求已提升到史无前例的程度，笔者在这部文档的多处反复强调过，技术架构者的第一职责就是做决策权衡，有利有弊才需要决策，有取有舍才需要权衡，如果架构者本身的知识面不足以覆盖所需要决策的内容，不清楚其中利弊，恐怕也就无可避免地陷入选择困难症的困境之中。\n后微服务时代\n后微服务时代（Cloud Native）\n从软件层面独力应对微服务架构问题，发展到软、硬一体，合力应对架构问题的时代，此即为“后微服务时代”。\n\n分布式架构中出现的问题 : 注册发现, 跟踪治理, 负载均衡, 传输通信等, 这些问题从原始分布式时代就已经出现了, 只要是分布式架构就无法避免, 但是这些问题一定要软件系统来自己解决吗?\n后微服务时代就是通过容器化和虚拟化技术, 在硬件层面实现了这些服务. \n表 1-1 列出了在同一个分布式服务的问题在传统 Spring Cloud 中提供的应用层面的解决方案与在 Kubernetes 中提供的基础设施层面的解决方案，尽管因为各自出发点不同，解决问题的方法和效果都有所差异，但这无疑是提供了一条全新的、前途更加广阔的解题思路。\n表 1-1 传统 Spring Cloud 与 Kubernetes 提供的解决方案对比\n\n\n\n\nKubernetes\nSpring Cloud\n\n\n\n弹性伸缩\nAutoscaling\nN&#x2F;A\n\n\n服务发现\nKubeDNS &#x2F; CoreDNS\nSpring Cloud Eureka\n\n\n配置中心\nConfigMap &#x2F; Secret\nSpring Cloud Config\n\n\n服务网关\nIngress Controller\nSpring Cloud Zuul\n\n\n负载均衡\nLoad Balancer\nSpring Cloud Ribbon\n\n\n服务安全\nRBAC API\nSpring Cloud Security\n\n\n跟踪监控\nMetrics API &#x2F; Dashboard\nSpring Cloud Turbine\n\n\n降级熔断\nN&#x2F;A\nSpring Cloud Hystrix\n\n\n一旦虚拟化的硬件跟上了软件的灵活性, 那些与业务无关的技术问题有可能从软件层面剥离, 悄无声息解决于硬件基础设施之内, 让软件只需要关注业务&#x2F;\n但是Kurbernets并没有完美解决所有的分布式问题, 仅从功能上看k8s甚至不如之前的Spring Cloud方案. 因为有一些问题处于应用系统与基础设置的边缘, 使得很难在基础设施层上进行处理. 举个例子，微服务 A 调用了微服务 B 的两个服务，称为 B1和 B2，假设 B1表现正常但 B2出现了持续的 500 错，那在达到一定阈值之后就应该对 B2进行熔断，以避免产生雪崩效应。如果仅在基础设施层面来处理，这会遇到一个两难问题，切断 A 到 B 的网络通路则会影响到 B1的正常调用，不切断的话则持续受 B2的错误影响。基础设施是针对整个容器来管理的，粒度相对粗旷，只能到容器层面，对单个远程服务就很难有效管控\n为了解决这一类问题，虚拟化的基础设施很快完成了第二次进化，引入了今天被称为“服务网格”（Service Mesh）的“边车代理模式”（Sidecar Proxy）. 系统自动在服务容器(通常是指 Kubernetes 的 Pod) 中注入一个通信代理服务器, 以类似网络安全里中间人的方式进行流量劫持, 在应用毫无感知的情况下, 悄然接管所有的对外通信. 这个代理处理实现正常的服务间通信以外 (数据平面通信), 还接收来自控制器的指令, 以实现熔断, 认证, 度量, 监控, 均衡负载等各种附加功能. 这样便实现了既不需要在应用层面加入额外的处理代码，也提供了几乎不亚于程序代码的精细管理能力\n\n","categories":["Architecture","Distributed Systems"],"tags":["Architecture","Distributed Systems"]},{"title":"JUC-原子类-AtomicInteger类详解","url":"/2025/07/23/language/Java/JUC/%E5%8E%9F%E5%AD%90%E7%B1%BB-AtomicInteger/","content":"原子类-AtomicInteger类详解说实话, 也没什么好额外讲的, 整个原子类家族, 就是一族封装了一些类CAS操作的类, 提供了我们为某个变量进行原子操作的能力, 让我们可以在多线程环境下, 对某个变量进行原子操作, 而不需要加锁.\n下面简单说明一下类的结构\n核心字段private static final Unsafe U = Unsafe.getUnsafe();private static final long VALUE    = U.objectFieldOffset(AtomicInteger.class, &quot;value&quot;);private volatile int value;\n\n\nUnsafe就是提供了操作系统层面的API来执行CAS操作类\nVALUE是使用了Unsafe获取到的AtomicInteger类中的value字段的属性在类中偏移量, 用于CAS操作\nvalue就是我们这个Integer类的值, 这里的value是保证可见性的, 所以如果使用Atomic是默认保证多线程之间的可见性, 不需要额外加volatile\n\n核心方法因为比较简单, 大多数方法就是让CAS操作更易用, 所以我只列举出来核心方法, 不提供源码了\n\nget() : 获取原子类的值\nset(int newValue) : 设置原子类的值\n\n核心方法一般分成getAndSet和setAndGet两种, 前者返回的是set前的值, 后者返回的是set后的值\ngetAndSet类getAndSet(int newValue) : 获取原子类的值, 并设置为newValue\ngetAndIncrement() : 获取原子类的值, 并将其加1\ngetAndDecrement() : 获取原子类的值, 并将其减1\ngetAndAdd(int delta) : 获取原子类的值, 并将其加上delta\nsetAndGet类setAndGet(int newValue) : 设置原子类的值为newValue, 并返回newValue\nincrementAndGet() : 将原子类的值加1, 并返回加1后的值\ndecrementAndGet() : 将原子类的值减1, 并返回减1后的值\naddAndGet(int delta) : 将原子类的值加上delta, 并返回加上delta后的值\nCompare And Exchange类 (JDK9)正宗的CAS操作\npublic final int compareAndExchangeAcquire(int expectedValue, int newValue) &#123;    return U.compareAndExchangeIntAcquire(this, VALUE, expectedValue, newValue);&#125;\n\n这个CAS是有内存屏障保障不会被重排序的, 任何内存操作不会跨越该操作进行重排序\n\npublic final int compareAndExchangeAcquire(int expectedValue, int newValue) &#123;    return U.compareAndExchangeIntAcquire(this, VALUE, expectedValue, newValue);&#125;\n\n\n这个CAS只保证该操作之后的读写操作不会被重排序到该操作之前, 但该操作之前的读写操作可以被重排序到该操作之后\n\n","categories":["Java","JUC","原子类"],"tags":["Java","JUC","原子类"]},{"title":"JUC-线程池-ThreadPoolExecutor类详解","url":"/2025/07/24/language/Java/JUC/%E7%BA%BF%E7%A8%8B%E6%B1%A0-ThreadPoolExecutor/","content":"线程池-ThreadPoolExecutor类详解ThreadPoolExecutor的构造方法public ThreadPoolExecutor(int corePoolSize,                            int maximumPoolSize,                            long keepAliveTime,                            TimeUnit unit,                            BlockingQueue&lt;Runnable&gt; workQueue) &#123;    this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,            Executors.defaultThreadFactory(), defaultHandler);&#125;\n\ncorePoolSize: 核心线程数, 线程池中即使没有任务也要保持存活的最大线程数量\nmaximumPoolSize: 线程池中允许的最大线程数量\nkeepAliveTime: 线程池中超过核心线程数的空闲线程存活时间\nunit: keepAliveTime的时间单位\nworkQueue: 任务队列, 用于存放待执行的任务\nthreadFactory: 线程工厂, 用于创建新线程\nhandler: 拒绝策略, 当任务无法被执行时的处理策略\n\n拒绝策略JDK提供了四种默认的拒绝策略\n\nAbortPolicy: 将任务丢弃并抛出一个RejectedExecutionException异常, 默认的拒绝策略\n\npublic static class AbortPolicy implements RejectedExecutionHandler &#123;    /**     * Creates an &#123;@code AbortPolicy&#125;.     */    public AbortPolicy() &#123; &#125;    /**     * Always throws RejectedExecutionException.     *     * @param r the runnable task requested to be executed     * @param e the executor attempting to execute this task     * @throws RejectedExecutionException always     */    public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123;        throw new RejectedExecutionException(&quot;Task &quot; + r.toString() +                                                &quot; rejected from &quot; +                                                e.toString());    &#125;&#125;\n\n\nDiscardPolicy: 将任务直接丢弃\n\npublic static class DiscardPolicy implements RejectedExecutionHandler &#123;    /**     * Creates a &#123;@code DiscardPolicy&#125;.     */    public discardpolicy() &#123; &#125;    /**     * does nothing, which has the effect of discarding task r.     *     * @param r the runnable task requested to be executed     * @param e the executor attempting to execute this task     */    public void rejectedexecution(runnable r, threadpoolexecutor e) &#123;    &#125;&#125;\n\n\nCallerRunPolicy: 直接在试图创建并执行任务的calling Thread线程执行这个任务\n\npublic static class CallerRunsPolicy implements RejectedExecutionHandler &#123;    /**     * Creates a &#123;@code CallerRunsPolicy&#125;.     */    public CallerRunsPolicy() &#123; &#125;    /**     * Executes task r in the caller&#x27;s thread, unless the executor     * has been shut down, in which case the task is discarded.     *     * @param r the runnable task requested to be executed     * @param e the executor attempting to execute this task     */    public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123;        if (!e.isShutdown()) &#123;            r.run();        &#125;    &#125;&#125;\n\n\nDiscardOldestPolicy: 将阻塞队列的最后一个任务丢弃, 然后重新执行这个任务\n\npublic static class DiscardOldestPolicy implements RejectedExecutionHandler &#123;    /**     * Creates a &#123;@code DiscardOldestPolicy&#125; for the given executor.     */    public DiscardOldestPolicy() &#123; &#125;    /**     * Obtains and ignores the next task that the executor     * would otherwise execute, if one is immediately available,     * and then retries execution of task r, unless the executor     * is shut down, in which case task r is instead discarded.     *     * @param r the runnable task requested to be executed     * @param e the executor attempting to execute this task     */    public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123;        if (!e.isShutdown()) &#123;            e.getQueue().poll();            e.execute(r);        &#125;    &#125;&#125;\n\n核心字段源码\nprivate final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3;private static final int COUNT_MASK = (1 &lt;&lt; COUNT_BITS) - 1;// runState is stored in the high-order bitsprivate static final int RUNNING    = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN   =  0 &lt;&lt; COUNT_BITS;private static final int STOP       =  1 &lt;&lt; COUNT_BITS;private static final int TIDYING    =  2 &lt;&lt; COUNT_BITS;private static final int TERMINATED =  3 &lt;&lt; COUNT_BITS;\n\n\nctl: Control线程控制信号, 是一个原子类, 前三位是线程池的状态位, 后29位是线程池中的线程数量\n int ctlOf(int rs, int wc) &#123; return rs | wc; &#125;方法构建ctl\nrs是RunState 线程池状态\nwc是WorkerCount 工人数量也就是线程池中的线程的数量\n\n\nint runStateOf(int c)     &#123; return c &amp; ~COUNT_MASK; &#125;: 获取线程池状态\nint workerCountOf(int c)  &#123; return c &amp; COUNT_MASK; &#125;: 获取工人数量\n\n\nCOUNT_BITS: 这个参数 &#x3D;&#x3D; 29, 是获取状态位和设置状态位需要移动的位数\nCONUT_MASK: 前29bit都是1, c &amp; COUNT_MASK来获取到wc\nRUNNING: 线程池的正常工作状态, 线程池可以接受新的任务并且会处理等待队列中的任务\nSHUTDOWN: 调用shutdown()方法的时候, 线程池进入到这个状态\n线程池不再接受新的任务\n继续执行等待队列中的已经存在的任务和正在执行的任务\n\n\nSTOP: 调用shutdownNow()方法\n不接受新的任务\n不处理队列中的任务\n尝试中断正在执行的任务\n\n\nTIDYING: 一种过渡状态, 在满足以下条件的时候进入\n所有的任务都已经终止\n工作线程的数量是0\n队列为空的时候进入到这个状态以后, 会执行 terminate()钩子方法\n\n\nTERMINATED: 线程池关闭, 所有的资源都得到释放\n\n\n任务的执行execute方法源码及解析\npublic void execute(Runnable command) &#123;    if (command == null)        throw new NullPointerException();    // 1. 如果现在的线程数量 &lt; 核心线程数量    int c = ctl.get();    if (workerCountOf(c) &lt; corePoolSize) &#123;        // 增加核心线程数量        if (addWorker(command, true))            return;        c = ctl.get();    &#125;    // 2. 现在的线程数量 &gt; 核心线程数量    // 将任务添加到等待队列中    if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123;        // 双重检查, 因为进入到if块以后可能状态线程池的状态发生了变化        int recheck = ctl.get();        // 如果双重检查的时候发现线程池的状态不再是RUNNING, 移除任务        // 并执行reject回调方法        if (! isRunning(recheck) &amp;&amp; remove(command))            reject(command);        // 执行到这个位置的时候, 逻辑上核心线程已经满了, 但是是可能出现其他的线程因为某些错误死掉的情况, wc实际记录的是还存活着的线程, 这个时候我们就需要创建一个非核心线程来执行        else if (workerCountOf(recheck) == 0)            addWorker(null, false);    &#125;    // 3. 等待队列也已经满了, 尝试创建非核心队列执行任务, 如果创建失败    // 说明线程数量已经超过了最大线程数量, 或者线程池的状态不允许创建新的线程了    else if (!addWorker(command, false))        // 执行reject方法        reject(command);&#125;\n\n线程池类执行任务的顺序是\n\n先尝试将任务交给核心线程, 如果核心线程的数量 &lt; 最大核心线程数量, 创建新的核心线程执行任务\n核心线程的数量超过了最大线程, 尝试将任务添加到等待队列里面\n等待队列也已经满了, 创建一个非核心线程执行任务, 如果创建失败, 说明线程状态不是RUNNING或者线程数量已经超过了最大的数量, 这个时候执行reject方法\n\n\n原文:Proceed in 3 steps:\n\nIf fewer than corePoolSize threads are running, try tostart a new thread with the given command as its firsttask.  The call to addWorker atomically checks runState andworkerCount, and so prevents false alarms that would addthreads when it shouldn’t, by returning false.\n\nIf a task can be successfully queued, then we still needto double-check whether we should have added a thread(because existing ones died since last checking) or thatthe pool shut down since entry into this method. So werecheck state and if necessary roll back the enqueuing ifstopped, or start a new thread if there are none.\n\nIf we cannot queue task, then we try to add a newthread.  If it fails, we know we are shut down or saturatedand so reject the task.\n\n\n\naddWorker方法比较长, 分成两部分解读\n方法签名: private boolean addWorker(Runnable firstTask, boolean core)\n\ncore: 添加的线程是不是核心线程\n\n增加线程的数量, 并没有真的增加线程\n\n\nretry:for (int c = ctl.get();;) &#123;    // 如果线程池的状态至少是SHUTDOWN, 这个时候我们不能再添加新的任务    // 如果状态是STOP, 说明这个时候不能再添加任务了, return false    // 如果状态是SHUTDOWN, 但是传入的任务不是null, 也返回null, 因为SHUTSOWN状态只能从任务队列中消费任务    // 如果传入的任务是空, 但是任务队列也是空的, 这个时候没有要消费的任务了, retuen false    if (runStateAtLeast(c, SHUTDOWN)        &amp;&amp; (runStateAtLeast(c, STOP)            || firstTask != null            || workQueue.isEmpty()))        return false;    // CAS增加worker的数量    for (;;) &#123;        // 超过线程数量的限制, 不能再添加worker了        if (workerCountOf(c)            &gt;= ((core ? corePoolSize : maximumPoolSize) &amp; COUNT_MASK))            return false;         // CAS成功增加worker的数量, 到下一步增加Worker        if (compareAndIncrementWorkerCount(c))            break retry;         c = ctl.get();  // CAS失败, 重新获取worker数量        if (runStateAtLeast(c, SHUTDOWN))            continue retry;        // CAS失败, 说明在这个for中的CAS之前worker的数量发生了变化, CAS尝试添加线程    &#125;&#125;\n\n\n\n添加Worker, 也就是真的添加工作线程\n\nWorker是一个继承了AQS, 实现了Runnable的类\nboolean workerStarted = false;boolean workerAdded = false;Worker w = null;try &#123;    w = new Worker(firstTask);    // w.thread是在构造方法中使用线程工厂创建的    // this.thread = getThreadFactory().newThread(this);    final Thread t = w.thread;    if (t != null) &#123;        final ReentrantLock mainLock = this.mainLock;        // 加锁保护HashSet的线程安全        // largestPoolSize 统计信息的安全        // 在双重检查时候, ws不会发生变化        mainLock.lock();        try &#123;            // 用于双重检查            // 如果在获取锁以前, 线程池shutdown了            int c = ctl.get();            // 状态是RUNNING, 或者状态是SHUTDOWN并且任务是空            // 前者正常情况, 后者是在创建非核心线程            if (isRunning(c) ||                (runStateLessThan(c, STOP) &amp;&amp; firstTask == null)) &#123;                // 创建线程失败                if (t.getState() != Thread.State.NEW)                    throw new IllegalThreadStateException();                // 创建成功, 将worker添加到线程池中                workers.add(w);                workerAdded = true;                int s = workers.size();                if (s &gt; largestPoolSize)                    largestPoolSize = s;            &#125;        &#125; finally &#123;            mainLock.unlock();        &#125;        // 启动worker的任务        if (workerAdded) &#123;            t.start();            workerStarted = true;        &#125;    &#125;&#125; finally &#123;    // 针对创建失败的Worker的处理    if (! workerStarted)        // 将这个worker从hashSet中移除        addWorkerFailed(w);&#125;return workerStarted;\n\nWorker类的构造方法Worker(Runnable firstTask) &#123;    setState(-1); // inhibit interrupts until runWorker    this.firstTask = firstTask;    this.thread = getThreadFactory().newThread(this);&#125;\n\nrunWorker方法Worker类的run方法实现内部是直接调用runWorker(Worker w) 方法\nfinal void runWorker(Worker w) &#123;    Thread wt = Thread.currentThread();    Runnable task = w.firstTask;    w.firstTask = null;    w.unlock();    boolean completedAbruptly = true;    try &#123;        // 工作者线程不断从队列中尝试获取Task然后执行        // 优先执行被分配的first Task        while (task != null || (task = getTask()) != null) &#123;            // 加锁确保任务执行的时候不会被shutdown中断            w.lock();            // 先检查线程池状态 &gt;= STOP, 是的话直接设置线程中断            // 再次检查, 在线程已被中断, 线程池 &gt;= STOP的时候                // 清除线程的中断标志, 然后设置线程被中断了            if ((runStateAtLeast(ctl.get(), STOP) ||                 (Thread.interrupted() &amp;&amp;                  runStateAtLeast(ctl.get(), STOP))) &amp;&amp;                !wt.isInterrupted())                wt.interrupt();            try &#123;                beforeExecute(wt, task);                try &#123;                    // 核心执行内容, 执行task                    task.run();                    afterExecute(task, null);                &#125; catch (Throwable ex) &#123;                    afterExecute(task, ex);                    throw ex;                &#125;            &#125; finally &#123;                // 扫尾工作                task = null;                w.completedTasks++;                w.unlock();            &#125;        &#125;        completedAbruptly = false;    &#125; finally &#123;        // 核心线程在RUNNIN步骤是不会走到这一步的,         // 因为会在getTask的过程中阻塞获取任务        processWorkerExit(w, completedAbruptly);    &#125;&#125;\n\ngetTask方法private Runnable getTask() &#123;    boolean timedOut = false; // Did the last poll() time out?    for (;;) &#123;        int c = ctl.get();        // 在线程池 &gt;= STOP, 等待队列是空的时候        // 工人的数量--, 并直接返回null, 这种情况是getTask失败的情况        if (runStateAtLeast(c, SHUTDOWN)            &amp;&amp; (runStateAtLeast(c, STOP) || workQueue.isEmpty())) &#123;            // workerCount--            decrementWorkerCount();            return null;        &#125;        // 运行到这里的时候, 说明运行状态是RUNNING        // 或者 == SHUTDOWN, 或者 &gt;= STOP 但是等待队列不是空        int wc = workerCountOf(c);        // 需不需要处理线程存活时间超时的情况        boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize;        // 这里是用于处理当前的Worker是不是要销毁        // 如果Worker的数量超过了最大线程池的大小, 减少多余的worker        // 当前线程需要考虑超时, 并且上一次获取任务时发生了超时, 这种情况下worker也应该被回收以节省资源        if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut))            // 保证不会过度销毁Worker, Worker数量至少大于1            // 或者等待队列中没有任务了, 这种情况也能销毁当前Worker            &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123;            if (compareAndDecrementWorkerCount(c))                return null;            continue;        &#125;        try &#123;            Runnable r = timed ?                // 如果是非核心线程就会获取任务就会有超时设置                workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) :                workQueue.take();            if (r != null)                // 成功获取到了任务                return r;            // 超时了, 在下一次循环中, 就会因为这个timeout = true            // 而导致这个非核心线程被销毁            timedOut = true;        &#125; catch (InterruptedException retry) &#123;            timedOut = false;        &#125;    &#125;&#125;\n\n\n到这里我们能简单总结下一个Worker的生命周期\n对于任何Worker, 会在runWorker方法中不断循环获取任务执行任务一旦没有获取到任务, worker就会被移除\n对于核心线程worker, 会在获取任务getTask()上一直阻塞直到获取任务对于非核心线程worker, 在getTask上只会阻塞这个worker存活时间超过这个时间, 就会在getTask的下一次循环中workerCOunt–返回null,然后结束runWorker中while循环, 然后将这个worker销毁\n\n任务的提交submit方法ThreadPoolExecutor类是继承自AbstractExecutorService的. 其中的submit方法也是在这个抽象类中实现的\npublic Future&lt;?&gt; submit(Runnable task) &#123;        if (task == null) throw new NullPointerException();        // 通过submit方法提交的Callable任务会被封装成一个FutureTask对象        // 普通的Runnable接口类, 也会被封装成FutureTask对象        RunnableFuture&lt;Void&gt; ftask = newTaskFor(task, null);        execute(ftask);        return ftask;    &#125;\n\n而execute就是我们第一个讲解的任务的执行的核心部分了\n这里线程池的设计我们能看到是使用了模板模式\n任务的关闭shutdown方法将所有的正在阻塞获取任务的空闲线程的状态变成interrupt, 来释放没有在\n怎么实现的SHUTDOWM状态的语义: \n\n线程池不再接受新的任务\n在addWorker的时候, 如果是addWorker(command, true&#x2F;false)形式都会返回false\n\n\n继续执行等待队列中的已经存在的任务和正在执行的任务\n只会通过interrupt唤醒没有在执行任务在阻塞获取Task的worker, 并且会删除这个空闲的worker\n不会影响正在执行的任务, 也不会影响在等待队列中还有任务的时候\n\n\n\n怎么从SHUTDOWN一步一步变成的TERMINATE\n\n在execute中会直接reject新的任务\nSHUTDOWN状态下并且队列为空的时候, 也就是开始出现空闲的worker的时候, 会在getTask方法返回null\n因为getTask方法返回了null, 触发了runWorker方法中的销毁worker, 并tryTerminate()\n在tryTerminate中再interrupt下一个worker, 这样渐进式将所有的worker都销毁\n\npublic void shutdown() &#123;    final ReentrantLock mainLock = this.mainLock;    mainLock.lock();    try &#123;        checkShutdownAccess();        advanceRunState(SHUTDOWN);        // 将所有没有执行任务的worker打上中断状态        interruptIdleWorkers();        onShutdown(); // hook for ScheduledThreadPoolExecutor    &#125; finally &#123;        mainLock.unlock();    &#125;    tryTerminate();&#125;private void interruptIdleWorkers() &#123;    interruptIdleWorkers(false);&#125;private void interruptIdleWorkers(boolean onlyOne) &#123;    final ReentrantLock mainLock = this.mainLock;    mainLock.lock();    try &#123;        for (Worker w : workers) &#123;            Thread t = w.thread;            // 成功获取到了worker的lock,             // 而worker的lock只会在runWorker方法类里面被获取走            // 说明这个worker是一个空闲的worker            // 通过中断来唤醒阻塞的worker来去检查是不是STOP了            if (!t.isInterrupted() &amp;&amp; w.tryLock()) &#123;                try &#123;                    t.interrupt();                &#125; catch (SecurityException ignore) &#123;                &#125; finally &#123;                    w.unlock();                &#125;            &#125;            if (onlyOne)                break;        &#125;    &#125; finally &#123;        mainLock.unlock();    &#125;&#125;\n\nshutdownNow方法shutdownNow会为所有的线程都打上interrupt状态\n\nSTOP: 调用shutdownNow()方法\n不接受新的任务\n同SHUTDOWN\n\n\n不处理队列中的任务\n会将队列清空并返回\n\n\n尝试中断正在执行的任务\n如果方法尝试执行, 但是还没有执行的时候, 也就是worker刚获取到下一轮的task的时候, 会因为状态是STOP, getTask() &#x3D; null, 进入到销毁worker的过程\n\n\n\n\n\npublic List&lt;Runnable&gt; shutdownNow() &#123;    List&lt;Runnable&gt; tasks;    final ReentrantLock mainLock = this.mainLock;    mainLock.lock();    try &#123;        checkShutdownAccess();        advanceRunState(STOP);        interruptWorkers();        tasks = drainQueue();    &#125; finally &#123;        mainLock.unlock();    &#125;    tryTerminate();    return tasks;&#125;private void interruptWorkers() &#123;    // assert mainLock.isHeldByCurrentThread();    for (Worker w : workers)        w.interruptIfStarted();&#125;void interruptIfStarted() &#123;    Thread t;    if (getState() &gt;= 0 &amp;&amp; (t = thread) != null &amp;&amp; !t.isInterrupted()) &#123;        try &#123;            t.interrupt();        &#125; catch (SecurityException ignore) &#123;        &#125;    &#125;&#125;","categories":["Java","JUC","线程池"],"tags":["Java","JUC","线程池"]},{"title":"分布式架构-远程服务调用 (RPC)","url":"/2025/08/01/project/Architecture/%E8%AE%BF%E9%97%AE%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1/%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E8%B0%83%E7%94%A8/","content":"远程服务调用 (RPC)\n参考: 主要是基于凤凰架构改动, 推荐阅读原文\n​\t凤凰架构: 远程服务调用\n\n进程间通信RPC最初出现的时候, 是希望能提供一种像调用本地方法一样调用远程方法的技术, 虽然现在已经不是这样了, 但至少它的初心是这样的\n我们是怎么调用一个本地方法的?\n// Caller : 方法的调用者, 也就是程序中的main函数// Callee : 被调用的方法, 也就是程序中的println()// Call Site : 调用点, 也就是发生方法调用的指令的位置// Parameter : 参数, 也就是hello world// Retval : 返回值, 由Callee传给Caller的数据public static void main(String[] args) &#123;    System.out.println(&quot;hello world!&quot;);&#125;\n\n完成这样的以一个方法的整体的流程是\n\n传递方法参数: 将方法的参数入栈,将hello world的引用地址入栈\n确定方法的版本: 根据println()的方法签名, 确定其执行的版本\n执行被回调的方法: 从栈中弹出Parameter的值或者引用, 以此为输入, 执行方法中的逻辑\n返回执行结果: 将执行的结果压栈, 并将程序的指令流恢复到Call Site的下一条指令\n\n如果要执行的方法不在当前进程的地址空间, 我们至少要面临两个直接问题\n\n传递方法参数和返回执行结果都直接依赖于本地内存中的栈, 如果是远程方法, 我们该怎么传递参数和返回执行结果\n方法版本的选择依赖于语言规则的定义, 如果Caller和Callee是不同的语言来实现的, 方法版本的选择就会使一项模糊的不可知行为\n\n我们先看到第一问题, 我们怎么解决两个进程之间的通信问题, 这个问题在计算机科学中被称为 “进程间通信 (Inter-Process Communication, IPC)“, 常见的解决方式有\n\n管道(Pipe)或者具名管道(Named Pipe): 可以通过管道在两个进程之间传递少量的字符流或者字节流. 普通管道只能用于有亲缘关系的进程, 而具名管道允许无亲缘关系进程间的通信\n信号(Sinal): 通知目标进程有某件事发生, 典型应用就是kill命令kill -9 pid的语义就是向pid是pid的进程发送编号是9的信号\n信号量(Semaphore): 信号量用于多个进程之间同步协作\n消息队列 (Message Queue): 上面的三种方式都只适用于传递较少的信息, 如果进程间要传递的数据量较多的时候, 使用消息队列. 进程可以向消息队列中添加消息, 赋予读权限的进程可以从消息队列中消费消息\n共享内存 (Shared Memory): 古老而最高效的通信方式, 允许多个进程访问同一块公共的内存空间. 可以通过让进程主动创建, 映射, 分离, 控制某一块内存的程序接口\n套接字接口 (Socket) : 上面的方式都只适用于单机进程间通信, 而Socket是更普适的进程间通信的手段, 可以用于不同机器间的进程通信. 在仅限于本机通信的时候, Socket有效率上的优化, 不再会经过协议栈, 而是简单地将应用层的数据从一个进程拷贝到另一个进程, 这种进程间通信的方式被称为: UNIX Domain Socket, 又叫做 IPC Socket\n\nIPC和RPC之间关系在介绍Socket这种通信方式的时候, 我们似乎发现了能让我们实现程序员透明地调用远程服务的方法, Socket是多系统支持的通用的基于网络的进程间通信的手段, 我们只要将远程方法的调用的通信细节隐藏在操作系统底层, 从应用层面上来看就能做到远程服务调用和本地的进程间通信在编码上完全一致.\n但是这种透明的调用形式反而造成了程序员误以为通信是无成本的假象, 因此被滥用导致分布式系统的性能显著降低\n最终这种模式也因为这种滥用现象被抛弃了\n1994 年至 1997 年间，由 ACM 和 Sun 院士Peter Deutsch、套接字接口发明者Bill Joy、Java 之父James Gosling等一众在 Sun Microsystems 工作的大佬们共同总结了通过网络进行分布式运算的八宗罪（8 Fallacies of Distributed Computing）：\n\nThe network is reliable —— 网络是可靠的。\nLatency is zero —— 延迟是不存在的。\nBandwidth is infinite —— 带宽是无限的。\nThe network is secure —— 网络是安全的。\nTopology doesn’t change —— 拓扑结构是一成不变的。\nThere is one administrator —— 总会有一个管理员。\nTransport cost is zero —— 不必考虑传输成本。\nThe network is homogeneous —— 网络是同质化的。\n\n上面的八条反话解释了远程服务调用如果要弄透明化, 就必须为这些罪过买单. 至此RPC是否能等同于IPC来实现暂时下了一个定论 : RPC应该是一种高层次或者说是语言层面的特征, 而不是像IPC一样是低层次或者说系统层次的特征成为了工业界和学术界的主流观点\n\n额外知识：首次提出远程服务调用的定义\nRemote procedure call is the synchronous language-level transfer of control between programs in disjoint address spaces whose primary communication medium is a narrow channel.\n远程服务调用是指位于互不重合的内存地址空间中的两个程序，在语言层面上，以同步的方式使用带宽有限的信道来传输程序控制信息。\n\n—— Bruce Jay Nelson，Remote Procedure Call，Xerox PARC，1981\n\n三个基本的问题RPC协议要解决的三个基本的问题\n\n如何表示数据: 因为在远程服务调用中, 我们如果想传递返回结果或者是参数, 不可避免的问题是Caller和Callee使用的语言可能不一样, 即使语言是统一的, 也面临着操作系统, 硬件指令层面上的细节上的差异. 我们需要一个中间转义层, 也就是序列化协议. 常见的有: \nONC RPC 的External Data Representation （XDR）\nCORBA 的Common Data Representation（CDR）\nJava RMI 的Java Object Serialization Stream Protocol\ngRPC 的Protocol Buffers\nWeb Service 的XML Serialization\n众多轻量级 RPC 支持的JSON Serialization\n…\n\n\n怎么传递数据: 怎么通过网络在两个服务之间的Endpoint之间互相操作交换数据. 这里的交换数据通常指的是应用层协议. 两个服务交互不是只扔个序列化数据流来表示参数和结果就行的，许多在此之外信息，譬如异常、超时、安全、认证、授权、事务，等等，都可能产生双方需要交换信息的需求。在计算机科学中，专门有一个名称“Wire Protocol”来用于表示这种两个 Endpoint 之间交换这类数据的行为，常见的 Wire Protocol 有：\nJava RMI 的Java Remote Message Protocol（JRMP，也支持RMI-IIOP）\nCORBA 的Internet Inter ORB Protocol（IIOP，是 GIOP 协议在 IP 协议上的实现版本）\nDDS 的Real Time Publish Subscribe Protocol（RTPS）\nWeb Service 的Simple Object Access Protocol（SOAP）\n如果要求足够简单，双方都是 HTTP Endpoint，直接使用 HTTP 协议也是可以的（如 JSON-RPC）\n…\n\n\n如何确定方法: 在远程调用方法的时候因为Caller和Calle语言上的差异, 每门语言的方法签名都可能有所差异, “如何表示同一个方法”和”如何找到对应的方法成了问题”. 最开始想的是直接通过UUID来给方法编号, 调用的时候直接调用编号就行了, 不过最终DCE还是弄出来了一个语言无关的接口描述语言 (Interface Description Language, IDL)\nAndroid 的Android Interface Definition Language（AIDL）\nCORBA 的OMG Interface Definition Language（OMG IDL）\nWeb Service 的Web Service Description Language（WSDL）\nJSON-RPC 的JSON Web Service Protocol（JSON-WSP）\n\n\n\n\n统一与分裂的RPC最初人们都想设计出来一种统一的RPC的解决方式, 在迭代过程中从CORBA到Web Service. 但是最终是失败的. 那些面向透明, 简单的RPC协议 DCE&#x2F;RPC、DCOM、Java RMI, 要么依赖于操作系统, 要么依赖于特定的语言, 总有先天的约束; 面向通用的, 普适的RPC协议; 如CORBA就无法逃过复杂性的困扰, 意图通过技术手段来屏蔽复杂性的RPC协议, 如Web Service, 又不免受到性能问题的束缚. 简单, 普适, 高性能似乎不可能同时满足.\n因为一直没有同时满足上面三个特性的”完美的RPC协议”出现, 所以远程服务调用这个领域里面开始分化. 继出现过 RMI（Sun&#x2F;Oracle）、Thrift（Facebook&#x2F;Apache）、Dubbo（阿里巴巴&#x2F;Apache）、gRPC（Google）、Motan1&#x2F;2（新浪）、Finagle（Twitter）、brpc（百度&#x2F;Apache）、.NET Remoting（微软）、Arvo（Hadoop）、JSON-RPC 2.0（公开规范，JSON-RPC 工作组）……等等难以穷举的协议和框架。这些 RPC 功能、特点不尽相同，有的是某种语言私有，有的能支持跨越多门语言，有的运行在应用层 HTTP 协议之上，有的能直接运行于传输层 TCP&#x2F;UDP 协议之上，但肯定不存在哪一款是“最完美的 RPC”。今时今日，任何一款具有生命力的 RPC 框架，都不再去追求大而全的“完美”，而是有自己的针对性特点作为主要的发展方向，举例分析如下。\n\n朝面向对象发展: 不满足于 RPC 将面向过程的编码方式带到分布式，希望在分布式系统中也能够进行跨进程的面向对象编程，代表为 RMI、.NET Remoting，之前的 CORBA 和 DCOM 也可以归入这类，这条线有一个别名叫做分布式对象（Distributed Object）。\n朝着性能发展: 代表为 gRPC 和 Thrift. 决定RPC性能的主要因素就两个因素: 序列化效率和信息的密度. 对于序列化效率, 序列化的输出结果容量越小, 速度越快, 效率越高; 信息密度取决于协议中有效荷载所占总传输数据的比例大小, 使用的传输协议的层次越高, 信息密度就越低, XML就是前车之鉴. gRPC 是基于 HTTP&#x2F;2 的，支持多路复用和 Header 压缩，Thrift 则直接基于传输层的 TCP 协议来实现，省去了额外应用层协议的开销.\n朝着简化发展: 代表是JSON-RPC, 说要选功能最强、速度最快的 RPC 可能会很有争议，但选功能弱的、速度慢的，JSON-RPC 肯定会候选人中之一。牺牲了功能和效率，换来的是协议的简单轻便，接口与格式都更为通用，尤其适合用于 Web 浏览器这类一般不会有额外协议支持、额外客户端支持的应用场合。\n\n经历了 RPC 框架的战国时代，开发者们终于认可了不同的 RPC 框架所提供的特性或多或少是有矛盾的，很难有某一种框架说“我全部都要”。要把面向对象那套全搬过来，就注定不会太简单，如建 Stub、Skeleton 就很烦了，即使由 IDL 生成也很麻烦；功能多起来，协议就要弄得复杂，效率一般就会受影响；要简单易用，那很多事情就必须遵循约定而不是配置才行；要重视效率，那就需要采用二进制的序列化器和较底层的传输协议，支持的语言范围容易受限。也正是每一种 RPC 框架都有不完美的地方，所以才导致不断有新的 RPC 轮子出现，决定了选择框架时在获得一些利益的同时，要付出另外一些代价。\n最近几年, RPC框架有明显的朝着更高层次 (不仅仅负责调用远程服务, 还管理远程服务) 与插件化方向发展的趋势, 不再独立追求解决RPC的全部三个问题 (表示数据,传递数据, 表示方法, 追求提供核心的, 更高层次的能力, 譬如提供负载均衡, 服务注册, 可观察性等方面的支持. 这一类框架的代表有 Facebook 的 Thrift 与阿里的 Dubbo。尤其是断更多年后重启的 Dubbo 表现得更为明显，它默认有自己的传输协议（Dubbo 协议），同时也支持其他协议；默认采用 Hessian 2 作为序列化器，如果你有 JSON 的需求，可以替换为 Fastjson，如果你对性能有更高的追求，可以替换为Kryo、FST、Protocol Buffers 等效率更好的序列化器，如果你不想依赖其他组件库，直接使用 JDK 自带的序列化器也是可以的。这种设计在一定程度上缓和了 RPC 框架必须取舍，难以完美的缺憾。\n","categories":["Architecture","Distributed Systems"],"tags":["Architecture","Distributed Systems"]}]