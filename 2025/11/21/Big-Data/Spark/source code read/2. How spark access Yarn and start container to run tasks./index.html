<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>[Spark Source Code 2] How spark access YARN and start container to run tasks. | Hexo</title><meta name="author" content="John Doe"><meta name="copyright" content="John Doe"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Spark on YARN的整体运行的架构YARN重要的角色  Resource Manager: 全局资源管理器 ApplicationManager: 应用管理器(RM子组件), 接收Client提交的请求 为每个APP分配一个appId 选择NM启动AM 管理AM的生命周期   Scheduler: 资源调度器(RM子组件) 根据调度策略分配资源, 响应AM的资源请求   NodeManag">
<meta property="og:type" content="article">
<meta property="og:title" content="[Spark Source Code 2] How spark access YARN and start container to run tasks.">
<meta property="og:url" content="https://functional.top/2025/11/21/Big-Data/Spark/source%20code%20read/2.%20How%20spark%20access%20Yarn%20and%20start%20container%20to%20run%20tasks./index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Spark on YARN的整体运行的架构YARN重要的角色  Resource Manager: 全局资源管理器 ApplicationManager: 应用管理器(RM子组件), 接收Client提交的请求 为每个APP分配一个appId 选择NM启动AM 管理AM的生命周期   Scheduler: 资源调度器(RM子组件) 根据调度策略分配资源, 响应AM的资源请求   NodeManag">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://functional.top/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-11-21T00:00:00.000Z">
<meta property="article:modified_time" content="2026-02-08T11:04:02.565Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="BigData">
<meta property="article:tag" content="Spark">
<meta property="article:tag" content="source_code">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://functional.top/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "[Spark Source Code 2] How spark access YARN and start container to run tasks.",
  "url": "https://functional.top/2025/11/21/Big-Data/Spark/source%20code%20read/2.%20How%20spark%20access%20Yarn%20and%20start%20container%20to%20run%20tasks./",
  "image": "https://functional.top/img/butterfly-icon.png",
  "datePublished": "2025-11-21T00:00:00.000Z",
  "dateModified": "2026-02-08T11:04:02.565Z",
  "author": [
    {
      "@type": "Person",
      "name": "John Doe",
      "url": "https://functional.top"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://functional.top/2025/11/21/Big-Data/Spark/source%20code%20read/2.%20How%20spark%20access%20Yarn%20and%20start%20container%20to%20run%20tasks./index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '[Spark Source Code 2] How spark access YARN and start container to run tasks.',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Hexo</span></a><a class="nav-page-title" href="/"><span class="site-name">[Spark Source Code 2] How spark access YARN and start container to run tasks.</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">[Spark Source Code 2] How spark access YARN and start container to run tasks.</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-11-21T00:00:00.000Z" title="Created 2025-11-21 00:00:00">2025-11-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-08T11:04:02.565Z" title="Updated 2026-02-08 11:04:02">2026-02-08</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Spark-on-YARN的整体运行的架构"><a href="#Spark-on-YARN的整体运行的架构" class="headerlink" title="Spark on YARN的整体运行的架构"></a>Spark on YARN的整体运行的架构</h1><h2 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h2><p>重要的角色</p>
<ul>
<li>Resource Manager: 全局资源管理器<ul>
<li>ApplicationManager: 应用管理器(RM子组件),<ul>
<li>接收Client提交的请求</li>
<li>为每个APP分配一个appId</li>
<li>选择NM启动AM</li>
<li>管理AM的生命周期</li>
</ul>
</li>
<li>Scheduler: 资源调度器(RM子组件) 根据调度策略分配资源, 响应AM的资源请求</li>
</ul>
</li>
<li>NodeManager: 节点管理器, 每台机器分配一个NM, 是这台机器在集群中的代理<ul>
<li>管理单个节点的资源</li>
<li>向RM报告节点状态和资源使用情况</li>
<li>接收从AM收到请求</li>
</ul>
</li>
<li>Container<ul>
<li>YARN中资源分配的基本单位</li>
<li>对应着一组CPU 内存等资源</li>
<li>运行ApplicationMaster (AM) 或Executor</li>
</ul>
</li>
</ul>
<h2 id="Spark-on-YARN的运行流程"><a href="#Spark-on-YARN的运行流程" class="headerlink" title="Spark on YARN的运行流程"></a>Spark on YARN的运行流程</h2><p>ApplicationMaster(区别于上面的ApplicationManager): 每个app都会分配一个AM, 该AM是这个app在YARN集群的代理, 运行在YARN集群中的app通过AM向RM(scheduler)请求资源</p>
<ol>
<li><strong>提交AM</strong><ol>
<li>Client准备好AM的运行环境, AM提交到ApplicationManager上</li>
<li>RM的ApplicationManager<ol>
<li>分配AppId</li>
<li>验证资源</li>
<li>选择NM启动AM</li>
</ol>
</li>
<li>NM启动AM<ol>
<li>下载Jar, 配置文件</li>
<li>启动AM的main入口 (通过<code>java org.apache.spark.deploy.yarn.ApplicationMaster</code>)</li>
</ol>
</li>
</ol>
</li>
<li><strong>AM请求资源</strong><ol>
<li>AM向RM注册自己<ol>
<li>AM调用<code>amClietn.registerApplicaionMaster()</code></li>
<li>告知RM自己的host和port</li>
</ol>
</li>
<li>AM请求Executor容器<ol>
<li>AM<strong>周期性调用</strong><code>amClient.allocate()</code>向Scheduler请求Container</li>
<li>请求内容: 容器数量, 容器的资源(CPU, 内存), 位置偏好</li>
</ol>
</li>
<li>Scheduler分配容器(运行资源)<ol>
<li>根据当前调度策略分配资源</li>
<li>返回已分配的容器列表给AM</li>
</ol>
</li>
</ol>
</li>
<li><strong>启动Executor</strong><ol>
<li>AM启动Executor<ol>
<li>AM通过nmClient.startContainer()向NM发送启动命令 (通过<code>java org.apache.spark.executor.YarnCoarseGrainedExecutorBackend</code>)</li>
</ol>
</li>
<li>Executor连接Driver执行任务<ol>
<li>Executor通过RPC连接到Driver</li>
<li>Driver的TaskScheduler分配Task到Executor</li>
<li>Executor返回Task的执行结果到Driver</li>
</ol>
</li>
</ol>
</li>
</ol>
<p>整体的流程循环是Client将AM放到YARN中执行 AM循环申请Executor(直到全部申请成功), Executor通过RPC和Driver机建立联系</p>
<h1 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h1><blockquote>
<p>源码确实是最明确的, 但是源码并不是最清晰的, 在对整体逻辑和功能性有基础的了解后, 通过源码明确细节是更明智的做法, 代码永远不能回答”为什么”, 只能回答”是什么”, “怎么做”</p>
</blockquote>
<h2 id="提交AM"><a href="#提交AM" class="headerlink" title="提交AM"></a>提交AM</h2><p><strong>这部分的核心的职责是将AM提交到YARN上运行</strong></p>
<p>Entry Point: <code>org.apache.spark.deploy.Client.scala</code>这个文件就是SparkSubmit要执行的app, 入口点是这个类的<code>start()</code>函数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 因为是在YARN集群上运行的, 所以清除掉本地的配置, 在YARN集群模式下, 使用YARN的缓存</span></span><br><span class="line">conf.remove(<span class="type">JARS</span>)  </span><br><span class="line">conf.remove(<span class="type">FILES</span>)  </span><br><span class="line">conf.remove(<span class="type">ARCHIVES</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment">// 关注构造和run()方法</span></span><br><span class="line"><span class="keyword">new</span> <span class="type">Client</span>(<span class="keyword">new</span> <span class="type">ClientArguments</span>(args), conf, <span class="literal">null</span>).run()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这里为什么要移除? 避免重复分发<br>在SparkSubmit中, 因为ClusterMode是YARN, 所以会将spark.jars&#x2F;spark.files&#x2F;spark&#x2F;archives都复制到spark.yarn.dist&#x2F;*这个对应的YARN配置上了. 因为确定是YARN Mode所以需要移除这些通用配置. Client就只使用YARN专用配置分发文件</p>
</blockquote>
<h3 id="Client构造函数"><a href="#Client构造函数" class="headerlink" title="Client构造函数"></a>Client构造函数</h3><p>需要关注的是</p>
<ul>
<li>ClusterMode这个时候因为AM和Driver是一体的, 都运行在集群上, 这个时候AM机要按照Driver机来配置</li>
<li>ClientMode AM和Driver机是分离的, AM只负责向ApplicationManager申请资源, Driver运行在Client机上, 所以这个时候集群中的AM机轻量配置即可</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> yarnClient = <span class="type">YarnClient</span>.createYarnClient  </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> hadoopConf = <span class="keyword">new</span> <span class="type">YarnConfiguration</span>(<span class="type">SparkHadoopUtil</span>.newConfiguration(sparkConf))</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> amMemoryOverheadFactor = <span class="keyword">if</span> (isClusterMode) &#123;  </span><br><span class="line">  sparkConf.get(<span class="type">DRIVER_MEMORY_OVERHEAD_FACTOR</span>)  </span><br><span class="line">&#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">  <span class="type">AM_MEMORY_OVERHEAD_FACTOR</span>  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="comment">// AM related configurations  </span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> amMemory = <span class="keyword">if</span> (isClusterMode) &#123;  </span><br><span class="line">  sparkConf.get(<span class="type">DRIVER_MEMORY</span>).toInt  </span><br><span class="line">&#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">  sparkConf.get(<span class="type">AM_MEMORY</span>).toInt  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> driverMinimumMemoryOverhead =  </span><br><span class="line">  <span class="keyword">if</span> (isClusterMode) &#123;  </span><br><span class="line">    sparkConf.get(<span class="type">DRIVER_MIN_MEMORY_OVERHEAD</span>)  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    <span class="number">384</span>L  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> amMemoryOverhead = &#123;  </span><br><span class="line">  <span class="keyword">val</span> amMemoryOverheadEntry = <span class="keyword">if</span> (isClusterMode) <span class="type">DRIVER_MEMORY_OVERHEAD</span> <span class="keyword">else</span> <span class="type">AM_MEMORY_OVERHEAD</span>  </span><br><span class="line">  sparkConf.get(amMemoryOverheadEntry).getOrElse(  </span><br><span class="line">    math.max((amMemoryOverheadFactor * amMemory).toLong,  </span><br><span class="line">      driverMinimumMemoryOverhead)).toInt  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> amCores = <span class="keyword">if</span> (isClusterMode) &#123;  </span><br><span class="line">  sparkConf.get(<span class="type">DRIVER_CORES</span>)  </span><br><span class="line">&#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">  sparkConf.get(<span class="type">AM_CORES</span>)  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="comment">// Executor related configurations  </span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> executorMemory = sparkConf.get(<span class="type">EXECUTOR_MEMORY</span>)  </span><br><span class="line"><span class="comment">// Executor offHeap memory in MiB.  </span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">val</span> executorOffHeapMemory = <span class="type">Utils</span>.executorOffHeapMemorySizeAsMb(sparkConf)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> executorMemoryOvereadFactor = sparkConf.get(<span class="type">EXECUTOR_MEMORY_OVERHEAD_FACTOR</span>)  </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> minMemoryOverhead = sparkConf.get(<span class="type">EXECUTOR_MIN_MEMORY_OVERHEAD</span>)  </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> executorMemoryOverhead = sparkConf.get(<span class="type">EXECUTOR_MEMORY_OVERHEAD</span>).getOrElse(  </span><br><span class="line">  math.max((executorMemoryOvereadFactor * executorMemory).toLong,  </span><br><span class="line">    minMemoryOverhead)).toInt </span><br></pre></td></tr></table></figure>

<h3 id="Client-run"><a href="#Client-run" class="headerlink" title="Client.run()"></a>Client.run()</h3><p>提交一个application到RM上(也就是AM)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">	submitApplication()  </span><br><span class="line">	<span class="keyword">if</span> (!launcherBackend.isConnected() &amp;&amp; fireAndForget) &#123;  </span><br><span class="line">	  <span class="keyword">val</span> report = getApplicationReport()  </span><br><span class="line">	  <span class="keyword">val</span> state = report.getYarnApplicationState  </span><br><span class="line">	  <span class="comment">// log</span></span><br><span class="line">	  <span class="comment">// 如果fail/killed, throw SparkException</span></span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		<span class="comment">// get YARNAppReport </span></span><br><span class="line">		<span class="comment">// 如果fail/killed/error, throw SparkException</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="将app提交到YARN上-重点"><a href="#将app提交到YARN上-重点" class="headerlink" title="将app提交到YARN上 (重点)"></a>将app提交到YARN上 (重点)</h3><ul>
<li>从RM的ApplicationManager中申请一个Application出来</li>
<li>获取YARN中AM的Stage Dir存储路径, (UserGroupName or hadoop homeDir) + appId</li>
<li>创建AM container context</li>
<li>基于container context和appId等信息, 创建一个ApplicationSubmissionContext</li>
<li>提交这个ApplicationSubmissionContext到RM, 并且通过launcherBackend监控这个任务的状态</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submitApplication</span></span>(): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  <span class="type">ResourceRequestHelper</span>.validateResources(sparkConf)  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">try</span> &#123;  </span><br><span class="line">    launcherBackend.connect()  </span><br><span class="line">    yarnClient.init(hadoopConf)  </span><br><span class="line">    yarnClient.start()  </span><br><span class="line"></span><br><span class="line">    <span class="comment">// Get a new application from our RM  </span></span><br><span class="line">    <span class="keyword">val</span> newApp = yarnClient.createApplication()  </span><br><span class="line">    <span class="keyword">val</span> newAppResponse = newApp.getNewApplicationResponse()  </span><br><span class="line">    <span class="keyword">this</span>.appId = newAppResponse.getApplicationId()  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// The app staging dir based on the STAGING_DIR configuration if configured  </span></span><br><span class="line">    <span class="comment">// otherwise based on the users home directory. </span></span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Set up the appropriate contexts to launch our AM  </span></span><br><span class="line">    <span class="keyword">val</span> containerContext = createContainerLaunchContext()  </span><br><span class="line">    <span class="keyword">val</span> appContext = createApplicationSubmissionContext(newApp, containerContext)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Finally, submit and monitor the application  </span></span><br><span class="line">    yarnClient.submitApplication(appContext)  </span><br><span class="line">    launcherBackend.setAppId(appId.toString)  </span><br><span class="line">    reportLauncherState(<span class="type">SparkAppHandle</span>.<span class="type">State</span>.<span class="type">SUBMITTED</span>)  </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>准备ContainerContext, 这一步设置了env, java opt, cmd</strong></p>
<ul>
<li>其中的重点是amClass的设置<ul>
<li>ClusterMode -&gt; <code>org.apache.spark.deploy.yarn.ApplicationMaster</code></li>
<li>ClientMode -&gt; <code>org.apache.spark.deploy.yarn.ExecutorLauncher</code></li>
<li><strong>这两个类的main方法就是接下来这两个类在yarn中要执行的entry point</strong></li>
</ul>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createContainerLaunchContext</span></span>(): <span class="type">ContainerLaunchContext</span> = &#123;  </span><br><span class="line">  <span class="keyword">val</span> pySparkArchives =  </span><br><span class="line">    <span class="keyword">if</span> (sparkConf.get(<span class="type">IS_PYTHON_APP</span>)) &#123;  </span><br><span class="line">      findPySparkArchives()  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">      <span class="type">Nil</span>  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 设置env</span></span><br><span class="line">  <span class="keyword">val</span> launchEnv = setupLaunchEnv(stagingDirPath, pySparkArchives)  </span><br><span class="line">  <span class="keyword">val</span> localResources = prepareLocalResources(stagingDirPath, pySparkArchives)  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> amContainer = <span class="type">Records</span>.newRecord(classOf[<span class="type">ContainerLaunchContext</span>])  </span><br><span class="line">  amContainer.setLocalResources(localResources.asJava)  </span><br><span class="line">  amContainer.setEnvironment(launchEnv.asJava)  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> javaOpts = <span class="type">ListBuffer</span>[<span class="type">String</span>]()  </span><br><span class="line">  </span><br><span class="line">  javaOpts += <span class="string">s&quot;-Djava.net.preferIPv6Addresses=<span class="subst">$&#123;Utils.preferIPv6&#125;</span>&quot;</span>  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Add Xmx for AM memory  </span></span><br><span class="line">  javaOpts += <span class="string">&quot;-Xmx&quot;</span> + amMemory + <span class="string">&quot;m&quot;</span>  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> tmpDir = <span class="keyword">new</span> <span class="type">Path</span>(<span class="type">Environment</span>.<span class="type">PWD</span>.$$(), <span class="type">YarnConfiguration</span>.<span class="type">DEFAULT_CONTAINER_TEMP_DIR</span>)  </span><br><span class="line">  javaOpts += <span class="string">&quot;-Djava.io.tmpdir=&quot;</span> + tmpDir  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Include driver-specific java options if we are launching a driver  </span></span><br><span class="line">  <span class="keyword">if</span> (isClusterMode) &#123;  </span><br><span class="line">    sparkConf.get(<span class="type">DRIVER_JAVA_OPTIONS</span>).foreach &#123; opts =&gt;  </span><br><span class="line">      javaOpts ++= <span class="type">Utils</span>.splitCommandString(opts)  </span><br><span class="line">        .map(<span class="type">Utils</span>.substituteAppId(_, <span class="keyword">this</span>.appId.toString))  </span><br><span class="line">        .map(<span class="type">YarnSparkHadoopUtil</span>.escapeForShell)  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">val</span> libraryPaths = <span class="type">Seq</span>(sparkConf.get(<span class="type">DRIVER_LIBRARY_PATH</span>),  </span><br><span class="line">      sys.props.get(<span class="string">&quot;spark.driver.libraryPath&quot;</span>)).flatten  </span><br><span class="line">    <span class="keyword">if</span> (libraryPaths.nonEmpty) &#123;  </span><br><span class="line">      prefixEnv = <span class="type">Some</span>(createLibraryPathPrefix(libraryPaths.mkString(<span class="type">File</span>.pathSeparator),  </span><br><span class="line">        sparkConf))  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    <span class="comment">// Validate and include yarn am specific java options in yarn-client mode.  </span></span><br><span class="line">    sparkConf.get(<span class="type">AM_JAVA_OPTIONS</span>).foreach &#123; opts =&gt;  </span><br><span class="line">      <span class="keyword">if</span> (opts.contains(<span class="string">&quot;-Dspark&quot;</span>)) &#123;  </span><br><span class="line">        <span class="keyword">val</span> msg = <span class="string">s&quot;<span class="subst">$&#123;AM_JAVA_OPTIONS.key&#125;</span> is not allowed to set Spark options (was &#x27;<span class="subst">$opts</span>&#x27;).&quot;</span>  </span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(msg)  </span><br><span class="line">      &#125;  </span><br><span class="line">      <span class="keyword">if</span> (opts.contains(<span class="string">&quot;-Xmx&quot;</span>)) &#123;  </span><br><span class="line">        <span class="keyword">val</span> msg = <span class="string">s&quot;<span class="subst">$&#123;AM_JAVA_OPTIONS.key&#125;</span> is not allowed to specify max heap memory settings &quot;</span> +  </span><br><span class="line">          <span class="string">s&quot;(was &#x27;<span class="subst">$opts</span>&#x27;). Use spark.yarn.am.memory instead.&quot;</span>  </span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(msg)  </span><br><span class="line">      &#125;  </span><br><span class="line">      javaOpts ++= <span class="type">Utils</span>.splitCommandString(opts)  </span><br><span class="line">        .map(<span class="type">Utils</span>.substituteAppId(_, <span class="keyword">this</span>.appId.toString))  </span><br><span class="line">        .map(<span class="type">YarnSparkHadoopUtil</span>.escapeForShell)  </span><br><span class="line">    &#125;  </span><br><span class="line">    sparkConf.get(<span class="type">AM_LIBRARY_PATH</span>).foreach &#123; paths =&gt;  </span><br><span class="line">      prefixEnv = <span class="type">Some</span>(createLibraryPathPrefix(paths, sparkConf))  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> userClass =  </span><br><span class="line">    <span class="keyword">if</span> (isClusterMode) &#123;  </span><br><span class="line">      <span class="type">Seq</span>(<span class="string">&quot;--class&quot;</span>, <span class="type">YarnSparkHadoopUtil</span>.escapeForShell(args.userClass))  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">      <span class="type">Nil</span>  </span><br><span class="line">    &#125;  </span><br><span class="line">  <span class="keyword">val</span> userJar =  </span><br><span class="line">    <span class="keyword">if</span> (args.userJar != <span class="literal">null</span>) &#123;  </span><br><span class="line">      <span class="type">Seq</span>(<span class="string">&quot;--jar&quot;</span>, args.userJar)  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">      <span class="type">Nil</span>  </span><br><span class="line">    &#125;  </span><br><span class="line">  <span class="keyword">val</span> primaryPyFile =  </span><br><span class="line">    <span class="keyword">if</span> (isClusterMode &amp;&amp; args.primaryPyFile != <span class="literal">null</span>) &#123;  </span><br><span class="line">      <span class="type">Seq</span>(<span class="string">&quot;--primary-py-file&quot;</span>, <span class="keyword">new</span> <span class="type">Path</span>(args.primaryPyFile).getName())  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">      <span class="type">Nil</span>  </span><br><span class="line">    &#125;  </span><br><span class="line">  <span class="keyword">val</span> primaryRFile =  </span><br><span class="line">    <span class="keyword">if</span> (args.primaryRFile != <span class="literal">null</span>) &#123;  </span><br><span class="line">      <span class="type">Seq</span>(<span class="string">&quot;--primary-r-file&quot;</span>, args.primaryRFile)  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">      <span class="type">Nil</span>  </span><br><span class="line">    &#125;  </span><br><span class="line">  <span class="keyword">val</span> amClass =  </span><br><span class="line">    <span class="keyword">if</span> (isClusterMode) &#123;  </span><br><span class="line">      <span class="type">Utils</span>.classForName(<span class="string">&quot;org.apache.spark.deploy.yarn.ApplicationMaster&quot;</span>).getName  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">      <span class="type">Utils</span>.classForName(<span class="string">&quot;org.apache.spark.deploy.yarn.ExecutorLauncher&quot;</span>).getName  </span><br><span class="line">    &#125;  </span><br><span class="line">  <span class="keyword">if</span> (args.primaryRFile != <span class="literal">null</span> &amp;&amp;  </span><br><span class="line">      (args.primaryRFile.endsWith(<span class="string">&quot;.R&quot;</span>) || args.primaryRFile.endsWith(<span class="string">&quot;.r&quot;</span>))) &#123;  </span><br><span class="line">    args.userArgs = <span class="type">ArrayBuffer</span>(args.primaryRFile) ++ args.userArgs  </span><br><span class="line">  &#125;  </span><br><span class="line">  <span class="keyword">val</span> userArgs = args.userArgs.flatMap &#123; arg =&gt;  </span><br><span class="line">    <span class="type">Seq</span>(<span class="string">&quot;--arg&quot;</span>, <span class="type">YarnSparkHadoopUtil</span>.escapeForShell(arg))  </span><br><span class="line">  &#125;  </span><br><span class="line">  <span class="keyword">val</span> amArgs =  </span><br><span class="line">    <span class="type">Seq</span>(amClass) ++ userClass ++ userJar ++ primaryPyFile ++ primaryRFile ++ userArgs ++  </span><br><span class="line">    <span class="type">Seq</span>(<span class="string">&quot;--properties-file&quot;</span>,  </span><br><span class="line">      buildPath(<span class="type">Environment</span>.<span class="type">PWD</span>.$$(), <span class="type">LOCALIZED_CONF_DIR</span>, <span class="type">SPARK_CONF_FILE</span>)) ++  </span><br><span class="line">    <span class="type">Seq</span>(<span class="string">&quot;--dist-cache-conf&quot;</span>,  </span><br><span class="line">      buildPath(<span class="type">Environment</span>.<span class="type">PWD</span>.$$(), <span class="type">LOCALIZED_CONF_DIR</span>, <span class="type">DIST_CACHE_CONF_FILE</span>))  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Command for the ApplicationMaster  </span></span><br><span class="line">  <span class="keyword">val</span> commands = prefixEnv ++  </span><br><span class="line">    <span class="type">Seq</span>(<span class="type">Environment</span>.<span class="type">JAVA_HOME</span>.$$() + <span class="string">&quot;/bin/java&quot;</span>, <span class="string">&quot;-server&quot;</span>) ++  </span><br><span class="line">    javaOpts ++ amArgs ++  </span><br><span class="line">    <span class="type">Seq</span>(  </span><br><span class="line">      <span class="string">&quot;1&gt;&quot;</span>, <span class="type">ApplicationConstants</span>.<span class="type">LOG_DIR_EXPANSION_VAR</span> + <span class="string">&quot;/stdout&quot;</span>,  </span><br><span class="line">      <span class="string">&quot;2&gt;&quot;</span>, <span class="type">ApplicationConstants</span>.<span class="type">LOG_DIR_EXPANSION_VAR</span> + <span class="string">&quot;/stderr&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> printableCommands = commands.map(s =&gt; <span class="keyword">if</span> (s == <span class="literal">null</span>) <span class="string">&quot;null&quot;</span> <span class="keyword">else</span> s).toList  </span><br><span class="line">  amContainer.setCommands(printableCommands.asJava)  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在设置完containerContext以后, 创建最终的<code>ApplicationSubmissionContext</code><br>核心内容就是将amContainerContext的内容都设置到newapp.getApplicationSubmissionContext上</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createApplicationSubmissionContext</span></span>(  </span><br><span class="line">    newApp: <span class="type">YarnClientApplication</span>,  </span><br><span class="line">    containerContext: <span class="type">ContainerLaunchContext</span>): <span class="type">ApplicationSubmissionContext</span> = &#123;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> componentName = <span class="keyword">if</span> (isClusterMode) &#123;  </span><br><span class="line">    config.<span class="type">YARN_DRIVER_RESOURCE_TYPES_PREFIX</span>  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    config.<span class="type">YARN_AM_RESOURCE_TYPES_PREFIX</span>  </span><br><span class="line">  &#125;  </span><br><span class="line">  <span class="keyword">val</span> yarnAMResources = getYarnResourcesAndAmounts(sparkConf, componentName)  </span><br><span class="line">  <span class="keyword">val</span> amResources = yarnAMResources ++  </span><br><span class="line">    getYarnResourcesFromSparkResources(<span class="type">SPARK_DRIVER_PREFIX</span>, sparkConf)  </span><br><span class="line">  logDebug(<span class="string">s&quot;AM resources: <span class="subst">$amResources</span>&quot;</span>)  </span><br><span class="line">  <span class="keyword">val</span> appContext = newApp.getApplicationSubmissionContext  </span><br><span class="line">  appContext.setApplicationName(sparkConf.get(<span class="string">&quot;spark.app.name&quot;</span>, <span class="string">&quot;Spark&quot;</span>))  </span><br><span class="line">  appContext.setQueue(sparkConf.get(<span class="type">QUEUE_NAME</span>))  </span><br><span class="line">  appContext.setAMContainerSpec(containerContext)  </span><br><span class="line">  appContext.setApplicationType(sparkConf.get(<span class="type">APPLICATION_TYPE</span>))  </span><br><span class="line">  </span><br><span class="line">  sparkConf.get(<span class="type">APPLICATION_TAGS</span>).foreach &#123; tags =&gt;  </span><br><span class="line">    appContext.setApplicationTags(<span class="keyword">new</span> java.util.<span class="type">HashSet</span>[<span class="type">String</span>](tags.asJava))  </span><br><span class="line">  &#125;  </span><br><span class="line">  sparkConf.get(<span class="type">MAX_APP_ATTEMPTS</span>) <span class="keyword">match</span> &#123;  </span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(v) =&gt; appContext.setMaxAppAttempts(v)  </span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt; logDebug(<span class="string">s&quot;<span class="subst">$&#123;MAX_APP_ATTEMPTS.key&#125;</span> is not set. &quot;</span> +  </span><br><span class="line">        <span class="string">&quot;Cluster&#x27;s default value will be used.&quot;</span>)  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  sparkConf.get(<span class="type">AM_ATTEMPT_FAILURE_VALIDITY_INTERVAL_MS</span>).foreach &#123; interval =&gt;  </span><br><span class="line">    appContext.setAttemptFailuresValidityInterval(interval)  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> capability = <span class="type">Records</span>.newRecord(classOf[<span class="type">Resource</span>])  </span><br><span class="line">  capability.setMemorySize(amMemory + amMemoryOverhead)  </span><br><span class="line">  capability.setVirtualCores(amCores)  </span><br><span class="line">  <span class="keyword">if</span> (amResources.nonEmpty) &#123;  </span><br><span class="line">    <span class="type">ResourceRequestHelper</span>.setResourceRequests(amResources, capability)  </span><br><span class="line">  &#125;  </span><br><span class="line">  logDebug(<span class="string">s&quot;Created resource capability for AM request: <span class="subst">$capability</span>&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">  sparkConf.get(<span class="type">AM_NODE_LABEL_EXPRESSION</span>) <span class="keyword">match</span> &#123;  </span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(expr) =&gt;  </span><br><span class="line">      <span class="keyword">val</span> amRequest = <span class="type">Records</span>.newRecord(classOf[<span class="type">ResourceRequest</span>])  </span><br><span class="line">      amRequest.setResourceName(<span class="type">ResourceRequest</span>.<span class="type">ANY</span>)  </span><br><span class="line">      amRequest.setPriority(<span class="type">Priority</span>.newInstance(<span class="number">0</span>))  </span><br><span class="line">      amRequest.setCapability(capability)  </span><br><span class="line">      amRequest.setNumContainers(<span class="number">1</span>)  </span><br><span class="line">      amRequest.setNodeLabelExpression(expr)  </span><br><span class="line">      appContext.setAMContainerResourceRequests(<span class="type">Collections</span>.singletonList(amRequest))  </span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;  </span><br><span class="line">      appContext.setResource(capability)  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  appContext.setUnmanagedAM(isClientUnmanagedAMEnabled)  </span><br><span class="line">  </span><br><span class="line">  sparkConf.get(<span class="type">APPLICATION_PRIORITY</span>).foreach &#123; appPriority =&gt;  </span><br><span class="line">    appContext.setPriority(<span class="type">Priority</span>.newInstance(appPriority))  </span><br><span class="line">  &#125;  </span><br><span class="line">  appContext  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>至此我们就完成了对要提交任务的所有的设置<br>接下来就会在<code>submitApplication</code>方法中的<code>yarnClient.submitApplication(appContext)</code>将任务提交到ApplicationManager上</p>
<h2 id="AM请求资源"><a href="#AM请求资源" class="headerlink" title="AM请求资源"></a>AM请求资源</h2><p><em>这里我们暂时只看到ApplicationMaster路线, 也就是在ClusterMode的情况下</em></p>
<p>Entry Point: <code>org.apache.spark.deploy.yarn.ApplicationMaster.scala</code><br>在main方法中主要是做些环境变化后的环境和配置上的处理, 最后逻辑在<code>master.run()</code>处展开, 在<code>master.run()</code></p>
<ul>
<li><strong>ClusterMode : runDriver()</strong></li>
<li><strong>ClientMode: runExecutorLauncher()</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">	<span class="comment">// 解析命令行参数</span></span><br><span class="line">	<span class="comment">// 加载Spark配置</span></span><br><span class="line">	<span class="comment">// 设置系统属性</span></span><br><span class="line">	<span class="comment">// ...</span></span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> yarnConf = <span class="keyword">new</span> <span class="type">YarnConfiguration</span>(<span class="type">SparkHadoopUtil</span>.newConfiguration(sparkConf))  </span><br><span class="line">	master = <span class="keyword">new</span> <span class="type">ApplicationMaster</span>(amArgs, sparkConf, yarnConf)  </span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 获取委托令牌(用于访问HDFS等)</span></span><br><span class="line">	<span class="comment">// ...</span></span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 关键入口点, master.run()方法</span></span><br><span class="line">	ugi.doAs(<span class="keyword">new</span> <span class="type">PrivilegedExceptionAction</span>[<span class="type">Unit</span>]() &#123;  </span><br><span class="line">		<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = <span class="type">System</span>.exit(master.run())  </span><br><span class="line">	&#125;)  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Int</span> = &#123;  </span><br><span class="line">  <span class="keyword">try</span> &#123;  </span><br><span class="line">	  <span class="comment">//...</span></span><br><span class="line">    <span class="keyword">val</span> stagingDirPath = <span class="keyword">new</span> <span class="type">Path</span>(<span class="type">System</span>.getenv(<span class="string">&quot;SPARK_YARN_STAGING_DIR&quot;</span>))  </span><br><span class="line">    <span class="keyword">val</span> stagingDirFs = stagingDirPath.getFileSystem(yarnConf)  </span><br><span class="line">	  <span class="comment">//...</span></span><br><span class="line">    <span class="keyword">if</span> (isClusterMode) &#123;  </span><br><span class="line">      runDriver() </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">      runExecutorLauncher()  </span><br><span class="line">    &#125;  </span><br><span class="line">		<span class="comment">//...</span></span><br><span class="line">	  exitCode </span><br><span class="line">	&#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="runDriver-重点"><a href="#runDriver-重点" class="headerlink" title="runDriver() (重点)"></a>runDriver() (重点)</h3><ol>
<li>startUserApplication(): 启动一个线程, 在线程里面重写的run方法是运行用户程序的main方法, 同时这个线程的名称是Driver, 启动Driver线程<ol>
<li>用户(Driver)线程在运行用户的代码的创建SparkContext(后简称sc)的时候, 会在初始化完毕以后调用wait, 等待Executor都初始化完毕, 防止Driver线程要执行任务, 但是Executor没有申请完毕的情况出现</li>
</ol>
</li>
<li>等待用户进程完成sc的初始化, 向RM注册AM</li>
<li>调用createAllocator(…), AM机向RM申请资源去创建Container</li>
<li>等待Container都申请完毕, 恢复Driver机的运行, 让Driver继续执行用户任务</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runDriver</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 启动Driver线程</span></span><br><span class="line">  userClassThread = startUserApplication()</span><br><span class="line"></span><br><span class="line">  logInfo(<span class="string">&quot;Waiting for spark context initialization...&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> totalWaitTime = sparkConf.get(<span class="type">AM_MAX_WAIT_TIME</span>)</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">   <span class="comment">// 等待用户线程中的SparkContext初始化完毕, timeout AM_MAX_WAIT_TIME ms</span></span><br><span class="line">   <span class="comment">// 用户线程new SparkContext的时候会在初始化完毕以后调用wait(), 这也是后面要resume的原因, 这是用户线程和AM线程同步的方式</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="type">ThreadUtils</span>.awaitResult(sparkContextPromise.future,</span><br><span class="line">      <span class="type">Duration</span>(totalWaitTime, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (sc != <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> rpcEnv = sc.env.rpcEnv</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> userConf = sc.getConf</span><br><span class="line">      <span class="keyword">val</span> host = userConf.get(<span class="type">DRIVER_HOST_ADDRESS</span>)</span><br><span class="line">      <span class="keyword">val</span> port = userConf.get(<span class="type">DRIVER_PORT</span>)</span><br><span class="line">      <span class="comment">// sc初始化完毕以后向RM注册AM</span></span><br><span class="line">      registerAM(host, port, userConf, sc.ui.map(_.webUrl), appAttemptId)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> driverRef = rpcEnv.setupEndpointRef(</span><br><span class="line">        <span class="type">RpcAddress</span>(host, port),</span><br><span class="line">        <span class="type">YarnSchedulerBackend</span>.<span class="type">ENDPOINT_NAME</span>)</span><br><span class="line">        <span class="comment">// 在这里会启动一个后台线程周期性从RM申请Executor</span></span><br><span class="line">      createAllocator(driverRef, userConf, rpcEnv, appAttemptId, distCacheConf())</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">&quot;User did not initialize spark context!&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 恢复用户线程的执行</span></span><br><span class="line">    resumeDriver()</span><br><span class="line">    <span class="comment">// 等待用户线程的执行</span></span><br><span class="line">    userClassThread.join()</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    resumeDriver()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="申请-container-重点"><a href="#申请-container-重点" class="headerlink" title="申请 container (重点)"></a>申请 container (重点)</h3><ol>
<li>创建allocator</li>
<li>第一次尝试申请资源</li>
<li>开启一个后台线程不断执行资源申请<ol>
<li>重试策略是指数退避的, 从0.1s开始, 时间最长到3.0s</li>
<li>这个后台线程不光是在不断申请资源, 同时是这个application的心跳发送线程, YARN中超过60s没有接收到心跳包就会被失联清理掉了</li>
</ol>
</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createAllocator</span></span>(</span><br><span class="line">		driverRef: <span class="type">RpcEndpointRef</span>,</span><br><span class="line">		_sparkConf: <span class="type">SparkConf</span>,</span><br><span class="line">		rpcEnv: <span class="type">RpcEnv</span>,</span><br><span class="line">		appAttemptId: <span class="type">ApplicationAttemptId</span>,</span><br><span class="line">		distCacheConf: <span class="type">SparkConf</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">	<span class="comment">// ...</span></span><br><span class="line">	<span class="keyword">val</span> appId = appAttemptId.getApplicationId().toString()</span><br><span class="line">	<span class="keyword">val</span> driverUrl = <span class="type">RpcEndpointAddress</span>(driverRef.address.host, driverRef.address.port,</span><br><span class="line">		<span class="type">CoarseGrainedSchedulerBackend</span>.<span class="type">ENDPOINT_NAME</span>).toString</span><br><span class="line">	<span class="keyword">val</span> localResources = prepareLocalResources(distCacheConf)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Before we initialize the allocator, let&#x27;s log the information about how executors will</span></span><br><span class="line">	<span class="comment">// be run up front, to avoid printing this out for every single executor being launched.</span></span><br><span class="line">	<span class="comment">// Use placeholders for information that changes such as executor IDs.</span></span><br><span class="line">	<span class="comment">// log...</span></span><br><span class="line">	</span><br><span class="line">	allocator = client.createAllocator(</span><br><span class="line">		yarnConf,</span><br><span class="line">		_sparkConf,</span><br><span class="line">		appAttemptId,</span><br><span class="line">		driverUrl,</span><br><span class="line">		driverRef,</span><br><span class="line">		securityMgr,</span><br><span class="line">		localResources)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Initialize the AM endpoint *after* the allocator has been initialized. This ensures</span></span><br><span class="line">	<span class="comment">// that when the driver sends an initial executor request (e.g. after an AM restart),</span></span><br><span class="line">	<span class="comment">// the allocator is ready to service requests.</span></span><br><span class="line">	rpcEnv.setupEndpoint(<span class="string">&quot;YarnAM&quot;</span>, <span class="keyword">new</span> <span class="type">AMEndpoint</span>(rpcEnv, driverRef))</span><br><span class="line">	<span class="comment">// 申请一次资源</span></span><br><span class="line">	allocator.allocateResources()</span><br><span class="line">	<span class="comment">//... metricsSystem</span></span><br><span class="line">	<span class="comment">// 开启一个后台线程不断地执行allocator.allocateResources(), 重试指数退避</span></span><br><span class="line">	reporterThread = launchReporterThread()</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>

<p>launchReporterThreadq()会启动一个run()内容是执行allocationThreadImpl()方法的线程<br>	1. 这个线程会不断地申请资源, 重试时间是指数退避的<br>	2. 同时这里每次的等待重试是通过在同步代码块中wait + 被唤醒后sleep最小重试间隔(0.2s的策略)<br>		1. 在等待重试的时候, 如果Driver机需要申请新的Executor, 这个时候就会notifyAll, 来加快申请, 但是还是会保证最小的重试时间, 也就是(sleep + wait &gt;&#x3D; 0.2s), 在wait前会记录wait的时间, 如果小于需要的时间, 说明被中断了, 这个时候会sleep<br>		2. 最小的睡眠时间是为了保证RM不会过载, 每次请求之间至少会间隔0.2s</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">allocationThreadImpl</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// The number of failures in a row until the allocation thread gives up.</span></span><br><span class="line">  <span class="keyword">val</span> reporterMaxFailures = sparkConf.get(<span class="type">MAX_REPORTER_THREAD_FAILURES</span>)</span><br><span class="line">  <span class="keyword">var</span> failureCount = <span class="number">0</span></span><br><span class="line">  <span class="keyword">while</span> (!finished) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">     <span class="comment">// 达到了最大失败重试次数</span></span><br><span class="line">      <span class="keyword">if</span> (allocator.getNumExecutorsFailed &gt;= maxNumExecutorFailures) &#123;</span><br><span class="line">        finish(<span class="type">FinalApplicationStatus</span>.<span class="type">FAILED</span>,</span><br><span class="line">          <span class="type">ApplicationMaster</span>.<span class="type">EXIT_MAX_EXECUTOR_FAILURES</span>,</span><br><span class="line">          <span class="string">s&quot;Max number of executor failures (<span class="subst">$maxNumExecutorFailures</span>) reached&quot;</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (allocator.isAllNodeExcluded) &#123;</span><br><span class="line">        finish(<span class="type">FinalApplicationStatus</span>.<span class="type">FAILED</span>,</span><br><span class="line">          <span class="type">ApplicationMaster</span>.<span class="type">EXIT_MAX_EXECUTOR_FAILURES</span>,</span><br><span class="line">          <span class="string">&quot;Due to executor failures all available nodes are excluded&quot;</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        logDebug(<span class="string">&quot;Sending progress&quot;</span>)</span><br><span class="line">        <span class="comment">// 申请资源</span></span><br><span class="line">        allocator.allocateResources()</span><br><span class="line">      &#125;</span><br><span class="line">      failureCount = <span class="number">0</span></span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> numPendingAllocate = allocator.getNumContainersPendingAllocate</span><br><span class="line">      <span class="keyword">var</span> sleepStartNs = <span class="number">0</span>L</span><br><span class="line">      <span class="keyword">var</span> sleepInterval = <span class="number">200</span>L <span class="comment">// ms</span></span><br><span class="line">      allocatorLock.synchronized &#123;</span><br><span class="line">        sleepInterval =</span><br><span class="line">        <span class="comment">// 如果有待分配的容器, 计算重试等待时间</span></span><br><span class="line">          <span class="keyword">if</span> (numPendingAllocate &gt; <span class="number">0</span> || allocator.getNumPendingLossReasonRequests &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">val</span> currentAllocationInterval =</span><br><span class="line">              math.min(heartbeatInterval, nextAllocationInterval)</span><br><span class="line">            nextAllocationInterval = currentAllocationInterval * <span class="number">2</span> <span class="comment">// avoid overflow</span></span><br><span class="line">            currentAllocationInterval</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            nextAllocationInterval = initialAllocationInterval</span><br><span class="line">            heartbeatInterval</span><br><span class="line">          &#125;</span><br><span class="line">        sleepStartNs = <span class="type">System</span>.nanoTime()</span><br><span class="line">        <span class="comment">// 可中断的等待</span></span><br><span class="line">        allocatorLock.wait(sleepInterval)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 被提前唤醒的话, 通过sleep来执行不可中断的等待</span></span><br><span class="line">      <span class="keyword">val</span> sleepDuration = <span class="type">System</span>.nanoTime() - sleepStartNs</span><br><span class="line">      <span class="keyword">if</span> (sleepDuration &lt; <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>.toNanos(sleepInterval)) &#123;</span><br><span class="line">        <span class="keyword">val</span> toSleep = math.max(<span class="number">0</span>, initialAllocationInterval - sleepDuration)</span><br><span class="line">        <span class="keyword">if</span> (toSleep &gt; <span class="number">0</span>) &#123;</span><br><span class="line">          <span class="type">Thread</span>.sleep(toSleep)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>申请资源<code>allocateResources()</code><br>	1. 在allocateResources中完成申请资源请求的添加<br>		1. 计算缺少的Executor数量<br>		2. 创建Container请求<br>		3. 向AMRMClient添加容器请求<br>	2. 在amClient.allocate(progressIndicator)中发送请求到RM<br>	3. 并且在已经申请好的Container中<strong>启动Executor</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">allocateResources</span></span>(): <span class="type">Unit</span> = synchronized &#123;  </span><br><span class="line">	<span class="comment">// 添加请求</span></span><br><span class="line">  updateResourceRequests()  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> progressIndicator = <span class="number">0.1</span>f  </span><br><span class="line">	<span class="comment">// 发送请求</span></span><br><span class="line">  <span class="keyword">val</span> allocateResponse = amClient.allocate(progressIndicator)  </span><br><span class="line"></span><br><span class="line">	<span class="comment">// ....health track	</span></span><br><span class="line">		<span class="keyword">if</span> (allocatedContainers.size &gt; <span class="number">0</span>) &#123;</span><br><span class="line">		<span class="comment">// 这里会在Container中启动Executor</span></span><br><span class="line">		handleAllocatedContainers(allocatedContainers.asScala.toSeq)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateResourceRequests</span></span>(): <span class="type">Unit</span> = synchronized &#123;  </span><br><span class="line">	<span class="comment">// 1. 计算缺少的Executor数量</span></span><br><span class="line">  <span class="keyword">val</span> missingPerProfile = targetNumExecutorsPerResourceProfileId.map &#123; <span class="keyword">case</span> (rpId, targetNum) =&gt;  </span><br><span class="line">    <span class="keyword">val</span> starting = getOrUpdateNumExecutorsStartingForRPId(rpId).get  </span><br><span class="line">    <span class="keyword">val</span> pending = pendingAllocatePerResourceProfileId.getOrElse(rpId, <span class="type">Seq</span>.empty).size  </span><br><span class="line">    <span class="keyword">val</span> running = getOrUpdateRunningExecutorForRPId(rpId).size   </span><br><span class="line">  </span><br><span class="line">  missingPerProfile.foreach &#123; <span class="keyword">case</span> (rpId, missing) =&gt;  </span><br><span class="line">    <span class="keyword">val</span> hostToLocalTaskCount =  </span><br><span class="line">      hostToLocalTaskCountPerResourceProfileId.getOrElse(rpId, <span class="type">Map</span>.empty)  </span><br><span class="line">    <span class="keyword">val</span> pendingAllocate = pendingAllocatePerResourceProfileId.getOrElse(rpId, <span class="type">Seq</span>.empty)  </span><br><span class="line">    <span class="keyword">val</span> numPendingAllocate = pendingAllocate.size  </span><br><span class="line">    <span class="comment">// Split the pending container request into three groups: locality matched list, locality unmatched list and non-locality list. </span></span><br><span class="line">    <span class="comment">// Take the locality matched container request into    </span></span><br><span class="line">		<span class="comment">// consideration of container placement, treat as allocated containers.    // For locality unmatched and locality free container requests, cancel these container    // requests, since required locality preference has been changed, recalculating using    // container placement strategy.    </span></span><br><span class="line">    <span class="keyword">val</span> (localRequests, staleRequests, anyHostRequests) = splitPendingAllocationsByLocality(  </span><br><span class="line">      hostToLocalTaskCount, pendingAllocate)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> (missing &gt; <span class="number">0</span>) &#123;  </span><br><span class="line">      <span class="keyword">val</span> resource = rpIdToYarnResource.get(rpId)  </span><br><span class="line">      <span class="comment">// cancel &quot;stale&quot; requests for locations that are no longer needed  </span></span><br><span class="line">      staleRequests.foreach &#123; stale =&gt;  </span><br><span class="line">        amClient.removeContainerRequest(stale)  </span><br><span class="line">      &#125;  </span><br><span class="line">      <span class="keyword">val</span> cancelledContainers = staleRequests.size  </span><br><span class="line">      <span class="comment">// consider the number of new containers and cancelled stale containers available  </span></span><br><span class="line">      <span class="keyword">val</span> availableContainers = missing + cancelledContainers  </span><br><span class="line">  </span><br><span class="line">      <span class="comment">// to maximize locality, include requests with no locality preference that can be cancelled  </span></span><br><span class="line">      <span class="keyword">val</span> potentialContainers = availableContainers + anyHostRequests.size  </span><br><span class="line">  </span><br><span class="line">      <span class="keyword">val</span> allocatedHostToContainer = getOrUpdateAllocatedHostToContainersMapForRPId(rpId)  </span><br><span class="line">      <span class="keyword">val</span> numLocalityAwareTasks = numLocalityAwareTasksPerResourceProfileId.getOrElse(rpId, <span class="number">0</span>)  </span><br><span class="line">      <span class="keyword">val</span> containerLocalityPreferences = containerPlacementStrategy.localityOfRequestedContainers(  </span><br><span class="line">        potentialContainers, numLocalityAwareTasks, hostToLocalTaskCount,  </span><br><span class="line">        allocatedHostToContainer, localRequests, rpIdToResourceProfile(rpId))  </span><br><span class="line">		  <span class="comment">// 2. 创建容器请求</span></span><br><span class="line">      <span class="keyword">val</span> newLocalityRequests = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[<span class="type">ContainerRequest</span>]  </span><br><span class="line">      containerLocalityPreferences.foreach &#123;  </span><br><span class="line">        <span class="keyword">case</span> <span class="type">ContainerLocalityPreferences</span>(nodes, racks) <span class="keyword">if</span> nodes != <span class="literal">null</span> =&gt;  </span><br><span class="line">          newLocalityRequests += createContainerRequest(resource, nodes, racks, rpId)  </span><br><span class="line">        <span class="keyword">case</span> _ =&gt;  </span><br><span class="line">      &#125;  </span><br><span class="line">  </span><br><span class="line">      <span class="keyword">if</span> (availableContainers &gt;= newLocalityRequests.size) &#123;  </span><br><span class="line">        <span class="comment">// more containers are available than needed for locality, fill in requests for any host  </span></span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until (availableContainers - newLocalityRequests.size)) &#123;  </span><br><span class="line">          newLocalityRequests += createContainerRequest(resource, <span class="literal">null</span>, <span class="literal">null</span>, rpId)  </span><br><span class="line">        &#125;  </span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">        <span class="keyword">val</span> numToCancel = newLocalityRequests.size - availableContainers  </span><br><span class="line">        <span class="comment">// cancel some requests without locality preferences to schedule more local containers  </span></span><br><span class="line">        anyHostRequests.slice(<span class="number">0</span>, numToCancel).foreach &#123; nonLocal =&gt;  </span><br><span class="line">          amClient.removeContainerRequest(nonLocal)  </span><br><span class="line">        &#125;  </span><br><span class="line">      &#125;  </span><br><span class="line">		  </span><br><span class="line">		  <span class="comment">// 3. 向AMRMClient添加容器请求</span></span><br><span class="line">      newLocalityRequests.foreach &#123; request =&gt;  </span><br><span class="line">        amClient.addContainerRequest(request)  </span><br><span class="line">      &#125;   </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="启动Executor"><a href="#启动Executor" class="headerlink" title="启动Executor"></a>启动Executor</h2><p>在YarnAllocator.scala中的handleAllocatedContainers </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleAllocatedContainers</span></span>(allocatedContainers: <span class="type">Seq</span>[<span class="type">Container</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> containersToUse = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Container</span>](allocatedContainers.size)</span><br><span class="line"><span class="comment">// 1. 容器匹配逻辑</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Match incoming requests by host</span></span><br><span class="line">  <span class="keyword">val</span> remainingAfterHostMatches = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Container</span>]</span><br><span class="line">  <span class="keyword">for</span> (allocatedContainer &lt;- allocatedContainers) &#123;</span><br><span class="line">    matchContainerToRequest(allocatedContainer, allocatedContainer.getNodeId.getHost,</span><br><span class="line">      containersToUse, remainingAfterHostMatches)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// a separate thread to perform the operation.</span></span><br><span class="line">  <span class="keyword">val</span> remainingAfterRackMatches = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Container</span>]</span><br><span class="line">  <span class="keyword">if</span> (remainingAfterHostMatches.nonEmpty) &#123;</span><br><span class="line">    <span class="keyword">var</span> exception: <span class="type">Option</span>[<span class="type">Throwable</span>] = <span class="type">None</span></span><br><span class="line">    <span class="keyword">val</span> thread = <span class="keyword">new</span> <span class="type">Thread</span>(<span class="string">&quot;spark-rack-resolver&quot;</span>) &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="keyword">for</span> (allocatedContainer &lt;- remainingAfterHostMatches) &#123;</span><br><span class="line">            <span class="keyword">val</span> rack = resolver.resolve(allocatedContainer.getNodeId.getHost)</span><br><span class="line">            matchContainerToRequest(allocatedContainer, rack, containersToUse,</span><br><span class="line">              remainingAfterRackMatches)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">            exception = <span class="type">Some</span>(e)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    thread.setDaemon(<span class="literal">true</span>)</span><br><span class="line">    thread.start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      thread.join()</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">InterruptedException</span> =&gt;</span><br><span class="line">        thread.interrupt()</span><br><span class="line">        <span class="keyword">throw</span> e</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (exception.isDefined) &#123;</span><br><span class="line">      <span class="keyword">throw</span> exception.get</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Assign remaining that are neither node-local nor rack-local</span></span><br><span class="line">  <span class="keyword">val</span> remainingAfterOffRackMatches = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Container</span>]</span><br><span class="line">  <span class="keyword">for</span> (allocatedContainer &lt;- remainingAfterRackMatches) &#123;</span><br><span class="line">    matchContainerToRequest(allocatedContainer, <span class="type">ANY_HOST</span>, containersToUse,</span><br><span class="line">      remainingAfterOffRackMatches)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 启动Executor</span></span><br><span class="line">  runAllocatedContainers(containersToUse)</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="启动Executor-重点"><a href="#启动Executor-重点" class="headerlink" title="启动Executor (重点)"></a>启动Executor (重点)</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Launches executors in the allocated containers.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runAllocatedContainers</span></span>(containersToUse: <span class="type">ArrayBuffer</span>[<span class="type">Container</span>]): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">  <span class="keyword">for</span> (container &lt;- containersToUse) &#123;</span><br><span class="line">   <span class="comment">// 生成 Executor ID</span></span><br><span class="line">    <span class="keyword">val</span> rpId = getResourceProfileIdFromPriority(container.getPriority)</span><br><span class="line">    executorIdCounter += <span class="number">1</span></span><br><span class="line">    <span class="keyword">val</span> executorHostname = container.getNodeId.getHost</span><br><span class="line">    <span class="keyword">val</span> containerId = container.getId</span><br><span class="line">    <span class="keyword">val</span> executorId = executorIdCounter.toString</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> yarnResourceForRpId = rpIdToYarnResource.get(rpId)</span><br><span class="line">    <span class="keyword">val</span> rp = rpIdToResourceProfile(rpId)</span><br><span class="line">    <span class="keyword">val</span> defaultResources = <span class="type">ResourceProfile</span>.getDefaultProfileExecutorResources(sparkConf)</span><br><span class="line">    <span class="keyword">val</span> containerMem = rp.executorResources.get(<span class="type">ResourceProfile</span>.<span class="type">MEMORY</span>).</span><br><span class="line">      map(_.amount).getOrElse(defaultResources.executorMemoryMiB).toInt</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> defaultCores = defaultResources.cores.get</span><br><span class="line">    <span class="keyword">val</span> containerCores = rp.getExecutorCores.getOrElse(defaultCores)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rpRunningExecs = getOrUpdateRunningExecutorForRPId(rpId).size</span><br><span class="line">    <span class="keyword">if</span> (rpRunningExecs &lt; getOrUpdateTargetNumExecutorsForRPId(rpId)) &#123;</span><br><span class="line">      getOrUpdateNumExecutorsStartingForRPId(rpId).incrementAndGet()</span><br><span class="line">      launchingExecutorContainerIds.add(containerId)</span><br><span class="line">      <span class="keyword">if</span> (launchContainers) &#123;</span><br><span class="line">       <span class="comment">// 线程池中异步启动</span></span><br><span class="line">        launcherPool.execute(() =&gt; &#123;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// 创建新的ExecutorRunnable !!!</span></span><br><span class="line">            <span class="keyword">new</span> <span class="type">ExecutorRunnable</span>(</span><br><span class="line">              <span class="type">Some</span>(container),</span><br><span class="line">              conf,</span><br><span class="line">              sparkConf,</span><br><span class="line">              driverUrl,</span><br><span class="line">              executorId,</span><br><span class="line">              executorHostname,</span><br><span class="line">              containerMem,</span><br><span class="line">              containerCores,</span><br><span class="line">              appAttemptId.getApplicationId.toString,</span><br><span class="line">              securityMgr,</span><br><span class="line">              localResources,</span><br><span class="line">              rp.id</span><br><span class="line">            ).run() <span class="comment">// 执行run()</span></span><br><span class="line">            updateInternalState(rpId, executorId, container)</span><br><span class="line">          &#125; </span><br><span class="line">    &#125; </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="ExecutorRunnable启动容器-重点"><a href="#ExecutorRunnable启动容器-重点" class="headerlink" title="ExecutorRunnable启动容器 (重点)"></a>ExecutorRunnable启动容器 (重点)</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  logDebug(<span class="string">&quot;Starting Executor Container&quot;</span>)  </span><br><span class="line">  nmClient = <span class="type">NMClient</span>.createNMClient()  </span><br><span class="line">  nmClient.init(conf)  </span><br><span class="line">  nmClient.start()  </span><br><span class="line">  startContainer()  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startContainer</span></span>(): java.util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">ByteBuffer</span>] = &#123;  </span><br><span class="line"></span><br><span class="line">	<span class="comment">// 1. 准备启动上下文</span></span><br><span class="line">  <span class="keyword">val</span> ctx = <span class="type">Records</span>.newRecord(classOf[<span class="type">ContainerLaunchContext</span>])  </span><br><span class="line">    .asInstanceOf[<span class="type">ContainerLaunchContext</span>]  </span><br><span class="line">  <span class="keyword">val</span> env = prepareEnvironment().asJava  </span><br><span class="line">  </span><br><span class="line">  ctx.setLocalResources(localResources.asJava)  </span><br><span class="line">  ctx.setEnvironment(env)  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> credentials = <span class="type">UserGroupInformation</span>.getCurrentUser().getCredentials()  </span><br><span class="line">  <span class="keyword">val</span> dob = <span class="keyword">new</span> <span class="type">DataOutputBuffer</span>()  </span><br><span class="line">  credentials.writeTokenStorageToStream(dob)  </span><br><span class="line">  ctx.setTokens(<span class="type">ByteBuffer</span>.wrap(dob.getData()))  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> commands = prepareCommand()  <span class="comment">// 生成容器启动的时候要执行的命令 </span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">// ... </span></span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Send the start request to the ContainerManager  </span></span><br><span class="line">  <span class="keyword">try</span> &#123;  </span><br><span class="line">	  <span class="comment">// 在容器中启动Executor !!!</span></span><br><span class="line">    nmClient.startContainer(container.get, ctx)  </span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;  </span><br><span class="line">    <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt;  </span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s&quot;Exception while starting container <span class="subst">$&#123;container.get.getId&#125;</span>&quot;</span> +  </span><br><span class="line">        <span class="string">s&quot; on host <span class="subst">$hostname</span>&quot;</span>, ex)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<pre><code>在这个生成命令的方法里, 会生成JVM参数, 以及最后要Executor要执行的`org.apache.spark.executor.YarnCoarseGrainedExecutorBackendl`类
</code></pre>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">prepareCommand</span></span>(): <span class="type">List</span>[<span class="type">String</span>] = &#123;  </span><br><span class="line">  <span class="comment">// Extra options for the JVM  </span></span><br><span class="line">  <span class="keyword">val</span> javaOpts = <span class="type">ListBuffer</span>[<span class="type">String</span>]()  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Set the JVM memory  </span></span><br><span class="line">  <span class="keyword">val</span> executorMemoryString = <span class="string">s&quot;<span class="subst">$&#123;executorMemory&#125;</span>m&quot;</span>  </span><br><span class="line">  javaOpts += <span class="string">&quot;-Xmx&quot;</span> + executorMemoryString  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Set extra Java options for the executor, if defined  </span></span><br><span class="line">  sparkConf.get(<span class="type">EXECUTOR_JAVA_OPTIONS</span>).foreach &#123; opts =&gt;  </span><br><span class="line">    <span class="keyword">val</span> subsOpt = <span class="type">Utils</span>.substituteAppNExecIds(opts, appId, executorId)  </span><br><span class="line">    javaOpts ++= <span class="type">Utils</span>.splitCommandString(subsOpt).map(<span class="type">YarnSparkHadoopUtil</span>.escapeForShell)  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Set the library path through a command prefix to append to the existing value of the  </span></span><br><span class="line">  <span class="comment">// env variable.  val prefixEnv = sparkConf.get(EXECUTOR_LIBRARY_PATH).map &#123; libPath =&gt;  </span></span><br><span class="line">    <span class="type">Client</span>.createLibraryPathPrefix(libPath, sparkConf)  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  javaOpts += <span class="string">&quot;-Djava.io.tmpdir=&quot;</span> +  </span><br><span class="line">    <span class="keyword">new</span> <span class="type">Path</span>(<span class="type">Environment</span>.<span class="type">PWD</span>.$$(), <span class="type">YarnConfiguration</span>.<span class="type">DEFAULT_CONTAINER_TEMP_DIR</span>)  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Certain configs need to be passed here because they are needed before the Executor  </span></span><br><span class="line">  <span class="comment">// registers with the Scheduler and transfers the spark configs. Since the Executor backend  // uses RPC to connect to the scheduler, the RPC settings are needed as well as the  // authentication settings.  sparkConf.getAll  </span></span><br><span class="line">    .filter &#123; <span class="keyword">case</span> (k, v) =&gt; <span class="type">SparkConf</span>.isExecutorStartupConf(k) &#125;  </span><br><span class="line">    .foreach &#123; <span class="keyword">case</span> (k, v) =&gt; javaOpts += <span class="type">YarnSparkHadoopUtil</span>.escapeForShell(<span class="string">s&quot;-D<span class="subst">$k</span>=<span class="subst">$v</span>&quot;</span>) &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// For log4j configuration to reference  </span></span><br><span class="line">  javaOpts += (<span class="string">&quot;-Dspark.yarn.app.container.log.dir=&quot;</span> + <span class="type">ApplicationConstants</span>.<span class="type">LOG_DIR_EXPANSION_VAR</span>)  </span><br><span class="line">  </span><br><span class="line">  <span class="type">YarnSparkHadoopUtil</span>.addOutOfMemoryErrorArgument(javaOpts)  </span><br><span class="line">  <span class="keyword">val</span> commands = prefixEnv ++  </span><br><span class="line">    <span class="type">Seq</span>(<span class="type">Environment</span>.<span class="type">JAVA_HOME</span>.$$() + <span class="string">&quot;/bin/java&quot;</span>, <span class="string">&quot;-server&quot;</span>) ++  </span><br><span class="line">    javaOpts ++  </span><br><span class="line">    <span class="type">Seq</span>(<span class="string">&quot;org.apache.spark.executor.YarnCoarseGrainedExecutorBackend&quot;</span>,  </span><br><span class="line">      <span class="string">&quot;--driver-url&quot;</span>, masterAddress,  </span><br><span class="line">      <span class="string">&quot;--executor-id&quot;</span>, executorId,  </span><br><span class="line">      <span class="string">&quot;--hostname&quot;</span>, hostname,  </span><br><span class="line">      <span class="string">&quot;--cores&quot;</span>, executorCores.toString,  </span><br><span class="line">      <span class="string">&quot;--app-id&quot;</span>, appId,  </span><br><span class="line">      <span class="string">&quot;--resourceProfileId&quot;</span>, resourceProfileId.toString) ++  </span><br><span class="line">    <span class="type">Seq</span>(  </span><br><span class="line">      <span class="string">s&quot;1&gt;<span class="subst">$&#123;ApplicationConstants.LOG_DIR_EXPANSION_VAR&#125;</span>/stdout&quot;</span>,  </span><br><span class="line">      <span class="string">s&quot;2&gt;<span class="subst">$&#123;ApplicationConstants.LOG_DIR_EXPANSION_VAR&#125;</span>/stderr&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> it would be nicer to just make sure there are no null commands here  </span></span><br><span class="line">  commands.map(s =&gt; <span class="keyword">if</span> (s == <span class="literal">null</span>) <span class="string">&quot;null&quot;</span> <span class="keyword">else</span> s).toList  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>NM接受到请求后执行命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">java -Xmx2048m \</span><br><span class="line">  org.apache.spark.executor.YarnCoarseGrainedExecutorBackend \</span><br><span class="line">  --driver-url spark://CoarseGrainedScheduler@driver:7077 \</span><br><span class="line">  --executor-id 1 \</span><br><span class="line">  --hostname worker-node-1 \</span><br><span class="line">  --cores 2 \</span><br><span class="line">  --app-id application_1234567890_0001 \</span><br><span class="line">  --resourceProfileId 0</span><br></pre></td></tr></table></figure>

<h3 id="Executor启动后连接Driver"><a href="#Executor启动后连接Driver" class="headerlink" title="Executor启动后连接Driver"></a>Executor启动后连接Driver</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">object</span> <span class="title">YarnCoarseGrainedExecutorBackend</span> <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">	  <span class="comment">// 解析参数, 创建RPC环境</span></span><br><span class="line">    <span class="keyword">val</span> createFn: (<span class="type">RpcEnv</span>, <span class="type">CoarseGrainedExecutorBackend</span>.<span class="type">Arguments</span>, <span class="type">SparkEnv</span>, <span class="type">ResourceProfile</span>) =&gt;</span><br><span class="line">      <span class="type">CoarseGrainedExecutorBackend</span> = &#123; <span class="keyword">case</span> (rpcEnv, arguments, env, resourceProfile) =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">YarnCoarseGrainedExecutorBackend</span>(rpcEnv, arguments.driverUrl, arguments.executorId,</span><br><span class="line">        arguments.bindAddress, arguments.hostname, arguments.cores,</span><br><span class="line">        env, arguments.resourcesFileOpt, resourceProfile)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> backendArgs = <span class="type">CoarseGrainedExecutorBackend</span>.parseArguments(args,</span><br><span class="line">      <span class="keyword">this</span>.getClass.getCanonicalName.stripSuffix(<span class="string">&quot;$&quot;</span>))</span><br><span class="line">      </span><br><span class="line">		<span class="comment">// 连接Driver</span></span><br><span class="line">    <span class="type">CoarseGrainedExecutorBackend</span>.run(backendArgs, createFn)</span><br><span class="line">    <span class="type">System</span>.exit(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="type">YarnCoarseGrainedExecutorBackend</span>.main()</span><br><span class="line">    │</span><br><span class="line">    ├─ 解析参数</span><br><span class="line">    ├─ 创建 <span class="type">RPC</span> 环境</span><br><span class="line">    ├─ 连接到 <span class="type">Driver</span></span><br><span class="line">    │</span><br><span class="line">    └─ 向 <span class="type">Driver</span> 注册 <span class="type">Executor</span></span><br><span class="line">    </span><br><span class="line"><span class="type">Driver</span> 的 <span class="type">CoarseGrainedSchedulerBackend</span> 接收注册</span><br><span class="line">    │</span><br><span class="line">    └─ <span class="type">TaskScheduler</span> 开始分配 <span class="type">Task</span> 到 <span class="type">Executor</span></span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://functional.top">John Doe</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://functional.top/2025/11/21/Big-Data/Spark/source%20code%20read/2.%20How%20spark%20access%20Yarn%20and%20start%20container%20to%20run%20tasks./">https://functional.top/2025/11/21/Big-Data/Spark/source%20code%20read/2.%20How%20spark%20access%20Yarn%20and%20start%20container%20to%20run%20tasks./</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/BigData/">BigData</a><a class="post-meta__tags" href="/tags/Spark/">Spark</a><a class="post-meta__tags" href="/tags/source-code/">source_code</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/11/24/Big-Data/Spark/source%20code%20read/3.%20How%20spark%20job%20trigger%20a%20job%20and%20how%20it%20split%20job%20to%20stages%20and%20schedule%20the%20stage./" title="[Spark Source Code 3] How your spark job trigger a job and how it split job to stages and schedule the stage."><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">[Spark Source Code 3] How your spark job trigger a job and how it split job to stages and schedule the stage.</div></div><div class="info-2"><div class="info-item-1">Spark启动一个job的过程前置条件在 AM 执行完 resumeDriver() 后,Executor 已经分配并注册完成,用户代码继续运行。当代码执行到 action 操作时,开始创建和提交 job。 详细步骤 代码执行到action操作, 这个时候已经有了有transformation操作形成的RDD DAG 窄依赖: 父RDD的每个分区只被一个子RDD分区使用, 如(map, filter) 宽依赖: 父RDD的每个分区都有可能被多个子RDD分区使用, 如(reduceByKey) 注意对于宽窄依赖来说, 依赖的粒度是分区级别, 同时这个属性是某个transfomation的固有属性, 这是这个算子的计算方式决定的, 如果他产生一条数据需要父RDD中不止一条数据, 涉及到数据的聚合,这个时候这个操作就会是宽依赖的操作 所有的action操作最后都会调用到SparkContext.runJob()   SparkContext提交Job到DAGScheduler sc.runJob进行参数清理和验证 调用DAGScheduler.runJob() -&gt; submit...</div></div></div></a><a class="pagination-related" href="/2025/11/18/Big-Data/Spark/source%20code%20read/1.%20How%20launch%20a%20spark%20application/" title="[Spark Source Code 1] How launch a spark Application"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">[Spark Source Code 1] How launch a spark Application</div></div><div class="info-2"><div class="info-item-1"> 只关注spark application通过spark-submit shell脚本启动的情况以问题为导向探究   ✅ spark-submit 脚本做了什么？ ✅ Launcher 层的作用是什么？为什么需要它？ ✅ 参数是如何从 Shell 传递到 Java 的？ ✅ 为什么使用 NULL 分隔符？ ✅ prepareSubmitEnvironment() 返回的 4 个值分别是什么？ ✅ Client 和 Cluster 模式的 childMainClass 有什么不同？ ✅ 不同集群管理器（YARN&#x2F;K8s&#x2F;Standalone）的启动有什么区别？ ✅ app.start() 之后发生了什么？ ✅ 配置的优先级是怎样的？ ✅ 为什么某些模式组合不支持（如 LOCAL + CLUSTER）？  How spark command is parsedshell脚本入口阶段 找到SPARK_HOME, JAVA_HOME, 将launcher入口类和submit入口类添加到执行路径中, 补全java命令, 执行命令  整体的流程entry pointfil...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/11/18/Big-Data/Spark/source%20code%20read/1.%20How%20launch%20a%20spark%20application/" title="[Spark Source Code 1] How launch a spark Application"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-18</div><div class="info-item-2">[Spark Source Code 1] How launch a spark Application</div></div><div class="info-2"><div class="info-item-1"> 只关注spark application通过spark-submit shell脚本启动的情况以问题为导向探究   ✅ spark-submit 脚本做了什么？ ✅ Launcher 层的作用是什么？为什么需要它？ ✅ 参数是如何从 Shell 传递到 Java 的？ ✅ 为什么使用 NULL 分隔符？ ✅ prepareSubmitEnvironment() 返回的 4 个值分别是什么？ ✅ Client 和 Cluster 模式的 childMainClass 有什么不同？ ✅ 不同集群管理器（YARN&#x2F;K8s&#x2F;Standalone）的启动有什么区别？ ✅ app.start() 之后发生了什么？ ✅ 配置的优先级是怎样的？ ✅ 为什么某些模式组合不支持（如 LOCAL + CLUSTER）？  How spark command is parsedshell脚本入口阶段 找到SPARK_HOME, JAVA_HOME, 将launcher入口类和submit入口类添加到执行路径中, 补全java命令, 执行命令  整体的流程entry pointfil...</div></div></div></a><a class="pagination-related" href="/2025/11/24/Big-Data/Spark/source%20code%20read/3.%20How%20spark%20job%20trigger%20a%20job%20and%20how%20it%20split%20job%20to%20stages%20and%20schedule%20the%20stage./" title="[Spark Source Code 3] How your spark job trigger a job and how it split job to stages and schedule the stage."><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-24</div><div class="info-item-2">[Spark Source Code 3] How your spark job trigger a job and how it split job to stages and schedule the stage.</div></div><div class="info-2"><div class="info-item-1">Spark启动一个job的过程前置条件在 AM 执行完 resumeDriver() 后,Executor 已经分配并注册完成,用户代码继续运行。当代码执行到 action 操作时,开始创建和提交 job。 详细步骤 代码执行到action操作, 这个时候已经有了有transformation操作形成的RDD DAG 窄依赖: 父RDD的每个分区只被一个子RDD分区使用, 如(map, filter) 宽依赖: 父RDD的每个分区都有可能被多个子RDD分区使用, 如(reduceByKey) 注意对于宽窄依赖来说, 依赖的粒度是分区级别, 同时这个属性是某个transfomation的固有属性, 这是这个算子的计算方式决定的, 如果他产生一条数据需要父RDD中不止一条数据, 涉及到数据的聚合,这个时候这个操作就会是宽依赖的操作 所有的action操作最后都会调用到SparkContext.runJob()   SparkContext提交Job到DAGScheduler sc.runJob进行参数清理和验证 调用DAGScheduler.runJob() -&gt; submit...</div></div></div></a><a class="pagination-related" href="/2026/02/08/Big-Data/Hive/" title="Hive Introduction"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-08</div><div class="info-item-2">Hive Introduction</div></div><div class="info-2"><div class="info-item-1">Hive的作用Hive将提交任务这一件事情简化, 屏蔽了执行引擎, 提供了HiveQL供用户使用, 用户可以使用SQL的形式提交并执行任务. 从输入数据到最后的执行经过的步骤:  用户通过客户端连接到HiveServer2提交SQL -&gt; Driver接受客户端的HiveQL -&gt; Compiler &#x2F; Semantic Analyzer &#x2F; Optimizer将QL转化成逻辑计划, 物理计划, 并进行CBO优化 -&gt; Execution Engine将物理计划分解成具体的执行任务, 提交到底层的计算引擎上 (MapReduce, Spark) -&gt; 在HDFS上存储数据, 由YARN分配资源给计算引擎 另一条线是HiveServer2 -&gt; Hive MetaStore Server访问元数据, 通过Thrift提供元数据 (表&#x2F;分区&#x2F;统计), 并将结果持久化到关系型数据库  HMS (Hive Metastore)HMS </div></div></div></a><a class="pagination-related" href="/2025/10/30/Big-Data/Livy/Livy_1_Basic/" title="Livy基础知识"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-30</div><div class="info-item-2">Livy基础知识</div></div><div class="info-2"><div class="info-item-1">概念理解Livy 解决了什么问题？为什么不直接用 spark-submit？解决了用户提交spark任务的易用性和控制性问题, 使用户能通过HTTP请求向spark提交任务  权限问题: 使用spark-submit, 用户需要登陆上服务器, 使用spark-submit命令提交, 这将spark所在的服务器的访问权限和提交spark任务的权限绑定, 但是实际上提交spark任务的role和访问服务器的role应该是分离的. 易用性问题: 在过去用户需要在本地写好代码, 通过SFTP上传到服务器上, 再通过spark-submit提交任务, 现在用户可以通过jupyter + livy的方式, 在本地编写代码, 动态变更并且随时通过HTTP向Livy提交任务 自动化问题: 直接通过spark-submit提交任务, 无法对于spark任务进行集中管理, 捕获一些指标, 或者进行配置覆盖等其他的自动化的操作, Livy的引入相当于为用户和spark-submit之间注入了一个管理的中间层  Livy 支持哪几种 Session 类型？它们的区别是什么？Session Type分成I...</div></div></div></a><a class="pagination-related" href="/2026/02/08/Big-Data/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%80%BB%E8%A7%88/" title="大数据结构总览"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-08</div><div class="info-item-2">大数据结构总览</div></div><div class="info-2"><div class="info-item-1">大数据结构大数据可以分成五个层次 数据收集:将数据从数据源收集到数据存储层, 常用的组件有关系型, 非关系型, 分布式消息队列三种  关系型: Sqooq&#x2F;Canal, 连接MySQL这种关系型数据库和Hadoop之间的桥梁, Sqool可以全量将数据库中的数据导入到Hadoop中, Canal则能实现增量的导入 非关系型: Flume, 流式数据收集, 比如日志等数据, 经过ETL以后导入到Hadoop中 分布式消息队列: [[Kafka]], 常用作消息总线, 有分布式的高容错的设计, 适配大数据场景  数据存储主要由分布式文件系统和分布式数据库组成  分布式文件系统: HDFS, Hadoop分布式文件系统, 有强大的容错机制, 社区开发了很多种文件格式 分布式数据库: HBase, 构建在HDFS之上的分布式数据库, 提供结构化和半结构化的数据库, 支持列无限拓展  资源管理和服务调度 YARN: 统一资源管理与调度系统, 能够管理集群中的各种资源, 并按照一定策略分配给上层的各类应用. 同时支持灵活的配置, 允许用户按照队列的方式组织和管理资源, 且每个队列的...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">John Doe</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-on-YARN%E7%9A%84%E6%95%B4%E4%BD%93%E8%BF%90%E8%A1%8C%E7%9A%84%E6%9E%B6%E6%9E%84"><span class="toc-number">1.</span> <span class="toc-text">Spark on YARN的整体运行的架构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#YARN"><span class="toc-number">1.1.</span> <span class="toc-text">YARN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-on-YARN%E7%9A%84%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="toc-number">1.2.</span> <span class="toc-text">Spark on YARN的运行流程</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90"><span class="toc-number">2.</span> <span class="toc-text">源码解析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8F%90%E4%BA%A4AM"><span class="toc-number">2.1.</span> <span class="toc-text">提交AM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Client%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0"><span class="toc-number">2.1.1.</span> <span class="toc-text">Client构造函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Client-run"><span class="toc-number">2.1.2.</span> <span class="toc-text">Client.run()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%86app%E6%8F%90%E4%BA%A4%E5%88%B0YARN%E4%B8%8A-%E9%87%8D%E7%82%B9"><span class="toc-number">2.1.3.</span> <span class="toc-text">将app提交到YARN上 (重点)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AM%E8%AF%B7%E6%B1%82%E8%B5%84%E6%BA%90"><span class="toc-number">2.2.</span> <span class="toc-text">AM请求资源</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#runDriver-%E9%87%8D%E7%82%B9"><span class="toc-number">2.2.1.</span> <span class="toc-text">runDriver() (重点)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%B3%E8%AF%B7-container-%E9%87%8D%E7%82%B9"><span class="toc-number">2.2.2.</span> <span class="toc-text">申请 container (重点)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8Executor"><span class="toc-number">2.3.</span> <span class="toc-text">启动Executor</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8Executor-%E9%87%8D%E7%82%B9"><span class="toc-number">2.3.1.</span> <span class="toc-text">启动Executor (重点)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ExecutorRunnable%E5%90%AF%E5%8A%A8%E5%AE%B9%E5%99%A8-%E9%87%8D%E7%82%B9"><span class="toc-number">2.3.2.</span> <span class="toc-text">ExecutorRunnable启动容器 (重点)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Executor%E5%90%AF%E5%8A%A8%E5%90%8E%E8%BF%9E%E6%8E%A5Driver"><span class="toc-number">2.3.3.</span> <span class="toc-text">Executor启动后连接Driver</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/08/Git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" title="Untitled">Untitled</a><time datetime="2026-02-08T11:04:02.565Z" title="Created 2026-02-08 11:04:02">2026-02-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/08/Big-Data/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%80%BB%E8%A7%88/" title="大数据结构总览">大数据结构总览</a><time datetime="2026-02-08T11:04:02.565Z" title="Created 2026-02-08 11:04:02">2026-02-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/08/How/How_to_Code/" title="Untitled">Untitled</a><time datetime="2026-02-08T11:04:02.565Z" title="Created 2026-02-08 11:04:02">2026-02-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/08/How/How_to_Learn_a_Language/" title="如何学习一门语言(以scala为例)">如何学习一门语言(以scala为例)</a><time datetime="2026-02-08T11:04:02.565Z" title="Created 2026-02-08 11:04:02">2026-02-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/08/Language/Scala%20Basic/" title="Scala基础语法">Scala基础语法</a><time datetime="2026-02-08T11:04:02.565Z" title="Created 2026-02-08 11:04:02">2026-02-08</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By John Doe</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.0</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>