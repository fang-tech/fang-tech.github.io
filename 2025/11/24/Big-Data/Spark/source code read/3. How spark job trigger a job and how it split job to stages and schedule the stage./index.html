<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>[Spark Source Code 3] How your spark job trigger a job and how it split job to stages and schedule the stage. | Hexo</title><meta name="author" content="John Doe"><meta name="copyright" content="John Doe"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Spark启动一个job的过程前置条件在 AM 执行完 resumeDriver() 后,Executor 已经分配并注册完成,用户代码继续运行。当代码执行到 action 操作时,开始创建和提交 job。 详细步骤 代码执行到action操作, 这个时候已经有了有transformation操作形成的RDD DAG 窄依赖: 父RDD的每个分区只被一个子RDD分区使用, 如(map, filte">
<meta property="og:type" content="article">
<meta property="og:title" content="[Spark Source Code 3] How your spark job trigger a job and how it split job to stages and schedule the stage.">
<meta property="og:url" content="https://functional.top/2025/11/24/Big-Data/Spark/source%20code%20read/3.%20How%20spark%20job%20trigger%20a%20job%20and%20how%20it%20split%20job%20to%20stages%20and%20schedule%20the%20stage./index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Spark启动一个job的过程前置条件在 AM 执行完 resumeDriver() 后,Executor 已经分配并注册完成,用户代码继续运行。当代码执行到 action 操作时,开始创建和提交 job。 详细步骤 代码执行到action操作, 这个时候已经有了有transformation操作形成的RDD DAG 窄依赖: 父RDD的每个分区只被一个子RDD分区使用, 如(map, filte">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://functional.top/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-11-24T00:00:00.000Z">
<meta property="article:modified_time" content="2026-02-08T11:04:02.565Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="BigData">
<meta property="article:tag" content="Spark">
<meta property="article:tag" content="source_code">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://functional.top/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "[Spark Source Code 3] How your spark job trigger a job and how it split job to stages and schedule the stage.",
  "url": "https://functional.top/2025/11/24/Big-Data/Spark/source%20code%20read/3.%20How%20spark%20job%20trigger%20a%20job%20and%20how%20it%20split%20job%20to%20stages%20and%20schedule%20the%20stage./",
  "image": "https://functional.top/img/butterfly-icon.png",
  "datePublished": "2025-11-24T00:00:00.000Z",
  "dateModified": "2026-02-08T11:04:02.565Z",
  "author": [
    {
      "@type": "Person",
      "name": "John Doe",
      "url": "https://functional.top"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://functional.top/2025/11/24/Big-Data/Spark/source%20code%20read/3.%20How%20spark%20job%20trigger%20a%20job%20and%20how%20it%20split%20job%20to%20stages%20and%20schedule%20the%20stage./index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '[Spark Source Code 3] How your spark job trigger a job and how it split job to stages and schedule the stage.',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Hexo</span></a><a class="nav-page-title" href="/"><span class="site-name">[Spark Source Code 3] How your spark job trigger a job and how it split job to stages and schedule the stage.</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">[Spark Source Code 3] How your spark job trigger a job and how it split job to stages and schedule the stage.</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-11-24T00:00:00.000Z" title="Created 2025-11-24 00:00:00">2025-11-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-08T11:04:02.565Z" title="Updated 2026-02-08 11:04:02">2026-02-08</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Spark启动一个job的过程"><a href="#Spark启动一个job的过程" class="headerlink" title="Spark启动一个job的过程"></a>Spark启动一个job的过程</h1><h2 id="前置条件"><a href="#前置条件" class="headerlink" title="前置条件"></a>前置条件</h2><p>在 AM 执行完 resumeDriver() 后,Executor 已经分配并注册完成,<br>用户代码继续运行。当代码执行到 action 操作时,开始创建和提交 job。</p>
<h2 id="详细步骤"><a href="#详细步骤" class="headerlink" title="详细步骤"></a>详细步骤</h2><ol>
<li>代码执行到action操作, 这个时候已经有了有transformation操作形成的RDD DAG<ul>
<li>窄依赖: 父RDD的每个分区只被一个子RDD分区使用, 如(map, filter)</li>
<li>宽依赖: 父RDD的每个分区都有可能被多个子RDD分区使用, 如(reduceByKey)</li>
<li>注意对于宽窄依赖来说, 依赖的粒度是分区级别, 同时这个属性是某个transfomation的固有属性, 这是这个算子的计算方式决定的, 如果他产生一条数据需要父RDD中不止一条数据, 涉及到数据的聚合,这个时候这个操作就会是宽依赖的操作</li>
<li>所有的action操作最后都会调用到SparkContext.runJob()</li>
</ul>
</li>
<li>SparkContext提交Job到DAGScheduler<ul>
<li>sc.runJob进行参数清理和验证</li>
<li>调用DAGScheduler.runJob() -&gt; submitJob()</li>
<li>生成唯一的JobId</li>
<li>向DAGScheduler事件循环发送JobSubmitted事件</li>
</ul>
</li>
<li>DAGScheduler 划分Stage<ul>
<li>从最终的RDD开始, 向上遍历已经存在的RDD依赖链</li>
<li>划分的规则: 碰到了宽依赖, 就切分出来新的Stage</li>
<li>窄依赖的RDD会被划分到一个Stage中</li>
<li>递归创建所有的Stage, 形成Stage DAG</li>
</ul>
</li>
<li>创建ActiveJob, 并提交Stage<ul>
<li>使用finalStage创建ActiveJob对象</li>
<li>调用SubmitStage开始调度</li>
<li>采用递归策略, 先提交父Stage, 再提交子Stage, 来保证Stage之间的拓扑顺序的不变</li>
</ul>
</li>
<li>将Stage分解成Tasks<ul>
<li>为Stage中的每个需要计算的partition创建一个Task</li>
<li>ShuffleMapStage -&gt; ShuffleMapTask</li>
<li>ResultStage -&gt; ResultTask</li>
<li>将Task封装成TaskSet</li>
</ul>
</li>
<li>TaskScheduler调度Tasks到Executor<ul>
<li>创建TaskSetManager管理TaskSet</li>
<li>根据调度策略和数据本地性分配Tasks</li>
<li>将Tasks序列化后发送给Executor执行</li>
</ul>
</li>
<li>Executor执行Tasks并返回结果<ul>
<li>Executor执行Task, 计算RDD的partition</li>
<li>ShuffleMapTask写入shuffle数据</li>
<li>ResultTask返回最后的结果</li>
<li>将数据返回给Driver</li>
</ul>
</li>
</ol>
<h1 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h1><blockquote>
<p>现在的时刻是AM执行完resumeDriver()的时机, 这个时候Executor已经分配好了, 用户的代码继续运行. </p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 用户代码</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;data.txt&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = rdd1.map(line =&gt; line.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd2.filter(arr =&gt; arr.length &gt; <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> rdd4 = rdd3.map(arr =&gt; (arr(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> rdd5 = rdd4.reduceByKey(_ + _)</span><br><span class="line"><span class="keyword">val</span> result = rdd5.collect()</span><br></pre></td></tr></table></figure>
<h2 id="1-用户的transformation操作形成RDD-DAG"><a href="#1-用户的transformation操作形成RDD-DAG" class="headerlink" title="1. 用户的transformation操作形成RDD DAG"></a>1. 用户的transformation操作形成RDD DAG</h2><h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3><p>Spark懒执行的实现方式就是, 执行transformation操作的时候, 实际上并不会真的去做这个生成新的RDD的操作, 而是记录这条操作路径, 而每个操作的必要信息都记录在RDD中<br>这里的核心信息有四个</p>
<ul>
<li>prev: 父RDD的引用</li>
<li>deps: 这个RDD对应的操作的依赖类型</li>
<li>func: 这个操作要执行的函数(会被清理闭包)</li>
<li>partitions: 分区信息</li>
</ul>
<p>RDD源码</p>
<ul>
<li>一定要重写的方法<ul>
<li>getPartitions</li>
<li>getDependencies</li>
<li>compute</li>
</ul>
</li>
<li>如果在继承这个类的时候, 使用的构造函数是只将父RDD传入的构造方法, 说明这个子类是窄依赖</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">RDD</span>[<span class="type">T</span>: <span class="type">ClassTag</span>](<span class="params">  </span></span></span><br><span class="line"><span class="params"><span class="class">    @transient private var _sc: <span class="type">SparkContext</span>,  </span></span></span><br><span class="line"><span class="params"><span class="class">    @transient private var deps: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]]  </span></span></span><br><span class="line"><span class="params"><span class="class">  </span>) <span class="keyword">extends</span> <span class="title">Serializable</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 窄依赖类型的RDD使用的构造函数, 只用传入父RDD的引用, 依赖关系自动指定为窄依赖</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(<span class="meta">@transient</span> oneParent: <span class="type">RDD</span>[_]) =  </span><br><span class="line">    <span class="keyword">this</span>(oneParent.context, <span class="type">List</span>(<span class="keyword">new</span> <span class="type">OneToOneDependency</span>(oneParent)))  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">conf</span> </span>= sc.conf  </span><br><span class="line">  <span class="comment">// =======================================================================  </span></span><br><span class="line">  <span class="comment">// Methods that should be implemented by subclasses of RDD  // =======================================================================  </span></span><br><span class="line">  <span class="comment">// compute a given partition.? TODO</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">	<span class="comment">// 返回这个RDD中的partition集合</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>]  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 返回依赖关系</span></span><br><span class="line"> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = deps  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 子类可选实现项: specify placement preferences.? TODO</span></span><br><span class="line"> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocations</span></span>(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span>  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">// 子类可选实现项: 定制的分区器</span></span><br><span class="line">  <span class="meta">@transient</span> <span class="keyword">val</span> partitioner: <span class="type">Option</span>[<span class="type">Partitioner</span>] = <span class="type">None</span>  </span><br><span class="line">  </span><br><span class="line"><span class="comment">// ... else code</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>窄依赖类型RDD实现: MapPartitionsRDD</p>
<ul>
<li>继承的时候使用的构造函数是只指定父RDD的构造函数, 说明这是一个窄依赖的RDD</li>
<li>getPartitions: 因为是窄依赖, 使用父RDD的分区情况就行</li>
<li>compute: TODO</li>
<li>partitioner: 使用父RDD的分区器</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">MapPartitionsRDD</span>[<span class="type">U</span>: <span class="type">ClassTag</span>, <span class="type">T</span>: <span class="type">ClassTag</span>](<span class="params">  </span></span></span><br><span class="line"><span class="params"><span class="class">    var prev: <span class="type">RDD</span>[<span class="type">T</span>],  </span></span></span><br><span class="line"><span class="params"><span class="class">    f: (<span class="type">TaskContext</span>, <span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">T</span>]</span>) <span class="title">=&gt;</span> <span class="title">Iterator</span>[<span class="type">U</span>],  <span class="comment">// (TaskContext, partition index, iterator)  </span></span></span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>,  </span><br><span class="line">    isFromBarrier: <span class="type">Boolean</span> = <span class="literal">false</span>,  </span><br><span class="line">    isOrderSensitive: <span class="type">Boolean</span> = <span class="literal">false</span>)  </span><br><span class="line">  <span class="keyword">extends</span> <span class="type">RDD</span>[<span class="type">U</span>](prev) &#123;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> partitioner = <span class="keyword">if</span> (preservesPartitioning) firstParent[<span class="type">T</span>].partitioner <span class="keyword">else</span> <span class="type">None</span>  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = firstParent[<span class="type">T</span>].partitions  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">U</span>] =  </span><br><span class="line">    f(context, split.index, firstParent[<span class="type">T</span>].iterator(split, context))  </span><br><span class="line">	<span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>宽依赖类型的RDD实现: ShuffledRDD</p>
<ul>
<li>getDependencies: 返回<code>List[ShuffleDependency]</code>, <strong>宽依赖的RDD的deps是<code>List[ShuffleDependency]</code></strong></li>
<li>partitioner: 涉及到shuffle操作, 所以需要传入shuffle后的分区使用的分区器</li>
<li>getPartitions: 包装一遍 ? TODO</li>
<li>compute: TODO</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShuffledRDD</span>[<span class="type">K</span>: <span class="type">ClassTag</span>, <span class="type">V</span>: <span class="type">ClassTag</span>, <span class="type">C</span>: <span class="type">ClassTag</span>](<span class="params">  </span></span></span><br><span class="line"><span class="params"><span class="class">    @transient var prev: <span class="type">RDD</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],  </span></span></span><br><span class="line"><span class="params"><span class="class">    part: <span class="type">Partitioner</span></span>)  </span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)](prev.context, <span class="type">Nil</span>) &#123;  </span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = &#123;  </span><br><span class="line">    <span class="keyword">val</span> serializer = userSpecifiedSerializer.getOrElse &#123;  </span><br><span class="line">      <span class="keyword">val</span> serializerManager = <span class="type">SparkEnv</span>.get.serializerManager  </span><br><span class="line">      <span class="keyword">if</span> (mapSideCombine) &#123;  </span><br><span class="line">        serializerManager.getSerializer(implicitly[<span class="type">ClassTag</span>[<span class="type">K</span>]], implicitly[<span class="type">ClassTag</span>[<span class="type">C</span>]])  </span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">        serializerManager.getSerializer(implicitly[<span class="type">ClassTag</span>[<span class="type">K</span>]], implicitly[<span class="type">ClassTag</span>[<span class="type">V</span>]])  </span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="type">List</span>(<span class="keyword">new</span> <span class="type">ShuffleDependency</span>(prev, part, serializer, keyOrdering, aggregator, mapSideCombine))  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> partitioner = <span class="type">Some</span>(part)  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = &#123;  </span><br><span class="line">    <span class="type">Array</span>.tabulate[<span class="type">Partition</span>](part.numPartitions)(i =&gt; <span class="keyword">new</span> <span class="type">ShuffledRDDPartition</span>(i))  </span><br><span class="line">  &#125;  </span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;  </span><br><span class="line">    <span class="keyword">val</span> dep = dependencies.head.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]]  </span><br><span class="line">    <span class="keyword">val</span> metrics = context.taskMetrics().createTempShuffleReadMetrics()  </span><br><span class="line">    <span class="type">SparkEnv</span>.get.shuffleManager.getReader(  </span><br><span class="line">      dep.shuffleHandle, split.index, split.index + <span class="number">1</span>, context, metrics)  </span><br><span class="line">      .read()  </span><br><span class="line">      .asInstanceOf[<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)]]  </span><br><span class="line">  &#125;  </span><br><span class="line">	<span class="comment">// ...  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="transformation-RDD-DAG"><a href="#transformation-RDD-DAG" class="headerlink" title="transformation -&gt; RDD DAG"></a>transformation -&gt; RDD DAG</h3><p>每个transformation操作实际上在背后就是创建了对应类型的RDD, 并且设置不同的func</p>
<ul>
<li>map和filter都是窄依赖的操作, 所以最后创建的都是窄依赖类型的RDD MapPartitionsRDD, 这里的clean是为了清理用户函数的闭包</li>
<li>不同点在于map操作对应it操作是map, filter对应的是filter</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;  </span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)  </span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (_, _, iter) =&gt; iter.map(cleanF))  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Boolean</span>): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;  </span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)  </span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">T</span>, <span class="type">T</span>](  </span><br><span class="line">    <span class="keyword">this</span>,  </span><br><span class="line">    (_, _, iter) =&gt; iter.filter(cleanF),  </span><br><span class="line">    preservesPartitioning = <span class="literal">true</span>)  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>reduceByKey是宽依赖操作, 最后创建的是ShuffledRDD, 同时需要指定partitionner</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(partitioner: <span class="type">Partitioner</span>, func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;  </span><br><span class="line">  combineByKeyWithClassTag[<span class="type">V</span>]((v: <span class="type">V</span>) =&gt; v, func, func, partitioner)  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKeyWithClassTag</span></span>[<span class="type">C</span>](  </span><br><span class="line">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,  </span><br><span class="line">    mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,  </span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,  </span><br><span class="line">    partitioner: <span class="type">Partitioner</span>,  </span><br><span class="line">    mapSideCombine: <span class="type">Boolean</span> = <span class="literal">true</span>,  </span><br><span class="line">    serializer: <span class="type">Serializer</span> = <span class="literal">null</span>)(<span class="keyword">implicit</span> ct: <span class="type">ClassTag</span>[<span class="type">C</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = self.withScope &#123;  </span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  <span class="keyword">if</span> (self.partitioner == <span class="type">Some</span>(partitioner)) &#123;  </span><br><span class="line">    self.mapPartitions(iter =&gt; &#123;  </span><br><span class="line">      <span class="keyword">val</span> context = <span class="type">TaskContext</span>.get()  </span><br><span class="line">      <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, aggregator.combineValuesByKey(iter, context))  </span><br><span class="line">    &#125;, preservesPartitioning = <span class="literal">true</span>)  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](self, partitioner)  </span><br><span class="line">      .setSerializer(serializer)  </span><br><span class="line">      .setAggregator(aggregator)  </span><br><span class="line">      .setMapSideCombine(mapSideCombine)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>最后用户的代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;data.txt&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = rdd1.map(line =&gt; line.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd2.filter(arr =&gt; arr.length &gt; <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> rdd4 = rdd3.map(arr =&gt; (arr(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> rdd5 = rdd4.reduceByKey(_ + _)</span><br><span class="line"><span class="keyword">val</span> result = rdd5.count()</span><br></pre></td></tr></table></figure>
<p>最后就会形成 RDD DAG</p>
<p>HadoopRDD rdd1 &lt;- MapPartionsRDD rdd2 &lt;- MapPartionsRDD rdd 3 &lt;- MapPartionsRDD rdd4 &lt;- ShuffledRDD rdd5 (<strong>finalRdd</strong>)</p>
<p>最后的count()是action操作, 我们进入到下一阶段</p>
<h2 id="2-Action-触发-Job提交"><a href="#2-Action-触发-Job提交" class="headerlink" title="2. Action 触发 Job提交"></a>2. Action 触发 Job提交</h2><h3 id="RDD-action-调用SparkContext-runJob"><a href="#RDD-action-调用SparkContext-runJob" class="headerlink" title="RDD.action()调用SparkContext.runJob()"></a>RDD.action()调用SparkContext.runJob()</h3><p><strong>所有的action类型的操作最后都是在调用SparkContext.runJob(rdd, func, …)</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// RDD.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span> = sc.runJob(<span class="keyword">this</span>, <span class="type">Utils</span>.getIteratorSize _).sum</span><br></pre></td></tr></table></figure>

<p>runJob有非常多种类型, 对应不同action操作传入的参数是不一样的情况</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// count的入口runJob</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>], func: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">U</span>): <span class="type">Array</span>[<span class="type">U</span>] = &#123;  </span><br><span class="line">  runJob(rdd, func, rdd.partitions.indices)  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 清理函数闭包, 并且解析出partitions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](  </span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],  </span><br><span class="line">    func: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">U</span>,  </span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>]): <span class="type">Array</span>[<span class="type">U</span>] = &#123;  </span><br><span class="line">  <span class="keyword">val</span> cleanedFunc = clean(func)  </span><br><span class="line">  runJob(rdd, (ctx: <span class="type">TaskContext</span>, it: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedFunc(it), partitions)  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 构建resultHandler, 返回的结果会存放在results中</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](  </span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],  </span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,  </span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>]): <span class="type">Array</span>[<span class="type">U</span>] = &#123;  </span><br><span class="line">  <span class="keyword">val</span> results = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">U</span>](partitions.size)  </span><br><span class="line">  runJob[<span class="type">T</span>, <span class="type">U</span>](rdd, func, partitions, (index, res) =&gt; results(index) = res)  </span><br><span class="line">  results  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 最后的核心入口函数, 这里的核心就是调用dagScheduler.runJob</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](  </span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],  </span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,  </span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],  </span><br><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  <span class="keyword">val</span> callSite = getCallSite()  </span><br><span class="line">  <span class="keyword">val</span> cleanedFunc = clean(func)  </span><br><span class="line">  dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)  </span><br><span class="line">  progressBar.foreach(_.finishAll())  </span><br><span class="line">  rdd.doCheckpoint()  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3 id="DAGScheduler-runJob"><a href="#DAGScheduler-runJob" class="headerlink" title="DAGScheduler.runJob()"></a>DAGScheduler.runJob()</h3><p>调用链DAGScheduler.runJob -&gt; DAGScheduler.submitJob -&gt; </p>
<ul>
<li>runJob()</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](  </span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],  </span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,  </span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],  </span><br><span class="line">    callSite: <span class="type">CallSite</span>,  </span><br><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,  </span><br><span class="line">    properties: <span class="type">Properties</span>): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  <span class="keyword">val</span> start = <span class="type">System</span>.nanoTime  </span><br><span class="line">  <span class="comment">// key code: submitJob()</span></span><br><span class="line">  <span class="comment">// 等待结束, 生成日志</span></span><br><span class="line">  <span class="keyword">val</span> waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)  </span><br><span class="line">  <span class="type">ThreadUtils</span>.awaitReady(waiter.completionFuture, <span class="type">Duration</span>.<span class="type">Inf</span>)  </span><br><span class="line">  waiter.completionFuture.value.get <span class="keyword">match</span> &#123;  </span><br><span class="line">    <span class="keyword">case</span> scala.util.<span class="type">Success</span>(_) =&gt;  </span><br><span class="line">			<span class="comment">// sucesss log</span></span><br><span class="line">    <span class="keyword">case</span> scala.util.<span class="type">Failure</span>(exception) =&gt;  </span><br><span class="line">      <span class="comment">// failure log and stack Trace  </span></span><br><span class="line">      <span class="keyword">throw</span> exception  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>submitJob()<ul>
<li>这里对于partitions为空的时候做了快速失败处理, 直接post job开始和job结束的消息</li>
<li>如果partitions非空, 也就是数据非空, 提交job (post JobSubmitted)</li>
<li>生成唯一jobId</li>
</ul>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submitJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](  </span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],  </span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,  </span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],  </span><br><span class="line">    callSite: <span class="type">CallSite</span>,  </span><br><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,  </span><br><span class="line">    properties: <span class="type">Properties</span>): <span class="type">JobWaiter</span>[<span class="type">U</span>] = &#123;  </span><br><span class="line">  <span class="comment">// Check to make sure we are not launching a task on a partition that does not exist.  </span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> jobId = nextJobId.getAndIncrement()  </span><br><span class="line">  <span class="keyword">if</span> (partitions.isEmpty) &#123;  </span><br><span class="line">		<span class="comment">// fast fail </span></span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> func2 = func.asInstanceOf[(<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _]  </span><br><span class="line">	<span class="comment">// 向DAGScheduler事件循环提交这个job任务</span></span><br><span class="line">  eventProcessLoop.post(<span class="type">JobSubmitted</span>(</span><br><span class="line">    jobId, rdd, func2, partitions.toArray, callSite, waiter,  </span><br><span class="line">    <span class="type">JobArtifactSet</span>.getActiveOrDefault(sc),  </span><br><span class="line">    <span class="type">Utils</span>.cloneProperties(properties)))  </span><br><span class="line">  waiter  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-DAGScheduler-事件循环处理-JobSubmitted"><a href="#3-DAGScheduler-事件循环处理-JobSubmitted" class="headerlink" title="3. DAGScheduler 事件循环处理 JobSubmitted"></a>3. DAGScheduler 事件循环处理 JobSubmitted</h2><h3 id="事件分发"><a href="#事件分发" class="headerlink" title="事件分发"></a>事件分发</h3><p>DAGSchedulerEventProcessLoop: DAGScheduler事件循环处理器, 在收到了Event以后通过onReceive来分发事件, JobSubmitted Event会被分发到handleJobSubmitted方法上</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[scheduler] <span class="class"><span class="keyword">class</span> <span class="title">DAGSchedulerEventProcessLoop</span>(<span class="params">dagScheduler: <span class="type">DAGScheduler</span></span>)  </span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">EventLoop</span>[<span class="type">DAGSchedulerEvent</span>](<span class="string">&quot;dag-scheduler-event-loop&quot;</span>) <span class="keyword">with</span> <span class="type">Logging</span> &#123;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> timer = dagScheduler.metricsSource.messageProcessingTimer  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">/**  </span></span><br><span class="line"><span class="comment">   * The main event loop of the DAG scheduler.   */</span>  </span><br><span class="line"> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = &#123;  </span><br><span class="line">    <span class="keyword">val</span> timerContext = timer.time()  </span><br><span class="line">    <span class="keyword">try</span> &#123;  </span><br><span class="line">      doOnReceive(event)  </span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;  </span><br><span class="line">      <span class="keyword">case</span> ex: <span class="type">ShuffleStatusNotFoundException</span> =&gt;  </span><br><span class="line">        dagScheduler.handleShuffleStatusNotFoundException(ex)  </span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;  </span><br><span class="line">      timerContext.stop()  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doOnReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> &#123;  </span><br><span class="line">    <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, callSite, listener, artifacts, properties) =&gt;  </span><br><span class="line">      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, artifacts,  </span><br><span class="line">        properties)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">case</span> <span class="type">MapStageSubmitted</span>(jobId, dependency, callSite, listener, artifacts, properties) =&gt;  </span><br><span class="line">      dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, artifacts,  </span><br><span class="line">        properties)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">case</span> <span class="type">StageCancelled</span>(stageId, reason) =&gt;  </span><br><span class="line">      dagScheduler.handleStageCancellation(stageId, reason)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">case</span> <span class="type">JobCancelled</span>(jobId, reason) =&gt;  </span><br><span class="line">      dagScheduler.handleJobCancellation(jobId, reason)  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">StageFailed</span>(stageId, reason, exception) =&gt;  </span><br><span class="line">      dagScheduler.handleStageFailed(stageId, reason, exception)  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="开始Stage划分和Job创建"><a href="#开始Stage划分和Job创建" class="headerlink" title="开始Stage划分和Job创建"></a>开始Stage划分和Job创建</h3><p> 核心代码</p>
<ul>
<li>createResultStage: 划分出来Stage, 得到最后的finalStage (也就是ResultStage, 同时也是在这一步创建的Stage DAG)</li>
<li>setActiveJob</li>
<li>submitStage: 提交finalStage也就是提交job</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(  </span><br><span class="line">    jobId: <span class="type">Int</span>,  </span><br><span class="line">    finalRDD: <span class="type">RDD</span>[_],  </span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,  </span><br><span class="line">    partitions: <span class="type">Array</span>[<span class="type">Int</span>],  </span><br><span class="line">    callSite: <span class="type">CallSite</span>,  </span><br><span class="line">    listener: <span class="type">JobListener</span>,  </span><br><span class="line">    artifacts: <span class="type">JobArtifactSet</span>,  </span><br><span class="line">    properties: <span class="type">Properties</span>): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  <span class="comment">// If this job belongs to a cancelled job group, skip running it  </span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">var</span> finalStage: <span class="type">ResultStage</span> = <span class="literal">null</span>  </span><br><span class="line">  <span class="keyword">try</span> &#123;  </span><br><span class="line">		<span class="comment">// 核心代码</span></span><br><span class="line">    finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)  </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Job submitted, clear internal data.  </span></span><br><span class="line">  barrierJobIdToNumTasksCheckFailures.remove(jobId)  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> job = <span class="keyword">new</span> <span class="type">ActiveJob</span>(jobId, finalStage, callSite, listener, artifacts, properties)  </span><br><span class="line">  clearCacheLocs()   </span><br><span class="line">  </span><br><span class="line">	</span><br><span class="line">  jobIdToActiveJob(jobId) = job  </span><br><span class="line">  activeJobs += job  </span><br><span class="line">  finalStage.setActiveJob(job)  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 提交job</span></span><br><span class="line">  <span class="keyword">val</span> stageIds = jobIdToStageIds(jobId).toArray  </span><br><span class="line">  <span class="keyword">val</span> stageInfos =  </span><br><span class="line">    stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo)).toImmutableArraySeq  </span><br><span class="line">  listenerBus.post(  </span><br><span class="line">    <span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, stageInfos,  </span><br><span class="line">      <span class="type">Utils</span>.cloneProperties(properties)))  </span><br><span class="line">  submitStage(finalStage)  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="4-Stage划分-核心逻辑"><a href="#4-Stage划分-核心逻辑" class="headerlink" title="4. Stage划分 (核心逻辑)"></a>4. Stage划分 (核心逻辑)</h2><p>在这一步会从finalRDD开始, 向上遍历整个RDD DAG</p>
<ul>
<li>调用getShuffleDependenciesAndResourceProfiles找出finalRDD的最近的宽依赖们</li>
<li>调用getOrCreateParentStages, 为每个ShuffleDependency创建一个ShuffleMapStage</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createResultStage</span></span>(  </span><br><span class="line">    rdd: <span class="type">RDD</span>[_],  </span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,  </span><br><span class="line">    partitions: <span class="type">Array</span>[<span class="type">Int</span>],  </span><br><span class="line">    jobId: <span class="type">Int</span>,  </span><br><span class="line">    callSite: <span class="type">CallSite</span>): <span class="type">ResultStage</span> = &#123;  </span><br><span class="line">  <span class="comment">// 创建finalRDD所处的Stage</span></span><br><span class="line">  <span class="keyword">val</span> (shuffleDeps, resourceProfiles) = getShuffleDependenciesAndResourceProfiles(rdd)  </span><br><span class="line">  <span class="keyword">val</span> resourceProfile = mergeResourceProfilesForStage(resourceProfiles) </span><br><span class="line">  <span class="comment">// 递归从finalRDD向前创建所有的父stage </span></span><br><span class="line">  <span class="keyword">val</span> parents = getOrCreateParentStages(shuffleDeps, jobId)  </span><br><span class="line">  </span><br><span class="line">	<span class="comment">// 创建最后的返回的数据的Stage</span></span><br><span class="line">  <span class="keyword">val</span> id = nextStageId.getAndIncrement()  </span><br><span class="line">  <span class="keyword">val</span> stage = <span class="keyword">new</span> <span class="type">ResultStage</span>(id, rdd, func, partitions, parents, jobId,  </span><br><span class="line">    callSite, resourceProfile.id)  </span><br><span class="line">  stageIdToStage(id) = stage  </span><br><span class="line">  updateJobIdStageIdMaps(jobId, stage)  </span><br><span class="line">  stage  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">getShuffleDependenciesAndResourceProfiles</span></span>(  </span><br><span class="line">    rdd: <span class="type">RDD</span>[_]): (<span class="type">HashSet</span>[<span class="type">ShuffleDependency</span>[_, _, _]], <span class="type">HashSet</span>[<span class="type">ResourceProfile</span>]) = &#123;  </span><br><span class="line">  <span class="keyword">val</span> parents = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">ShuffleDependency</span>[_, _, _]]  </span><br><span class="line">  <span class="keyword">val</span> resourceProfiles = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">ResourceProfile</span>]  </span><br><span class="line">  <span class="keyword">val</span> visited = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">RDD</span>[_]]  </span><br><span class="line">  <span class="keyword">val</span> waitingForVisit = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">RDD</span>[_]]  </span><br><span class="line">  waitingForVisit += rdd  </span><br><span class="line">  <span class="keyword">while</span> (waitingForVisit.nonEmpty) &#123;  </span><br><span class="line">    <span class="keyword">val</span> toVisit = waitingForVisit.remove(<span class="number">0</span>)  </span><br><span class="line">    <span class="keyword">if</span> (!visited(toVisit)) &#123;  </span><br><span class="line">      visited += toVisit  </span><br><span class="line">      <span class="type">Option</span>(toVisit.getResourceProfile()).foreach(resourceProfiles += _)  </span><br><span class="line">      toVisit.dependencies.foreach &#123;  </span><br><span class="line">        <span class="keyword">case</span> shuffleDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt;  </span><br><span class="line">          parents += shuffleDep  </span><br><span class="line">        <span class="keyword">case</span> dependency =&gt;  </span><br><span class="line">          waitingForVisit.prepend(dependency.rdd)  </span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">  (parents, resourceProfiles)  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://functional.top">John Doe</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://functional.top/2025/11/24/Big-Data/Spark/source%20code%20read/3.%20How%20spark%20job%20trigger%20a%20job%20and%20how%20it%20split%20job%20to%20stages%20and%20schedule%20the%20stage./">https://functional.top/2025/11/24/Big-Data/Spark/source%20code%20read/3.%20How%20spark%20job%20trigger%20a%20job%20and%20how%20it%20split%20job%20to%20stages%20and%20schedule%20the%20stage./</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/BigData/">BigData</a><a class="post-meta__tags" href="/tags/Spark/">Spark</a><a class="post-meta__tags" href="/tags/source-code/">source_code</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2026/02/08/AI-Framwork/Agent/" title=""><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2"></div></div><div class="info-2"><div class="info-item-1">Agent Agent是对以LLM为驱动工程系统, 在整个工程化的结构里面, 最核心的两个脉络是上下文和与外界交互的能力, 目前出现的任何相关的技术都可以被分类到两者之一, 解决领域中某个特定的问题, 还有一个副主题是规范制定, 这是为了让解决上述两个问题的组件能以通用的形式接入Agent系统 (仅关注Agent本身的能力, 对于监测, 可视化, 后训练等内容, 在这里被归为Agent调优, 在agent侧本质上也是在调优上面两者, 调优模型不在讨论范围内)  Agent是对LLM的一个工程化包装, 底层调用的模型服务, LLM api能提供的能力及其有限. 你无法通过LLM api去调用你电脑上的某个api, 没办法让ta对和你的对话保有记忆. 一个agent的输入是(用户的message), 输出是(执行action, 输出response). 上下文 agent的运行, 实际上就是上下文的流转, 理解LLM在某个时刻点为什么这么做了, 为什么不这么做的核心就是, 看那一刻该LLM持有了什么上下文  我们假设model是稳定的, 宽泛地将llm视作一个函数, 即假设我们的输入...</div></div></div></a><a class="pagination-related" href="/2025/11/21/Big-Data/Spark/source%20code%20read/2.%20How%20spark%20access%20Yarn%20and%20start%20container%20to%20run%20tasks./" title="[Spark Source Code 2] How spark access YARN and start container to run tasks."><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">[Spark Source Code 2] How spark access YARN and start container to run tasks.</div></div><div class="info-2"><div class="info-item-1">Spark on YARN的整体运行的架构YARN重要的角色  Resource Manager: 全局资源管理器 ApplicationManager: 应用管理器(RM子组件), 接收Client提交的请求 为每个APP分配一个appId 选择NM启动AM 管理AM的生命周期   Scheduler: 资源调度器(RM子组件) 根据调度策略分配资源, 响应AM的资源请求   NodeManager: 节点管理器, 每台机器分配一个NM, 是这台机器在集群中的代理 管理单个节点的资源 向RM报告节点状态和资源使用情况 接收从AM收到请求   Container YARN中资源分配的基本单位 对应着一组CPU 内存等资源 运行ApplicationMaster (AM) 或Executor    Spark on YARN的运行流程ApplicationMaster(区别于上面的ApplicationManager): 每个app都会分配一个AM, 该AM是这个app在YARN集群的代理, 运行在YARN集群中的app通过AM向RM(scheduler)请求资源  提交AM Cl...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/11/18/Big-Data/Spark/source%20code%20read/1.%20How%20launch%20a%20spark%20application/" title="[Spark Source Code 1] How launch a spark Application"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-18</div><div class="info-item-2">[Spark Source Code 1] How launch a spark Application</div></div><div class="info-2"><div class="info-item-1"> 只关注spark application通过spark-submit shell脚本启动的情况以问题为导向探究   ✅ spark-submit 脚本做了什么？ ✅ Launcher 层的作用是什么？为什么需要它？ ✅ 参数是如何从 Shell 传递到 Java 的？ ✅ 为什么使用 NULL 分隔符？ ✅ prepareSubmitEnvironment() 返回的 4 个值分别是什么？ ✅ Client 和 Cluster 模式的 childMainClass 有什么不同？ ✅ 不同集群管理器（YARN&#x2F;K8s&#x2F;Standalone）的启动有什么区别？ ✅ app.start() 之后发生了什么？ ✅ 配置的优先级是怎样的？ ✅ 为什么某些模式组合不支持（如 LOCAL + CLUSTER）？  How spark command is parsedshell脚本入口阶段 找到SPARK_HOME, JAVA_HOME, 将launcher入口类和submit入口类添加到执行路径中, 补全java命令, 执行命令  整体的流程entry pointfil...</div></div></div></a><a class="pagination-related" href="/2025/11/21/Big-Data/Spark/source%20code%20read/2.%20How%20spark%20access%20Yarn%20and%20start%20container%20to%20run%20tasks./" title="[Spark Source Code 2] How spark access YARN and start container to run tasks."><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-21</div><div class="info-item-2">[Spark Source Code 2] How spark access YARN and start container to run tasks.</div></div><div class="info-2"><div class="info-item-1">Spark on YARN的整体运行的架构YARN重要的角色  Resource Manager: 全局资源管理器 ApplicationManager: 应用管理器(RM子组件), 接收Client提交的请求 为每个APP分配一个appId 选择NM启动AM 管理AM的生命周期   Scheduler: 资源调度器(RM子组件) 根据调度策略分配资源, 响应AM的资源请求   NodeManager: 节点管理器, 每台机器分配一个NM, 是这台机器在集群中的代理 管理单个节点的资源 向RM报告节点状态和资源使用情况 接收从AM收到请求   Container YARN中资源分配的基本单位 对应着一组CPU 内存等资源 运行ApplicationMaster (AM) 或Executor    Spark on YARN的运行流程ApplicationMaster(区别于上面的ApplicationManager): 每个app都会分配一个AM, 该AM是这个app在YARN集群的代理, 运行在YARN集群中的app通过AM向RM(scheduler)请求资源  提交AM Cl...</div></div></div></a><a class="pagination-related" href="/2026/02/08/Big-Data/Hive/" title="Hive Introduction"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-08</div><div class="info-item-2">Hive Introduction</div></div><div class="info-2"><div class="info-item-1">Hive的作用Hive将提交任务这一件事情简化, 屏蔽了执行引擎, 提供了HiveQL供用户使用, 用户可以使用SQL的形式提交并执行任务. 从输入数据到最后的执行经过的步骤:  用户通过客户端连接到HiveServer2提交SQL -&gt; Driver接受客户端的HiveQL -&gt; Compiler &#x2F; Semantic Analyzer &#x2F; Optimizer将QL转化成逻辑计划, 物理计划, 并进行CBO优化 -&gt; Execution Engine将物理计划分解成具体的执行任务, 提交到底层的计算引擎上 (MapReduce, Spark) -&gt; 在HDFS上存储数据, 由YARN分配资源给计算引擎 另一条线是HiveServer2 -&gt; Hive MetaStore Server访问元数据, 通过Thrift提供元数据 (表&#x2F;分区&#x2F;统计), 并将结果持久化到关系型数据库  HMS (Hive Metastore)HMS </div></div></div></a><a class="pagination-related" href="/2025/10/30/Big-Data/Livy/Livy_1_Basic/" title="Livy基础知识"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-30</div><div class="info-item-2">Livy基础知识</div></div><div class="info-2"><div class="info-item-1">概念理解Livy 解决了什么问题？为什么不直接用 spark-submit？解决了用户提交spark任务的易用性和控制性问题, 使用户能通过HTTP请求向spark提交任务  权限问题: 使用spark-submit, 用户需要登陆上服务器, 使用spark-submit命令提交, 这将spark所在的服务器的访问权限和提交spark任务的权限绑定, 但是实际上提交spark任务的role和访问服务器的role应该是分离的. 易用性问题: 在过去用户需要在本地写好代码, 通过SFTP上传到服务器上, 再通过spark-submit提交任务, 现在用户可以通过jupyter + livy的方式, 在本地编写代码, 动态变更并且随时通过HTTP向Livy提交任务 自动化问题: 直接通过spark-submit提交任务, 无法对于spark任务进行集中管理, 捕获一些指标, 或者进行配置覆盖等其他的自动化的操作, Livy的引入相当于为用户和spark-submit之间注入了一个管理的中间层  Livy 支持哪几种 Session 类型？它们的区别是什么？Session Type分成I...</div></div></div></a><a class="pagination-related" href="/2026/02/08/Big-Data/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%80%BB%E8%A7%88/" title="大数据结构总览"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-08</div><div class="info-item-2">大数据结构总览</div></div><div class="info-2"><div class="info-item-1">大数据结构大数据可以分成五个层次 数据收集:将数据从数据源收集到数据存储层, 常用的组件有关系型, 非关系型, 分布式消息队列三种  关系型: Sqooq&#x2F;Canal, 连接MySQL这种关系型数据库和Hadoop之间的桥梁, Sqool可以全量将数据库中的数据导入到Hadoop中, Canal则能实现增量的导入 非关系型: Flume, 流式数据收集, 比如日志等数据, 经过ETL以后导入到Hadoop中 分布式消息队列: [[Kafka]], 常用作消息总线, 有分布式的高容错的设计, 适配大数据场景  数据存储主要由分布式文件系统和分布式数据库组成  分布式文件系统: HDFS, Hadoop分布式文件系统, 有强大的容错机制, 社区开发了很多种文件格式 分布式数据库: HBase, 构建在HDFS之上的分布式数据库, 提供结构化和半结构化的数据库, 支持列无限拓展  资源管理和服务调度 YARN: 统一资源管理与调度系统, 能够管理集群中的各种资源, 并按照一定策略分配给上层的各类应用. 同时支持灵活的配置, 允许用户按照队列的方式组织和管理资源, 且每个队列的...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">John Doe</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark%E5%90%AF%E5%8A%A8%E4%B8%80%E4%B8%AAjob%E7%9A%84%E8%BF%87%E7%A8%8B"><span class="toc-number">1.</span> <span class="toc-text">Spark启动一个job的过程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E7%BD%AE%E6%9D%A1%E4%BB%B6"><span class="toc-number">1.1.</span> <span class="toc-text">前置条件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.2.</span> <span class="toc-text">详细步骤</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90"><span class="toc-number">2.</span> <span class="toc-text">源码解析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%94%A8%E6%88%B7%E7%9A%84transformation%E6%93%8D%E4%BD%9C%E5%BD%A2%E6%88%90RDD-DAG"><span class="toc-number">2.1.</span> <span class="toc-text">1. 用户的transformation操作形成RDD DAG</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD"><span class="toc-number">2.1.1.</span> <span class="toc-text">RDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transformation-RDD-DAG"><span class="toc-number">2.1.2.</span> <span class="toc-text">transformation -&gt; RDD DAG</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">2.1.3.</span> <span class="toc-text">小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Action-%E8%A7%A6%E5%8F%91-Job%E6%8F%90%E4%BA%A4"><span class="toc-number">2.2.</span> <span class="toc-text">2. Action 触发 Job提交</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD-action-%E8%B0%83%E7%94%A8SparkContext-runJob"><span class="toc-number">2.2.1.</span> <span class="toc-text">RDD.action()调用SparkContext.runJob()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DAGScheduler-runJob"><span class="toc-number">2.2.2.</span> <span class="toc-text">DAGScheduler.runJob()</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-DAGScheduler-%E4%BA%8B%E4%BB%B6%E5%BE%AA%E7%8E%AF%E5%A4%84%E7%90%86-JobSubmitted"><span class="toc-number">2.3.</span> <span class="toc-text">3. DAGScheduler 事件循环处理 JobSubmitted</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8B%E4%BB%B6%E5%88%86%E5%8F%91"><span class="toc-number">2.3.1.</span> <span class="toc-text">事件分发</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%80%E5%A7%8BStage%E5%88%92%E5%88%86%E5%92%8CJob%E5%88%9B%E5%BB%BA"><span class="toc-number">2.3.2.</span> <span class="toc-text">开始Stage划分和Job创建</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Stage%E5%88%92%E5%88%86-%E6%A0%B8%E5%BF%83%E9%80%BB%E8%BE%91"><span class="toc-number">2.4.</span> <span class="toc-text">4. Stage划分 (核心逻辑)</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/08/Git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" title="Untitled">Untitled</a><time datetime="2026-02-08T11:04:02.565Z" title="Created 2026-02-08 11:04:02">2026-02-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/08/Big-Data/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%80%BB%E8%A7%88/" title="大数据结构总览">大数据结构总览</a><time datetime="2026-02-08T11:04:02.565Z" title="Created 2026-02-08 11:04:02">2026-02-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/08/How/How_to_Code/" title="Untitled">Untitled</a><time datetime="2026-02-08T11:04:02.565Z" title="Created 2026-02-08 11:04:02">2026-02-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/08/How/How_to_Learn_a_Language/" title="如何学习一门语言(以scala为例)">如何学习一门语言(以scala为例)</a><time datetime="2026-02-08T11:04:02.565Z" title="Created 2026-02-08 11:04:02">2026-02-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/08/Language/Scala%20Basic/" title="Scala基础语法">Scala基础语法</a><time datetime="2026-02-08T11:04:02.565Z" title="Created 2026-02-08 11:04:02">2026-02-08</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By John Doe</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.0</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>